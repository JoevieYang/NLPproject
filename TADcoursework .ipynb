{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI3Iyv2K-Lkg"
      },
      "source": [
        "# **Coursework of Text as Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyQ-y5IZVOx4"
      },
      "source": [
        "In this coursework, there are seven sections. A dataset of dialogs in movie is used to do clustering, classification and more. The last part is an review of a selected paper. The tool name convokit is used to operate dataset which is designed by Cornell group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-5m0XfcJ6LR2"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AZFvVPtJ_59M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbac9345-2f0e-4732-a268-e998c3d455e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting convokit\n",
            "  Downloading convokit-2.5.3.tar.gz (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.0/168.0 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.9/dist-packages (from convokit) (1.4.4)\n",
            "Collecting msgpack-numpy>=0.4.3.2\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.9/dist-packages (from convokit) (3.5.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from convokit) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from convokit) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.9/dist-packages (from convokit) (3.8.1)\n",
            "Collecting dill>=0.2.9\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.9/dist-packages (from convokit) (1.1.1)\n",
            "Collecting clean-text>=0.1.1\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (4.39.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.0->convokit) (8.4.0)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.4->convokit) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.4->convokit) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.4->convokit) (2022.10.31)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.4->convokit) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->convokit) (3.1.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (2.27.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (1.10.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (8.1.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (63.4.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=2.3.5->convokit) (1.0.4)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.1.1->convokit) (0.2.6)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.0->convokit) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.3.5->convokit) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.2)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-2.5.3-py3-none-any.whl size=204125 sha256=8acb47cbbe8940236502835cb77de8e07259b30886dd0e3f11a1d27691aa1a69\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/ef/38/9588c33dabed09f2a31ce3fbde7f6a6a1c19593c1ffd8d74aa\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171048 sha256=b32b3ace5c120befc1cec23c9e49e364747c1406eddb4e2ea1939541d6ff7b78\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/7a/e9/22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dill, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-2.5.3 dill-0.3.6 emoji-1.7.0 ftfy-6.1.1 msgpack-numpy-0.4.8 unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip3 install convokit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QUqMH0t4CdD"
      },
      "source": [
        "## **Part 1 Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_viEDo7iFwHN"
      },
      "source": [
        "### **Glance of Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaZEMmMv_ld0",
        "outputId": "7b6c6a06-4c30-4efd-a657-36952826d1c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
          ]
        }
      ],
      "source": [
        "from convokit import Corpus, download\n",
        "corpus = Corpus(filename=download(\"movie-corpus\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "11x5-sSIFuub",
        "outputId": "fec9153c-ac57-4623-9516-8cfa11b02569"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   vectors meta.character_name meta.movie_idx             meta.movie_name  \\\n",
              "id                                                                          \n",
              "u0      []              BIANCA             m0  10 things i hate about you   \n",
              "u2      []             CAMERON             m0  10 things i hate about you   \n",
              "u3      []            CHASTITY             m0  10 things i hate about you   \n",
              "u4      []                JOEY             m0  10 things i hate about you   \n",
              "u5      []                 KAT             m0  10 things i hate about you   \n",
              "\n",
              "   meta.gender meta.credit_pos  \n",
              "id                              \n",
              "u0           f               4  \n",
              "u2           m               3  \n",
              "u3           ?               ?  \n",
              "u4           m               6  \n",
              "u5           f               2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a90c4e4-099b-4fcd-a0b5-9bb677e83f35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vectors</th>\n",
              "      <th>meta.character_name</th>\n",
              "      <th>meta.movie_idx</th>\n",
              "      <th>meta.movie_name</th>\n",
              "      <th>meta.gender</th>\n",
              "      <th>meta.credit_pos</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>u0</th>\n",
              "      <td>[]</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>f</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u2</th>\n",
              "      <td>[]</td>\n",
              "      <td>CAMERON</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>m</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u3</th>\n",
              "      <td>[]</td>\n",
              "      <td>CHASTITY</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u4</th>\n",
              "      <td>[]</td>\n",
              "      <td>JOEY</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>m</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u5</th>\n",
              "      <td>[]</td>\n",
              "      <td>KAT</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>f</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a90c4e4-099b-4fcd-a0b5-9bb677e83f35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a90c4e4-099b-4fcd-a0b5-9bb677e83f35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a90c4e4-099b-4fcd-a0b5-9bb677e83f35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "corpus.get_speakers_dataframe().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "3JXRafPPGH6P",
        "outputId": "04e0b125-8351-415f-91da-4a4c4b1763c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5af2a39f-559a-471a-bbca-1b286de2cbdd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>text</th>\n",
              "      <th>speaker</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>meta.movie_id</th>\n",
              "      <th>meta.parsed</th>\n",
              "      <th>vectors</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>L1045</th>\n",
              "      <td>None</td>\n",
              "      <td>They do not!</td>\n",
              "      <td>u0</td>\n",
              "      <td>L1044</td>\n",
              "      <td>L1044</td>\n",
              "      <td>m0</td>\n",
              "      <td>[{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PR...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L1044</th>\n",
              "      <td>None</td>\n",
              "      <td>They do to!</td>\n",
              "      <td>u2</td>\n",
              "      <td>None</td>\n",
              "      <td>L1044</td>\n",
              "      <td>m0</td>\n",
              "      <td>[{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PR...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L985</th>\n",
              "      <td>None</td>\n",
              "      <td>I hope so.</td>\n",
              "      <td>u0</td>\n",
              "      <td>L984</td>\n",
              "      <td>L984</td>\n",
              "      <td>m0</td>\n",
              "      <td>[{'rt': 1, 'toks': [{'tok': 'I', 'tag': 'PRP',...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L984</th>\n",
              "      <td>None</td>\n",
              "      <td>She okay?</td>\n",
              "      <td>u2</td>\n",
              "      <td>None</td>\n",
              "      <td>L984</td>\n",
              "      <td>m0</td>\n",
              "      <td>[{'rt': 1, 'toks': [{'tok': 'She', 'tag': 'PRP...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L925</th>\n",
              "      <td>None</td>\n",
              "      <td>Let's go.</td>\n",
              "      <td>u0</td>\n",
              "      <td>L924</td>\n",
              "      <td>L924</td>\n",
              "      <td>m0</td>\n",
              "      <td>[{'rt': 0, 'toks': [{'tok': 'Let', 'tag': 'VB'...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5af2a39f-559a-471a-bbca-1b286de2cbdd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5af2a39f-559a-471a-bbca-1b286de2cbdd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5af2a39f-559a-471a-bbca-1b286de2cbdd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      timestamp          text speaker reply_to conversation_id meta.movie_id  \\\n",
              "id                                                                             \n",
              "L1045      None  They do not!      u0    L1044           L1044            m0   \n",
              "L1044      None   They do to!      u2     None           L1044            m0   \n",
              "L985       None    I hope so.      u0     L984            L984            m0   \n",
              "L984       None     She okay?      u2     None            L984            m0   \n",
              "L925       None     Let's go.      u0     L924            L924            m0   \n",
              "\n",
              "                                             meta.parsed vectors  \n",
              "id                                                                \n",
              "L1045  [{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PR...      []  \n",
              "L1044  [{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PR...      []  \n",
              "L985   [{'rt': 1, 'toks': [{'tok': 'I', 'tag': 'PRP',...      []  \n",
              "L984   [{'rt': 1, 'toks': [{'tok': 'She', 'tag': 'PRP...      []  \n",
              "L925   [{'rt': 0, 'toks': [{'tok': 'Let', 'tag': 'VB'...      []  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus.get_utterances_dataframe().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "IfUbJZxWGItj",
        "outputId": "01953191-e49e-4175-bce7-26f8aecb7667"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a8abbc1b-b2b0-45ac-9aa5-38f5dc14ce9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vectors</th>\n",
              "      <th>meta.movie_idx</th>\n",
              "      <th>meta.movie_name</th>\n",
              "      <th>meta.release_year</th>\n",
              "      <th>meta.rating</th>\n",
              "      <th>meta.votes</th>\n",
              "      <th>meta.genre</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>L1044</th>\n",
              "      <td>[]</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>1999</td>\n",
              "      <td>6.90</td>\n",
              "      <td>62847</td>\n",
              "      <td>['comedy', 'romance']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L984</th>\n",
              "      <td>[]</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>1999</td>\n",
              "      <td>6.90</td>\n",
              "      <td>62847</td>\n",
              "      <td>['comedy', 'romance']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L924</th>\n",
              "      <td>[]</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>1999</td>\n",
              "      <td>6.90</td>\n",
              "      <td>62847</td>\n",
              "      <td>['comedy', 'romance']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L870</th>\n",
              "      <td>[]</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>1999</td>\n",
              "      <td>6.90</td>\n",
              "      <td>62847</td>\n",
              "      <td>['comedy', 'romance']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>L866</th>\n",
              "      <td>[]</td>\n",
              "      <td>m0</td>\n",
              "      <td>10 things i hate about you</td>\n",
              "      <td>1999</td>\n",
              "      <td>6.90</td>\n",
              "      <td>62847</td>\n",
              "      <td>['comedy', 'romance']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8abbc1b-b2b0-45ac-9aa5-38f5dc14ce9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a8abbc1b-b2b0-45ac-9aa5-38f5dc14ce9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a8abbc1b-b2b0-45ac-9aa5-38f5dc14ce9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      vectors meta.movie_idx             meta.movie_name meta.release_year  \\\n",
              "id                                                                           \n",
              "L1044      []             m0  10 things i hate about you              1999   \n",
              "L984       []             m0  10 things i hate about you              1999   \n",
              "L924       []             m0  10 things i hate about you              1999   \n",
              "L870       []             m0  10 things i hate about you              1999   \n",
              "L866       []             m0  10 things i hate about you              1999   \n",
              "\n",
              "      meta.rating meta.votes             meta.genre  \n",
              "id                                                   \n",
              "L1044        6.90      62847  ['comedy', 'romance']  \n",
              "L984         6.90      62847  ['comedy', 'romance']  \n",
              "L924         6.90      62847  ['comedy', 'romance']  \n",
              "L870         6.90      62847  ['comedy', 'romance']  \n",
              "L866         6.90      62847  ['comedy', 'romance']  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus.get_conversations_dataframe().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AXKMa7XLSRp"
      },
      "source": [
        "### **Questions and Answers (Data Preperation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4X8I22C90iW"
      },
      "source": [
        "(a) What is your dataset, \n",
        "and why did you select it? \n",
        "How might automatic classification/labelling of your dataset be\n",
        "used in practice? [2 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDs7i5xv_fPT"
      },
      "source": [
        "The dataset will be used in this coursework is 'Cornell Movie-Dialogs Corpus', a large collection of fictional conversations extracted from raw movie scripts. The size of this dataset can be seen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4OdGhomLYVx",
        "outputId": "9f3a8682-e02d-446f-f681-c567c1524834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Speakers: 9035\n",
            "Number of Utterances: 304713\n",
            "Number of Conversations: 83097\n"
          ]
        }
      ],
      "source": [
        "corpus.print_summary_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfp3pN3X0w_E"
      },
      "source": [
        "Since the conversation objects as samples that will be used in the trial are over 10000, 7000 conversation objects will be selected randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTChro_z1QBs",
        "outputId": "c31681a6-23ef-48af-b2fd-ba385a0f4827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n"
          ]
        }
      ],
      "source": [
        "conver_data = []\n",
        "for i in range(6000):\n",
        "  convs = corpus.random_conversation()\n",
        "  conver_data.append(convs)\n",
        "print(len(conver_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVAVrKA6R6ar"
      },
      "source": [
        "This dataset was selected because of good struction of data, like documents can be conversations which are the combinations of utterance and there is a well-designed tool to process data, like using get_id function to search utterances of a converstaion, plus, my interest. In this dataset, sentences and information of dialog were collected as three objects: Speakers, Utterances, Conversations. Each kind of objects maintains a column named 'meta-data' (shown in dataframes above). The relationship among three objects are like this:\n",
        "\n",
        "![relation.png](data:image/png;base64,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)\n",
        "\n",
        "The idea is to classify genre of conversations using uttrances and genre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya3MHSUl9tZZ"
      },
      "source": [
        "(b) Provide a summary of the labels and input text to be used. \n",
        "What labels are to be predicted? Did you need to do\n",
        "any preprocessing to create the labels or to make sure the number of labels was > 3 and <= 10? \n",
        "What is the text that\n",
        "will be used for classification? [4 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_j5gIDF8kOF"
      },
      "source": [
        "For each conversation the genre is a string with shape of list, in the trial a multiclass classification will conduct, so the processes below extract the first item of the gerne list, just assuming the first one is the most correlated and suitable one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0lyPTJPbq6C"
      },
      "source": [
        "There are totally 24 genres and only the ten most frequent ones are selected: most frequent ones:  ['drama', 'thriller', 'comedy', 'action', 'romance', 'crime', 'sci-fi', 'adventure', 'mystery', 'horror']. And the text that will be used to classify are utterances being inlcuded in a conversation, they are combined as a string text relating to one conversation's label. All processes including removing conversation with empty genre values are shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw_eLef39u-C",
        "outputId": "b4ce79e7-1c2b-4f66-94b6-48f7922823f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['romance', 'comedy', 'drama', 'mystery', 'thriller', 'crime', 'action', 'adventure', 'fantasy', 'horror', 'music', 'sport', 'film', 'noir', 'family', 'biography', 'documentary', 'short', 'history', 'war', 'western', 'animation', 'musical', 'adult']\n",
            "total genres count:  24\n",
            "[('drama', 3219), ('thriller', 2436), ('comedy', 1853), ('romance', 1513), ('action', 1464), ('crime', 1433), ('sci-fi', 1088), ('adventure', 1045), ('mystery', 1014), ('horror', 794)]\n",
            "most frequent ones:  ['drama', 'thriller', 'comedy', 'romance', 'action', 'crime', 'sci-fi', 'adventure', 'mystery', 'horror']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "all_genre = []\n",
        "all = []\n",
        "pattern = '[a-zA-Z]+'\n",
        "count = 1\n",
        "for convs in conver_data:\n",
        "  # print(convs)\n",
        "  convs_genre = re.findall(pattern, convs.meta['genre'])\n",
        "  for g in convs_genre:\n",
        "    if g == 'fi':\n",
        "      pass\n",
        "    elif g == 'sci':\n",
        "      all.append('sci-fi')\n",
        "    else:\n",
        "      all.append(g)\n",
        "      if g not in all_genre:\n",
        "        all_genre.append(g)\n",
        "print(all_genre)\n",
        "print(\"total genres count: \", len(all_genre))\n",
        "\n",
        "counts = Counter(all).most_common(10)\n",
        "print(counts)\n",
        "most_ten_fre = [ counts[i][0] for i in range(10) ]\n",
        "print(\"most frequent ones: \", most_ten_fre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7j9pzkTr5S0",
        "outputId": "b89d58de-c9b0-445c-fb09-50ba8011e95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of label-list:  5445\n",
            "length of texts-list:  5445\n",
            "one label example:  horror\n",
            "one text example:  All right, forget it.  Tony, you got anything we can put on his face?\n",
            "It's the truth, I swear.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from random import sample\n",
        "# count = 0\n",
        "labels, texts = [], []\n",
        "# delete samples whose genre are not the ten most frequent ones\n",
        "for con in conver_data:\n",
        "  convs_genre = re.findall(pattern, con.meta['genre'])\n",
        "  if len(convs_genre) != 0:\n",
        "    genre = convs_genre[0]\n",
        "    if genre == 'sci' or genre == 'fi':\n",
        "      genre = 'sci-fi'\n",
        "    if genre not in most_ten_fre:\n",
        "      # conver_data.remove(con)\n",
        "      pass\n",
        "    else:\n",
        "      # count += 1\n",
        "      labels.append(genre)\n",
        "      utters = list(con.iter_utterances()) \n",
        "      content = \"\"\n",
        "      for utt in utters:  \n",
        "        content += utt.text + '\\n'\n",
        "      texts.append(content)\n",
        "      # con.add_meta('one_genre', genre)\n",
        "      # con.meta['genre'] = genre\n",
        "print(\"length of label-list: \", len(labels))\n",
        "print(\"length of texts-list: \", len(texts))\n",
        "print(\"one label example: \", labels[300])\n",
        "print(\"one text example: \", texts[300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn5H-Cfw9wVC"
      },
      "source": [
        "(c) Is the dataset already split into a training, validation and test set? \n",
        "If not, use a 60/20/20% split to create the\n",
        "training, validation and test set. \n",
        "Provide a table with the label counts for each split of the dataset and comment on\n",
        "the distribution across labels and across data splits. \n",
        "Be sure you use the same splits throughout the entire report.\n",
        "[2 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyVjDR0wU1zr"
      },
      "source": [
        "The process to split data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qemLd_hud_BY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "texts_train, texts_val, labels_train, labels_val = train_test_split(texts_train, labels_train, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5XO7q35en4t",
        "outputId": "251e4cb9-4a96-4946-e7cb-7d5217b0f9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------size--\n",
            "texts_train:  3267\n",
            "labels_train:  3267\n",
            "texts_val:  1089\n",
            "labels_val:  1089\n",
            "texts_test 1089\n",
            "labels_test 1089\n"
          ]
        }
      ],
      "source": [
        "print(\"--------------size--\")\n",
        "print(\"texts_train: \", len(texts_train))\n",
        "print(\"labels_train: \", len(labels_train))\n",
        "print(\"texts_val: \", len(texts_val))\n",
        "print(\"labels_val: \", len(labels_val))\n",
        "print(\"texts_test\", len(texts_test))\n",
        "print(\"labels_test\", len(labels_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laCKPaX3F81-"
      },
      "source": [
        "## **Part 2 Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQoEZAbiNFxg"
      },
      "source": [
        "In this part, the conversations stored in texts list will be used to do clustering. First, tokenization is conducted with spacy tools to remove stop words, whitespace and puctuation, also, to calculate similarity (the replacement of distance for k-means algorithm) between two conversations, lemma is used as the root of words. And then sparse TF-IDF vectors will be obtained from these lemmas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector and Method Preparation"
      ],
      "metadata": {
        "id": "XgF_fA4hZWqN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyQMg9QjWrJb"
      },
      "source": [
        "To calculate the sparse TF-IDF vectors, vocabulary of this corpus is needed: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "94t6E3ybRkNr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "def text_pipeline_spacy(conv_text):\n",
        "  tokens = []\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(conv_text)\n",
        "  for token in doc:\n",
        "    if not token.is_stop and not token.is_space and not token.is_punct:\n",
        "      tokens.append(token.lemma_.lower())\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HK6NMs6-iMdk"
      },
      "outputs": [],
      "source": [
        "stopword_text = \"\"\"\n",
        "A               CANNOT          INTO            OUR             THUS\n",
        "ABOUT           CO              IS              OURS            TO\n",
        "ABOVE           COULD           IT              OURSELVES       TOGETHER\n",
        "ACROSS          DOWN            ITS             OUT             TOO\n",
        "AFTER           DURING          ITSELF          OVER            TOWARD\n",
        "AFTERWARDS      EACH            LAST            OWN             TOWARDS\n",
        "AGAIN           EG              LATTER          PER             UNDER\n",
        "AGAINST         EITHER          LATTERLY        PERHAPS         UNTIL\n",
        "ALL             ELSE            LEAST           RATHER          UP\n",
        "ALMOST          ELSEWHERE       LESS            SAME            UPON\n",
        "ALONE           ENOUGH          LTD             SEEM            US\n",
        "ALONG           ETC             MANY            SEEMED          VERY\n",
        "ALREADY         EVEN            MAY             SEEMING         VIA\n",
        "ALSO            EVER            ME              SEEMS           WAS\n",
        "ALTHOUGH        EVERY           MEANWHILE       SEVERAL         WE\n",
        "ALWAYS          EVERYONE        MIGHT           SHE             WELL\n",
        "AMONG           EVERYTHING      MORE            SHOULD          WERE\n",
        "AMONGST         EVERYWHERE      MOREOVER        SINCE           WHAT\n",
        "AN              EXCEPT          MOST            SO              WHATEVER\n",
        "AND             FEW             MOSTLY          SOME            WHEN\n",
        "ANOTHER         FIRST           MUCH            SOMEHOW         WHENCE\n",
        "ANY             FOR             MUST            SOMEONE         WHENEVER\n",
        "ANYHOW          FORMER          MY              SOMETHING       WHERE\n",
        "ANYONE          FORMERLY        MYSELF          SOMETIME        WHEREAFTER\n",
        "ANYTHING        FROM            NAMELY          SOMETIMES       WHEREAS\n",
        "ANYWHERE        FURTHER         NEITHER         SOMEWHERE       WHEREBY\n",
        "ARE             HAD             NEVER           STILL           WHEREIN\n",
        "AROUND          HAS             NEVERTHELESS    SUCH            WHEREUPON\n",
        "AS              HAVE            NEXT            THAN            WHEREVER\n",
        "AT              HE              NO              THAT            WHETHER\n",
        "BE              HENCE           NOBODY          THE             WHITHER\n",
        "BECAME          HER             NONE            THEIR           WHICH\n",
        "BECAUSE         HERE            NOONE           THEM            WHILE\n",
        "BECOME          HEREAFTER       NOR             THEMSELVES      WHO\n",
        "BECOMES         HEREBY          NOT             THEN            WHOEVER\n",
        "BECOMING        HEREIN          NOTHING         THENCE          WHOLE\n",
        "BEEN            HEREUPON        NOW             THERE           WHOM\n",
        "BEFORE          HERS            NOWHERE         THEREAFTER      WHOSE\n",
        "BEFOREHAND      HERSELF         OF              THEREBY         WHY\n",
        "BEHIND          HIM             OFF             THEREFORE       WILL\n",
        "BEING           HIMSELF         OFTEN           THEREIN         WITH\n",
        "BELOW           HIS             ON              THEREUPON       WITHIN\n",
        "BESIDE          HOW             ONCE            THESE           WITHOUT\n",
        "BESIDES         HOWEVER         ONE             THEY            WOULD\n",
        "BETWEEN         I               ONLY            THIS            YET\n",
        "BEYOND          IE              ONTO            THOSE           YOU\n",
        "BOTH            IF              OR              THOUGH          YOUR\n",
        "BUT             IN              OTHER           THROUGH         YOURS\n",
        "BY              INC             OTHERS          THROUGHOUT      YOURSELF\n",
        "CAN             INDEED          OTHERWISE       THRU            YOURSELVES\n",
        "\"\"\"\n",
        "\n",
        "stopwords = set( [ s.lower().strip() for s in stopword_text.strip().split() ] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JCKnWrFiiAVS"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "# def tokenize_rulebased(text):\n",
        "\n",
        "#   tokens = re.findall(pattern, text)\n",
        "#   return tokens\n",
        "def tokenize_rulebased(text):\n",
        "  prev_c = ''\n",
        "  tokens = []\n",
        "  for c in text:\n",
        "    # Ignore characters that are whitespace (e.g. spaces, tabs, etc)\n",
        "    if not c in string.whitespace:\n",
        "      if prev_c in string.whitespace:\n",
        "        is_new_token = True\n",
        "      elif c in string.ascii_letters and not prev_c in string.ascii_letters:\n",
        "        is_new_token = True\n",
        "      elif c in string.punctuation and not prev_c in string.punctuation:\n",
        "        is_new_token = True\n",
        "      elif c in string.digits and not prev_c in string.digits:\n",
        "        is_new_token = True\n",
        "      else: # This is continuation of a token\n",
        "        is_new_token = False\n",
        "      if is_new_token:\n",
        "        tokens.append(c)\n",
        "      else:\n",
        "        tokens[-1] += c\n",
        "    prev_c = c \n",
        "  return tokens\n",
        "\n",
        "def simple_stem_with_morerules(tokens):\n",
        "  stems = []\n",
        "  for item in tokens:\n",
        "    if item not in string.punctuation:\n",
        "      if item.endswith(\"ing\"):\n",
        "        item = item[:-3]\n",
        "      if item.endswith(\"ed\"):\n",
        "        item = item[:-2]\n",
        "      if item.endswith(\"s\") and len(item) > 3:\n",
        "        item = item[:-1]\n",
        "      stems.append(item)\n",
        "    else:\n",
        "      pass\n",
        "  return stems\n",
        "\n",
        "def lowercase_tokens(tokens):\n",
        "  return [ token.lower() for token in tokens]\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "  new_token = []\n",
        "  for token in tokens:\n",
        "    if token not in stopwords:\n",
        "      new_token.append(token)\n",
        "  return new_token\n",
        "\n",
        "def text_pipeline(text):\n",
        "  tokens = tokenize_rulebased(text)\n",
        "  stemmed = simple_stem_with_morerules(tokens)\n",
        "  lowercased = lowercase_tokens(stemmed)\n",
        "  nostopwords = remove_stopwords(lowercased)\n",
        "  return nostopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_tLmjkd4NGI5"
      },
      "outputs": [],
      "source": [
        "def make_vocabulary(texts):\n",
        "  unique_tokens = sorted(set( t for token_list in texts for t in token_list ))\n",
        "  \n",
        "  token_to_id = { v:i for i,v in enumerate(unique_tokens)}\n",
        "  \n",
        "  return token_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxu28cVHW_N5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "conva_token_all = []\n",
        "example =   [\n",
        "    \"My favourite soft drink is Apple Tango, but I also love Irn Bru doesn't.\",\n",
        "    \"Irn Bru is a great drink.\",\n",
        "  ]\n",
        "for conva in tqdm(texts):\n",
        "  #这时候每一个conva都是一长段的字符串\n",
        "  token_list = text_pipeline_spacy(conva)\n",
        "  #之后数据处理用的都是token，这里提前处理了\n",
        "  conva_token_all.append(token_list)\n",
        "vocabularies = make_vocabulary(conva_token_all)\n",
        "# print(vocabularies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Uq1t76oxfR",
        "outputId": "c42af0e6-5022-4b1c-f6ba-203a553cf38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5445/5445 [51:10<00:00,  1.77it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm \n",
        "conva_token_all = []\n",
        "example =   [\n",
        "    \"My favourite soft drink is Apple Tango, but I also love Irn Bru doesn't.\",\n",
        "    \"Irn Bru is a great drink.\",\n",
        "  ]\n",
        "for conva in tqdm(texts):\n",
        "  #这时候每一个conva都是一长段的字符串\n",
        "  token_list = text_pipeline_spacy(conva)\n",
        "  #之后数据处理用的都是token，这里提前处理了\n",
        "  conva_token_all.append(token_list)\n",
        "vocabularies = make_vocabulary(conva_token_all)\n",
        "# print(vocabularies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQNk8OMi58Y-",
        "outputId": "e605e45f-9b09-4a29-c230-e1750d3dc065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=5d9dc05f6b6340f943cdc853210a1608d53bad7c662a590be09b51448844c4c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "f9DC5qryNeAS"
      },
      "outputs": [],
      "source": [
        "def make_tf_sparse(tokens, vocab):\n",
        "  counts = Counter(tokens)\n",
        "  dic = {}\n",
        "  for item in counts.keys():\n",
        "    dic[vocab[item]] = counts[item]\n",
        "  return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "g44-mPsSNlBk"
      },
      "outputs": [],
      "source": [
        "def doc_frequency(corpus):\n",
        "  dic = {}\n",
        "  for tokens in corpus:\n",
        "    counts = Counter(tokens)\n",
        "    for item in counts.keys():\n",
        "      if item not in dic.keys() or len(dic.keys()) == 0:\n",
        "        dic[item] = 1\n",
        "      else:\n",
        "        dic[item] += 1\n",
        "  return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "IOHnGzvPN8yb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def make_tfidf_sparse(tokens, vocab, doc_freq, N):\n",
        "  sparse_vector = {}\n",
        "  counts = Counter(tokens)\n",
        "  for t,c in counts.items():\n",
        "    tf = 1 + math.log10(c) if c > 0 else 0\n",
        "    idf = math.log10(N / doc_freq[t])\n",
        "    sparse_vector[vocab[t]] = tf*idf\n",
        "      \n",
        "  return sparse_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "b-q1y0OOUav4"
      },
      "outputs": [],
      "source": [
        "def normalize_sparse_vector(sv):\n",
        "  d = math.sqrt( sum( val*val for index,val in sv.items() ) )\n",
        "  norm_vector = { index:val/d for index,val in sv.items() }\n",
        "  return norm_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "L-LoNM1l8s9o"
      },
      "outputs": [],
      "source": [
        "doc_freq = doc_frequency(conva_token_all) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CQG6eLpHgOke"
      },
      "outputs": [],
      "source": [
        "cluster_texts = texts\n",
        "cluster_labels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "LAb94biW_cTV"
      },
      "outputs": [],
      "source": [
        "N = len(texts)\n",
        "sparse_tfidf_vec = [make_tfidf_sparse(tokens, vocabularies, doc_freq, N) for tokens in conva_token_all]\n",
        "nor_vec = [normalize_sparse_vector(v) for v in sparse_tfidf_vec]\n",
        "# for item in nor_vec:\n",
        "#   if len(item.items()) == 0:\n",
        "#     index = nor_vec.index(item)\n",
        "#     nor_vec.remove(item)\n",
        "    # cluster_labels.remove(cluster_labels[index])\n",
        "    # cluster_texts.remove(cluster_texts[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb0n99yCQH4N",
        "outputId": "a52f26bd-89b4-4abb-c01c-a2c983ea3890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5390\n",
            "5445\n"
          ]
        }
      ],
      "source": [
        "print(len(texts))\n",
        "print(len(nor_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7yp1s-OuUVGA"
      },
      "outputs": [],
      "source": [
        "def sparse_dot_prod(sv1, sv2):\n",
        "  dot_prod = 0\n",
        "  indices = set(sv1).intersection(sv2)\n",
        "  for i in indices:\n",
        "    dot_prod += sv1.get(i,0) * sv2.get(i,0)\n",
        "  return dot_prod\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "_h9l10obOJGu"
      },
      "outputs": [],
      "source": [
        "def sparse_cosine_similarity(sv1, sv2):\n",
        "  # print(sv1, sv2)\n",
        "  d1 = math.sqrt( sum( val*val for index,val in sv1.items() ) )\n",
        "  d2 = math.sqrt( sum( val*val for index,val in sv2.items() ) )\n",
        "  sim = 0\n",
        "  if d1 == 0 or d2 == 0:\n",
        "    return sim\n",
        "  else:\n",
        "    return sparse_dot_prod(sv1,sv2) / (d1*d2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means Implementation"
      ],
      "metadata": {
        "id": "O7fT2FoPZho1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "KCoHZgvQqOJ3"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xaKtPluFqPzN"
      },
      "outputs": [],
      "source": [
        "def calculate_centroid(centroids, current_clusters, k):\n",
        "  # if len(centroids == 0):\n",
        "  #   centroids = random.sample(data_points, k)\n",
        "  #   return centroids\n",
        "  #base on similarity to find a new data point whose similarity \n",
        "  #is the closest one to mean value of similarities in this class\n",
        "  # else:\n",
        "    new_centroids = []\n",
        "  # for cluster in current_clusters:\n",
        "    for i in range(k):\n",
        "      similarities = {}\n",
        "      for data_point in current_clusters[i]:\n",
        "        similarities[current_clusters[i].index(data_point)] = sparse_cosine_similarity(centroids[i], data_point)\n",
        "      average = sum(similarities.values()) / len(similarities)\n",
        "      #likeliest = max(candidate_tokens, key=lambda t:unigram_token_prob(t, unigram_counts, unigram_N))\n",
        "      a = min(similarities.keys(), key=lambda x: (similarities[x] - average))\n",
        "      # print(a, len(current_clusters[i]))\n",
        "      new = current_clusters[i][a]\n",
        "      new_centroids.append(new)\n",
        "    return new_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "W_Ac1k23tNaZ"
      },
      "outputs": [],
      "source": [
        "def assign_data_points(data_points, centroids):\n",
        "  #here we use similarity between data_points and centroids\n",
        "  assigned_clusters = [ [] for _ in range(len(centroids))]\n",
        "  for item in data_points:\n",
        "    max_sim = -math.inf\n",
        "    center_index = 0\n",
        "    for cent in centroids:\n",
        "      similarity = sparse_cosine_similarity(item, cent)\n",
        "      if similarity > max_sim:\n",
        "        max_sim = similarity\n",
        "        center_index = centroids.index(cent)\n",
        "    #通过上面的for循环找到了similarity最大的，以及对应的center\n",
        "    assigned_clusters[center_index].append(item)\n",
        "  return assigned_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt5Aq7I4qvRA"
      },
      "outputs": [],
      "source": [
        "# def k_means(data_points, k):\n",
        "#   if len(centroids == 0):\n",
        "#     centroids = random.sample(data_points, k)\n",
        "#   #assignment of data_points\n",
        "#   cur_assigned_clusters = assign_data_points(data_points, centroids)\n",
        "#   #calculate new centeroids\n",
        "#   calculate_centroid\n",
        "#   return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cZpXOjv9T_t_"
      },
      "outputs": [],
      "source": [
        "def isConvergence(last_cen, cen, k):\n",
        "  change = []\n",
        "  for i in range(k):\n",
        "    dis = sparse_cosine_similarity(last_cen[i], cen[i])\n",
        "    change.append(dis)\n",
        "  mean = sum(change) / k\n",
        "  if mean < 2E-3:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUA5yIifqbmn"
      },
      "outputs": [],
      "source": [
        "max_run = 2000\n",
        "k = 5\n",
        "data_points = nor_vec\n",
        "last_centroids = None\n",
        "centroids = []\n",
        "result = None\n",
        "# cur_assigned_clusters = []\n",
        "for i in range(max_run):\n",
        "  if len(centroids) == 0:\n",
        "    centroids = random.sample(data_points, k)\n",
        "  #assignment of data_points\n",
        "  last_centroids = centroids\n",
        "  cur_assigned_clusters = assign_data_points(data_points, centroids)\n",
        "  #calculate new centeroids\n",
        "  centroids = calculate_centroid(centroids, cur_assigned_clusters,k)\n",
        "  if isConvergence(last_centroids, centroids, k):\n",
        "    result = cur_assigned_clusters\n",
        "  elif i - max_run == 1:\n",
        "    result = cur_assigned_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "4mb-GDTQpec5"
      },
      "outputs": [],
      "source": [
        "max_run = 2000\n",
        "k = 5\n",
        "data_points = nor_vec\n",
        "last_centroids = None\n",
        "centroids = []\n",
        "result = None\n",
        "# cur_assigned_clusters = []\n",
        "for i in range(max_run):\n",
        "  if len(centroids) == 0:\n",
        "    centroids = random.sample(data_points, k)\n",
        "  #assignment of data_points\n",
        "  last_centroids = centroids\n",
        "  cur_assigned_clusters = assign_data_points(data_points, centroids)\n",
        "  #calculate new centeroids\n",
        "  centroids = calculate_centroid(centroids, cur_assigned_clusters,k)\n",
        "  if isConvergence(last_centroids, centroids, k):\n",
        "    result = cur_assigned_clusters\n",
        "  elif i - max_run == 1:\n",
        "    result = cur_assigned_clusters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs2-6olv9QZw",
        "outputId": "06e896fa-da38-4b36-a165-b6ffe9c2769c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{3232: 0.35434744670591445, 5738: 0.3973246375944822, 7346: 0.22665799660864702, 6511: 0.30167552591670066, 4821: 0.3036030319530424, 6925: 0.34152120369175004, 3218: 0.6044646519255756}, {8018: 0.4817285192259219, 10365: 0.13078422297636855, 11259: 0.11808102194433333, 6226: 0.24836274925149493, 11655: 0.19044439123870088, 2479: 0.2197457275298275, 6186: 0.1741677593132616, 5141: 0.4817285192259219, 1283: 0.2670752124199708, 1844: 0.22582770970879734, 747: 0.1855249750801884, 2532: 0.3299013120092415, 10776: 0.2503401619368697}, {1384: 0.2798048050636484, 7133: 0.34797964702867973, 9165: 0.6220494765627816, 8892: 0.38037736237778214, 4247: 0.4811730760495557, 10480: 0.1935443569329476}, {2425: 0.6600270227699419, 6186: 0.3862237012931263, 8478: 0.6443567193510786}, {4718: 0.3070677585363081, 1691: 0.24849104416366818, 4248: 0.3625968535476582, 9841: 0.2958010063328611, 1823: 0.3995546455118135, 492: 0.38421577595614276, 11112: 0.3995546455118135, 10160: 0.1630975440997123, 2394: 0.3625968535476582}, {4583: 0.13225666190713983, 4710: 0.1261316211505558, 11631: 0.21196408814339573, 3768: 0.16388442202878262, 3095: 0.1617365742144154, 5945: 0.15155343600926702, 9704: 0.14742630760576883, 10157: 0.1041893795719532, 9987: 0.09758030574322536, 2582: 0.18363807317539296, 2304: 0.18737655102748804, 8015: 0.15616482672138307, 6058: 0.08708077451278616, 1006: 0.18248417583612014, 6609: 0.15789663872021245, 3796: 0.10981697260559932, 6226: 0.09136671095414957, 940: 0.13593828559949894, 4238: 0.1739077582340405, 10494: 0.14020659144063574, 11642: 0.09077367526880342, 5919: 0.1261316211505558, 9523: 0.1147093477969672, 5788: 0.1687767972201505, 8646: 0.12950432490082295, 10480: 0.09333275685207375, 5749: 0.10277674959986426, 7366: 0.0945460193605692, 7128: 0.2305638419507333, 85: 0.2305638419507333, 9196: 0.18737655102748804, 11632: 0.2305638419507333, 9750: 0.2305638419507333, 6205: 0.14698716290957853, 9068: 0.19336433433605818, 10133: 0.17160401733618214, 5029: 0.11072667913336016, 3934: 0.2305638419507333, 3874: 0.12558950629690524, 8725: 0.123524351221569, 11347: 0.1110368996125038}, {860: 0.703349531927257, 7356: 0.33470291114921985, 6697: 0.6271151387153286}, {9450: 0.22701803768208267, 11590: 0.191400111588504, 582: 0.3702864371677198, 10430: 0.1648356741936763, 5621: 0.3702864371677198, 7970: 0.21445660604551264, 747: 0.17004061096805317, 7772: 0.35882002607657515, 3902: 0.1963768178942242, 11464: 0.18073039560868187, 6451: 0.23028172518513476, 2462: 0.2959353399649735, 3193: 0.44152228935487714}, {8995: 0.578245624535648, 5968: 0.2791302917230236, 9874: 0.7666278614477811}, {7896: 0.8157378316388082, 7346: 0.5784218097834966}, {7303: 0.7677993590837798, 5968: 0.32054126946323025, 10122: 0.5547408753295885}, {4046: 0.12740887036371465, 1593: 0.1313281362003892, 3222: 0.13349417598729132, 3831: 0.1758154624420257, 8044: 0.26230880533084483, 7133: 0.11278581295881196, 4624: 0.09710015327575458, 2171: 0.06753415928646833, 4537: 0.11101555736910991, 5746: 0.16794436385202033, 6592: 0.11564994522808372, 8493: 0.12141676761197451, 1420: 0.136314181501042, 10533: 0.1520734212511633, 11458: 0.1796364873088168, 576: 0.16058208295785617, 825: 0.21317531331533293, 10674: 0.13997151506387565, 9919: 0.16169232731129646, 5757: 0.26230880533084483, 3157: 0.14053168408392924, 6158: 0.16058208295785617, 7748: 0.09550700807461301, 5049: 0.24114816210347761, 6076: 0.05980450506553451, 9101: 0.17407051009044539, 9351: 0.2287699793243287, 10019: 0.1489395587890556, 7151: 0.10377637484756627, 502: 0.26230880533084483, 1499: 0.16936454340607449, 2471: 0.26230880533084483, 6210: 0.11347705052815266, 1339: 0.26230880533084483, 1384: 0.11798943156038161}, {4325: 0.11152837499064629, 11334: 0.3474486840830462, 6919: 0.25717414609565276, 11375: 0.3474486840830462, 9553: 0.44306923907479734, 10861: 0.21435721108598368, 2546: 0.3474486840830462, 8142: 0.23017750194001946, 4579: 0.3085695567809698, 1338: 0.3916738283894803, 2171: 0.12408267309859032}, {10453: 0.2418912985980952, 2603: 0.34150105225818483, 6931: 0.3972711793311777, 11227: 0.1935576870980741, 2475: 0.2703283684359343, 1892: 0.4736983669579152, 2266: 0.4736983669579152, 7940: 0.32820565246307576}, {9331: 0.6441162142546517, 10453: 0.5408855251040905, 8307: 0.5408855251040905}, {3754: 0.42900121064220026, 9084: 0.23846262278989522, 6186: 0.20450972941595988, 7748: 0.267952768054608, 4332: 0.6293580236374381, 6451: 0.4993787155545909}, {7356: 0.336631961980177, 3969: 0.8289283142704584, 8892: 0.4467177766488735}, {3404: 0.3659688864336598, 8293: 0.33868735748269807, 2374: 0.4828163248828868, 4865: 0.25465296696125006, 6421: 0.38213424265069507, 4821: 0.24250301426876109, 5759: 0.4828163248828868, 2171: 0.12430613813982369}, {6186: 0.28934805625050963, 1862: 0.9572239562109026}, {8228: 0.3759986103761273, 11596: 0.5896562797635893, 2528: 0.33452970123111414, 10365: 0.22655212967476204, 8816: 0.5896562797635893}, {1809: 0.37925689627069964, 4872: 0.24227561002395281, 7005: 0.3133231494446778, 6506: 0.4522184889510778, 1168: 0.4522184889510778, 10448: 0.34980394578486695, 10776: 0.23500466598234893, 4583: 0.25940280702878066, 7688: 0.21310186795935207}, {10785: 0.24661344433884744, 10212: 0.5189325432555917, 10660: 0.3040169489251166, 4430: 0.5156407222085277, 9260: 0.29426387097117046, 1898: 0.40811374161552777, 1844: 0.2417252844321035}, {4826: 0.20923777840867275, 6058: 0.1940852386887932, 10981: 0.363115845264573, 8456: 0.3209949161910739, 3900: 0.1407970111915815, 2009: 0.3312525924625451, 3563: 0.28913166338904606, 10045: 0.31261378442236054, 9256: 0.191299072033794, 6644: 0.26211172358014806, 7334: 0.23862960254683355, 10785: 0.14519656599985348, 9499: 0.29397497638217596, 8099: 0.3209949161910739}, {9827: 0.3469474366840262, 3942: 0.2921586067691022, 8042: 0.33247514330155603, 9515: 0.20896555472115685, 423: 0.413359864017015, 7399: 0.24998211421851843, 10535: 0.43572575295698324, 10122: 0.29865564496505037, 8682: 0.2863468080600698, 6758: 0.21368304925958298}, {4475: 0.10129772747853696, 10600: 0.09364579424521205, 6234: 0.13275003981809758, 6058: 0.07830927056324168, 5601: 0.0962128497601916, 6769: 0.1287188826642116, 9365: 0.20527801572480195, 9411: 0.1808290743906656, 10480: 0.08393150094994999, 11347: 0.09985233437470561, 9713: 0.1055192134545923, 4372: 0.2262322825949258, 2722: 0.07830927056324168, 8793: 0.11893466472966056, 6018: 0.08228741011784724, 5812: 0.03791479538931735, 3: 0.14199196862623228, 10742: 0.16850241759694184, 6076: 0.04727190747375486, 4826: 0.08442299843178375, 8516: 0.30626561690053117, 10767: 0.11108194541308804, 4325: 0.062424384252191954, 5099: 0.15167221045100965, 11680: 0.14510778173504407, 11315: 0.06898912174960369, 5892: 0.20733952336137512, 4012: 0.16297448203365913, 7429: 0.11392415024603172, 6570: 0.06814503137999102, 10365: 0.056290498429865235, 8945: 0.14365682529838453, 10453: 0.10587671407583495, 11168: 0.20733952336137512, 1813: 0.19061329232521665, 4935: 0.16410284335450712, 10133: 0.15431862541995606, 11531: 0.19061329232521665, 622: 0.1738870612890582, 10910: 0.08988177689977832, 11511: 0.11293908079635005, 11262: 0.160383056334543, 7037: 0.16850241759694184, 5043: 0.20733952336137512, 9731: 0.16850241759694184, 3660: 0.20733952336137512, 6875: 0.15431862541995606}, {8803: 0.14340822817336332, 6018: 0.21186913791588516, 6451: 0.2784346260329455, 6274: 0.5338465022480686, 3900: 0.19029865711763286, 5682: 0.3907850515253887, 7697: 0.3446878917699155, 9461: 0.35781669451553605, 7346: 0.15386124020621117, 1063: 0.2860077377448768, 6570: 0.17545611207173273}, {4710: 0.5217167499423988, 585: 0.7548071630626014, 895: 0.39758996393134466}, {2528: 0.2968265910627371, 2171: 0.14652326627348555, 4947: 0.24670929751880513, 8892: 0.3480051705595009, 5788: 0.41659831011403076, 10064: 0.32314181240539847, 11562: 0.2886642967311848, 6186: 0.1581517711680728, 10279: 0.2350592072106669, 10579: 0.4235768471052432, 1420: 0.29574957804992474}, {2659: 0.3352388161097689, 11227: 0.2295862382669536, 7483: 0.5618719037904273, 1283: 0.31150752357110373, 1832: 0.5165452862542956, 3980: 0.39414348797073506}, {10776: 0.1827975058891154, 9800: 0.2098742115262058, 6058: 0.13285355434210885, 8139: 0.26662711518418003, 729: 0.22711808690603907, 2252: 0.3517564706711414, 3054: 0.3517564706711414, 9205: 0.1827975058891154, 7046: 0.26180524656621174, 9528: 0.2252136241602181, 4960: 0.30678085861867654, 4032: 0.2252136241602181, 2251: 0.3517564706711414, 9317: 0.3233800188421543, 8803: 0.09449302748081559}, {6117: 0.1691026990242702, 5812: 0.072207217942884, 1163: 0.16429452924942492, 10480: 0.09443266944305159, 4577: 0.31866128501490726, 10483: 0.07033052581184163, 2171: 0.10166339766135288, 8803: 0.08153129691910094, 4325: 0.070234669659828, 8889: 0.3759828206536841, 3321: 0.19949925077382571, 9360: 0.24665545385775106, 4448: 0.0863234182500431, 5749: 0.13529145285025582, 1559: 0.34438177104380957, 9697: 0.2221714365576447, 3900: 0.10818972184781318, 5758: 0.12066859802397391, 7748: 0.1105068127114378, 11037: 0.34438177104380957, 4818: 0.1302367059947212, 1007: 0.2545375469023635, 8415: 0.18708683363954637, 4795: 0.2646993322148996, 10288: 0.10347299642669124, 7356: 0.10749573154195718}, {10480: 0.0889135667955459, 1844: 0.13396375737396005, 4862: 0.12141550781413521, 4448: 0.08127825952462057, 6592: 0.12599259182925787, 11547: 0.10362814599760627, 4680: 0.14262038926397233, 4406: 0.24922902099912647, 4478: 0.2627141923704186, 4402: 0.2627141923704186, 4204: 0.19355503419531875, 6537: 0.27671710657222, 10666: 0.21660808692035202, 8819: 0.22617596827409317, 9107: 0.20918666157524415, 4527: 0.28576724509545187, 766: 0.24922902099912647, 2918: 0.28576724509545187, 11008: 0.2627141923704186, 10061: 0.1501543647159628, 338: 0.22617596827409317, 4687: 0.28576724509545187}, {11259: 1.0}, {10453: 0.28934758330764604, 7356: 0.20069014106666028, 472: 0.5209218661726812, 1073: 0.46049565986954627, 10220: 0.4147849994056096, 6527: 0.20659173485374183, 11649: 0.42173316119954435}, {11640: 0.13516727423270852, 6693: 0.388703092284259, 6117: 0.1908659155364713, 1780: 0.27473144776324077, 6015: 0.35274904148836905, 6058: 0.16833053935063444, 3739: 0.4067580730919069, 6825: 0.23418406753392387, 1401: 0.4097348637488782, 3716: 0.36220619387678404, 11545: 0.21585920922272864}, {11640: 0.40927466719369204, 8185: 0.9124112268015407}, {4865: 0.25266056851427304, 2171: 0.12333357002277057, 9395: 0.3565389037599712, 10461: 0.4017499640197442, 3481: 0.34535109163571603, 2554: 0.3705502102003362, 4017: 0.28758679352296773, 10314: 0.47903878202779726, 6592: 0.2112045336555392, 7346: 0.13806496961072184}, {9697: 0.5276963560937539, 11464: 0.29508091439721895, 5399: 0.720879298862842, 8892: 0.33881683280925184}, {4521: 0.4830428042451937, 5389: 0.44407538783960504, 9562: 0.4212809104923493, 8972: 0.4830428042451937, 2394: 0.3823134940867607, 4325: 0.11178164045536727}, {11680: 0.28626215158694357, 6371: 0.17114722275036068, 11315: 0.1770684370430257, 10480: 0.1655763168630385, 8414: 0.4892307206656876, 10458: 0.5321604973041348, 270: 0.4463009440272404, 10910: 0.23069181561780214, 9515: 0.2346254914628137}, {9193: 0.6452610466928569, 6332: 0.5894443865493548, 5092: 0.4859974246704333}, {4862: 0.36899763210969266, 2656: 0.46343481945048504, 9359: 0.6717973033778158, 7346: 0.2503079791137543, 4537: 0.3675638915680096}, {6186: 0.08956816014800781, 6052: 0.1740936729420442, 9595: 0.2963102283057301, 10529: 0.2550994759433191, 9331: 0.19599851787930062, 5197: 0.38550849504779733, 11359: 0.1358854540833773, 8803: 0.0865831186675973, 4680: 0.16085876336773866, 2924: 0.2493170306180807, 7662: 0.2703091384732198, 6180: 0.13264322875221565, 9289: 0.2359375673259343, 4325: 0.07458653262648778, 3636: 0.1748220772534316, 6130: 0.1361468603214499, 4818: 0.13830640007121175, 3120: 0.3223113181382404, 1508: 0.19599851787930062, 2538: 0.2398898134134185, 1118: 0.2963102283057301}, {213: 0.14818044906240138, 10854: 0.1521044614911457, 215: 0.17517436608016895, 11657: 0.19994102680488487, 9084: 0.09569792138645215, 440: 0.22686912238811527, 1577: 0.19994102680488487, 10861: 0.08892820035839344, 3675: 0.12324759707672153, 11315: 0.06652738280362319, 10910: 0.08667452530537191, 384: 0.19994102680488487, 9003: 0.19994102680488487, 6896: 0.09886158949716685, 9897: 0.09826489553740557, 3213: 0.12240132965633817, 10686: 0.22686912238811527, 9902: 0.17437655022883042, 6893: 0.1624897455499514, 5865: 0.19994102680488487, 4449: 0.18381163789857155, 4325: 0.04626864487817898, 9703: 0.19994102680488487, 4376: 0.12010963662037664, 8986: 0.16768224899225823, 10365: 0.0542818844808951, 4738: 0.19994102680488487, 7335: 0.19994102680488487, 6371: 0.06430268993069356, 10552: 0.14025519546661872, 9347: 0.26012927323701135, 4152: 0.08667452530537191, 8803: 0.05371054839266809, 1728: 0.12324759707672153, 7041: 0.148812073652776, 3988: 0.14211777241620377, 438: 0.18381163789857155, 3850: 0.22686912238811527, 4739: 0.19994102680488487}, {8803: 0.08533865774160991, 4325: 0.07351449887571825, 10160: 0.11309555594016975, 2171: 0.08178972869251856, 1844: 0.1489234234284366, 6720: 0.13799937643784832, 1525: 0.29205135536639254, 3345: 0.2664239797269904, 9080: 0.17455079906783877, 5609: 0.23128127406866833, 3115: 0.33589186766585455, 6058: 0.11998286334999059, 7758: 0.20511516248475264, 7066: 0.4133095580230082, 11585: 0.220547552545536, 7346: 0.09155898434073473, 8950: 0.1341900187283864, 9993: 0.16951765392079177, 9729: 0.12105460470182258, 6021: 0.20511516248475264, 10480: 0.0988425005090184, 4698: 0.25817380751043484, 3232: 0.18622866836974003, 1566: 0.1538604112059483, 8373: 0.2258055503466431}, {4418: 0.1606147562895099, 7772: 0.2808678820355239, 3582: 0.23167378434739466, 3610: 0.17151430804444343, 6371: 0.11114892369911959, 970: 0.17302269769264633, 5936: 0.1687018377267046, 6697: 0.17628000813324446, 8558: 0.1687018377267046, 10305: 0.17628000813324446, 6192: 0.2656383227297789, 2113: 0.23167378434739466, 9592: 0.1888153086838627, 3900: 0.09469129399852722, 11680: 0.14289335302750697, 3942: 0.16374470758262621, 9528: 0.1700761019795031, 5446: 0.19770924596501047, 3933: 0.23167378434739466, 11590: 0.11515433272225377, 717: 0.174608473752094, 10343: 0.2656383227297789, 9704: 0.13055310583778404, 2171: 0.0889792349256782, 1984: 0.23667899881116136, 4599: 0.11867701693945898, 3822: 0.10892881464512275, 2457: 0.22277984706624693, 8633: 0.20547884642682762}, {4325: 0.165481882948502, 2195: 0.5053000827618497, 6593: 0.37640897244426513, 6210: 0.2377789758930086, 7748: 0.20012468128921576, 6058: 0.20759140358959813, 8943: 0.4166203708302742, 4745: 0.28563173193305874, 8957: 0.425162225838758}, {7346: 0.1393140085836513, 10160: 0.22388586302829172, 4704: 0.1831014614747391, 6201: 0.40538449466027177, 5006: 0.3207704061065894, 10365: 0.13123055327407293, 8803: 0.12984930515270635, 11257: 0.27048736543680957, 7640: 0.2787347597358903, 239: 0.36639047943050995, 8793: 0.27727347050206413, 11499: 0.4833725251197953}, {1702: 0.35129819041068716, 5608: 0.7355522913951053, 11227: 0.3005537071542701, 8803: 0.197592848119501, 6826: 0.40779770478611327, 10365: 0.19969470573005632}, {7336: 0.2508627180366835, 6923: 0.18464874506619294, 10147: 0.46824671816322516, 9704: 0.23012893142208346, 1204: 0.2685969637271847, 4634: 0.37060284790987547, 6128: 0.16479376167601859, 8155: 0.3375708216941979, 3088: 0.37060284790987547, 4421: 0.2672172423971882, 4187: 0.2672172423971882}, {1956: 0.8184646717342199, 9729: 0.4910933222330495, 4325: 0.29823301289612736}, {7366: 0.16025290290338662, 2368: 0.35927330062099805, 8493: 0.23534607719436407, 8022: 0.32774722939057604, 9730: 0.21766281994951714, 9244: 0.3408317311561371, 4828: 0.3408317311561371, 7356: 0.13841348206951892, 7264: 0.32774722939057604, 10430: 0.14589903949745514, 6527: 0.14248373754638596, 9336: 0.28607203010966104, 10252: 0.39079937185142005}, {10160: 0.30898333882208795, 10279: 0.3584750758603497, 5983: 0.6353297647582794, 6069: 0.48340156897596787, 4818: 0.37242976444026743}, {4448: 0.08884072537082396, 7122: 0.18524754868244345, 5294: 0.20531691757553566, 7067: 0.10661685381014353, 4475: 0.15260464586877248, 1327: 0.3123562129650622, 7133: 0.13430486775907607, 4624: 0.11562645072965573, 2797: 0.2472203169758107, 10348: 0.21911251734932696, 6570: 0.10266023375167342, 8493: 0.14458257195712887, 9768: 0.3123562129650622, 2910: 0.2619602045347041, 8942: 0.3123562129650622, 11358: 0.2619602045347041, 4826: 0.1271829299585425, 1136: 0.22530388609649094, 6469: 0.1925425376428449, 10483: 0.07238145865281868, 6186: 0.08680170298605656, 9396: 0.2871582087498832, 6180: 0.12854632858635837, 10564: 0.0920038530667082, 1844: 0.1464283001150945, 6210: 0.13512799052518418, 11642: 0.12297557675996035}, {7586: 0.5333715306646776, 9496: 0.33355814041145543, 10785: 0.19607041241847756, 10234: 0.3539502011509658, 10708: 0.4903440590790609, 1846: 0.44731658749344416}, {9934: 0.5247364104966923, 10249: 0.8512647646291058}, {1057: 0.40074151894241933, 8803: 0.19417386895077993, 9704: 0.46218612511797325, 1047: 0.34427967345213817, 4325: 0.1672699694187466, 3259: 0.664514103040095}, {2656: 0.4626711054299925, 316: 0.4764082636262824, 7346: 0.2498954859106821, 4455: 0.7046437828223873}, {6186: 0.16379531919481694, 8803: 0.15833650636126467, 11259: 0.18796995790301557, 11315: 0.19612001153803355, 6511: 0.29416605686176206, 3328: 0.3183690351819116, 10886: 0.4943204319965233, 6247: 0.5140549590503083, 4624: 0.21818778610449635, 576: 0.36083412833676537}, {6923: 0.3001265253993205, 6557: 0.7610842981986433, 1112: 0.4795803141412286, 3636: 0.31729715106727635}, {4475: 0.2014334409841828, 8787: 0.28566572381249566, 8558: 0.26184425465814515, 4771: 0.25146418925337566, 3822: 0.1690697900265521, 6252: 0.2598020362613088, 10610: 0.2142605106776984, 6698: 0.3125188255825408, 3900: 0.14697155427450828, 7741: 0.30181130692071706, 1612: 0.30181130692071706, 8803: 0.11075711468296835, 7489: 0.22000915801770707, 1520: 0.26855071602519365, 4905: 0.21199308621760374, 4543: 0.26855071602519365, 6069: 0.2296383191737763}, {4631: 0.5280626286189901, 4865: 0.33209860835596355, 5609: 0.35234270447489074, 4821: 0.3162535843261419, 11454: 0.32840299000500117, 5658: 0.5280626286189901}, {3697: 0.3135524163460771, 4246: 0.31837688863892066, 4967: 0.2482045445233274, 3900: 0.15503841339064248, 5904: 0.43493062933976717, 9539: 0.31837688863892066, 4807: 0.43493062933976717, 10717: 0.39984445728197054, 924: 0.2740618461609193}, {5520: 0.20252945532651898, 4241: 0.22484441482354692, 3466: 0.3163265396498557, 6153: 0.22804774869999392, 5936: 0.20089295856210274, 3483: 0.2758810015704967, 4090: 0.23543546349113784, 6494: 0.22189763331147241, 4579: 0.20252945532651898, 2600: 0.2758810015704967, 7870: 0.29080824627638074, 6851: 0.1609848769667023, 8844: 0.2758810015704967, 4930: 0.15689009238559656, 10995: 0.2652899529029058, 6371: 0.10173323464925288, 3469: 0.16153029605808925, 7205: 0.3163265396498557}, {2384: 0.5913103200363485, 4862: 0.2512332816833902, 4679: 0.5157053325692182, 10365: 0.16053452859325812, 5263: 0.5436088839249389}, {10483: 0.12497453331926503, 4448: 0.15339326396030495, 6076: 0.0945100652851629, 4367: 0.17612408286563308, 11533: 0.24300512370705485, 11547: 0.1503219193452115, 4418: 0.16968193120275096, 5812: 0.07580252160350043, 1638: 0.2347674465170197, 9223: 0.15520354330239544, 4325: 0.09592722779979246, 10829: 0.38109049675809764, 1499: 0.26764963670813124, 4423: 0.4145310171788085, 10480: 0.12897710258025713, 9571: 0.3368845333063054, 6226: 0.16426832285436485, 7346: 0.11947302480861098, 11640: 0.12571779518950427, 8646: 0.17896281177886653, 2143: 0.27247811041979875, 5749: 0.1847824454758481}, {9729: 0.2702120011733896, 4308: 0.7091064883218466, 9223: 0.2654948242842744, 5707: 0.5946982099518991}, {6923: 0.35595942846816175, 2531: 0.7570316810229357, 11655: 0.2742883751635305, 6825: 0.47430138779299197}, {10747: 0.14282419393488655, 9987: 0.09018380565192677, 8950: 0.09000977127760904, 8215: 0.15146212909512408, 5968: 0.0775853912615181, 10883: 0.2130873084940101, 2622: 0.11370626073025238, 6277: 0.34137865159801273, 2351: 0.2130873084940101, 5376: 0.16482913266278623, 8145: 0.17870749035840092, 9846: 0.15146212909512408, 576: 0.16971847111366245, 1742: 0.10568600267568039, 4325: 0.04931084511423269, 5464: 0.13427222002731948, 11019: 0.1050418158971908, 3192: 0.1155788326716097, 11593: 0.18584194723073325, 5332: 0.18584194723073325, 4283: 0.17870749035840092, 6355: 0.09888002473221186, 9511: 0.14592821441763873, 3802: 0.2130873084940101, 3525: 0.2130873084940101, 8254: 0.19589739942620552, 6648: 0.2130873084940101, 6401: 0.19589739942620552, 529: 0.2130873084940101, 294: 0.18584194723073325, 2352: 0.19589739942620552, 10154: 0.16865203816292865, 2161: 0.2130873084940101, 2960: 0.10015208799819506}, {4160: 0.25538031391606, 6713: 0.5272889459532326, 10219: 0.40528577181968084, 6234: 0.25948599415365325, 9246: 0.23758540383304647, 7346: 0.11680842944088653, 4418: 0.165897531430554, 6527: 0.14776541546027577, 6069: 0.2257312839623177, 5553: 0.32937103000006535, 4823: 0.3398963723918763, 6210: 0.1753301188235607}, {4865: 1.0}, {9205: 0.43685053772291244, 7232: 0.5824368879817245, 11232: 0.3903620576611838, 4429: 0.5635125048726893}, {10361: 0.4715033677464819, 3822: 0.1933467371048493, 1202: 0.2718904594151427, 10002: 0.3266848299342356, 4475: 0.23035752603689572, 10600: 0.16368303578114585, 8646: 0.20355911851758302, 8207: 0.33075151239963596, 7995: 0.41121690731908533, 10767: 0.25260746484313956, 4332: 0.30992698103629546}, {9987: 0.25794697420156765, 4624: 0.14082758596687034, 11528: 0.18071107226899752, 10157: 0.17191467097335547, 4258: 0.2549910202617755, 1406: 0.349745210318427, 10480: 0.15400101479805414, 5812: 0.10275984889025613, 8528: 0.3317927196102635, 7896: 0.1546320310140018, 9290: 0.24563503755265673, 11128: 0.3317927196102635, 3536: 0.28836523917145207, 8142: 0.18169503031780532, 10288: 0.12970031546117547, 10610: 0.19770099702442906, 2171: 0.09794704026476755, 9223: 0.1424378103264995, 10822: 0.2605327798754952, 7151: 0.15051033244686945, 4537: 0.16100955994023233, 4412: 0.08348021414124779}, {6875: 0.25629763893242796, 8183: 0.344356555472684, 9084: 0.11158165387424838, 9576: 0.17767958393514416, 6018: 0.10504426422405821, 6592: 0.19752779986322702, 4127: 0.18852224520195443, 5812: 0.0629701859541804, 11347: 0.16583816420398784, 8803: 0.12035200588555539, 4870: 0.21081066222024278, 11315: 0.11457948752671228, 227: 0.344356555472684, 5771: 0.14301278749528595, 4880: 0.31657706021642734, 10483: 0.07979681129173204, 9741: 0.24156020787651986, 1359: 0.25629763893242796, 2171: 0.08865821503513653, 1480: 0.19964057041450442, 3615: 0.17831073619250004, 9223: 0.12892969489353007, 7896: 0.1821018826933498, 4: 0.19857177800779924, 7133: 0.148064164326076, 9929: 0.14927872212297075}, {4488: 0.20950090109873284, 7758: 0.19523215703051852, 8954: 0.25358694899321255, 6495: 0.25358694899321255, 5777: 0.181526481821032, 11379: 0.26371078402301473, 7858: 0.2250494608567838, 1812: 0.23389348019674944, 7067: 0.10320896915236563, 4624: 0.14562504813558136, 9672: 0.24573429247479878, 2728: 0.3023721071892457, 2171: 0.07784887748982613, 3594: 0.3023721071892457, 3754: 0.17626430266075066, 6226: 0.15589271724904705, 2713: 0.2280706934308503, 4771: 0.1417478885329319, 3618: 0.3023721071892457, 6720: 0.13135019178716437, 747: 0.11645060529397495, 4090: 0.2250494608567838}, {7346: 0.18219204720213958, 11023: 0.4493275649952516, 6834: 0.26753928548129574, 4110: 0.4791580703295593, 417: 0.39833197929953273, 8517: 0.5301536560252782, 11259: 0.15495098226306844}, {4947: 0.557904696406464, 10559: 0.7131375633251712, 6742: 0.4244728089080012}, {6681: 0.4640947513507943, 5673: 0.5829241945959027, 4421: 0.42030814572800884, 6069: 0.4102120990706017, 4818: 0.3160419933080668}, {3796: 0.40028512765477897, 9256: 0.4070332965362246, 10417: 0.5337283500550255, 6971: 0.569224010789822, 11655: 0.2553699002959034}, {10483: 0.03794727381231479, 4412: 0.05756851665583794, 10600: 0.09107539128908439, 9294: 0.08738365235494529, 4446: 0.26235066759788367, 4818: 0.07027002692994958, 4448: 0.046576338665703786, 4984: 0.1858133322752581, 11618: 0.1462538453812711, 2507: 0.11805749326685269, 11507: 0.12412681660680056, 3954: 0.1637583292872939, 9800: 0.09770580815313835, 6216: 0.13733732083363165, 10564: 0.06275474588400666, 3277: 0.11487365474965726, 6076: 0.037335711323274526, 3331: 0.13733732083363165, 3289: 0.12188202165700261, 1535: 0.1958672361856357, 9854: 0.12960967124531714, 4325: 0.060710950956980335, 8778: 0.1086715174301715, 10582: 0.1637583292872939, 4704: 0.06203163784233285, 6018: 0.049953668478686375, 7985: 0.08997815856482382, 5812: 0.04797431633373558, 6916: 0.09711933945170705, 9450: 0.08419981383774987, 5601: 0.07598963900981687, 8231: 0.14282017547214823, 11359: 0.06904000474967818, 1780: 0.1313309999405967, 11547: 0.059383894924834604, 917: 0.0681353528829256, 4644: 0.14282017547214823, 7475: 0.13308448837538647, 7133: 0.07041172816958588, 7336: 0.08773336361502587, 9985: 0.1637583292872939, 10019: 0.09298236588481922, 2945: 0.11639916701848604, 5666: 0.1637583292872939, 11545: 0.1031880624675998, 3817: 0.14282017547214823, 4447: 0.1637583292872939, 4008: 0.14282017547214823, 10480: 0.050951735719569145, 7838: 0.10806568660093052, 8950: 0.06917281872921607, 2248: 0.14282017547214823, 7217: 0.11639916701848604, 11642: 0.06447214480604867, 7366: 0.06715145809618875, 3981: 0.13308448837538647, 11280: 0.13308448837538647, 10479: 0.12960967124531714, 1074: 0.14282017547214823, 1057: 0.09078929071963923, 2480: 0.11346125099640568, 1763: 0.08808961191133438, 2130: 0.098310897988377, 11259: 0.040140348977494786, 10160: 0.058298958926029126, 6371: 0.05266603478037157, 2470: 0.09655037991911007, 1980: 0.1637583292872939, 1163: 0.0681353528829256, 1603: 0.13308448837538647, 7019: 0.10399979494647031, 4836: 0.13308448837538647, 8812: 0.1086715174301715, 1098: 0.13733732083363165, 11648: 0.14282017547214823}, {8335: 0.7668170061531705, 917: 0.4358522633745669, 1384: 0.47119474061746036}, {2569: 0.21380005731761548, 5771: 0.09323210363756719, 1053: 0.14494678703902522, 84: 0.22449101671529328, 6058: 0.08478715240574343, 1082: 0.13838069563616973, 7151: 0.08881464680979229, 5662: 0.1957875763555854, 109: 0.18827130760968627, 89: 0.22449101671529328, 6826: 0.12446011307770198, 2257: 0.22449101671529328, 8803: 0.06030546011335896, 8892: 0.10551188721549944, 2974: 0.1574765916383409, 11177: 0.20638116216248978, 9871: 0.18244123663481504, 6222: 0.1776777218027819, 1266: 0.29206954650370076, 8060: 0.20638116216248978, 4771: 0.10523830359156791, 9580: 0.18827130760968627, 10564: 0.066123347829742, 9741: 0.1574765916383409, 8020: 0.1957875763555854, 5946: 0.15046765917026067, 2415: 0.22449101671529328, 1963: 0.16708413599587757, 10106: 0.22449101671529328, 5968: 0.08173749759968284, 3289: 0.16708413599587757, 6076: 0.05118232355708892, 11423: 0.18827130760968627, 3117: 0.12746652915257894, 8646: 0.09691806380108738, 7922: 0.10635025852489846}, {11590: 0.27373642228518485, 10480: 0.1964710732015613, 4312: 0.5805159007847072, 11259: 0.1547821154847261, 11234: 0.3967640209221988, 8950: 0.26673199136739956, 7922: 0.2991455977830952, 2479: 0.28804551329028505, 4127: 0.2657112680108967, 10785: 0.2321267892918213}, {3192: 0.2743040461143969, 4517: 0.26619866236819983, 9067: 0.35735122160421395, 4785: 0.14702581357961783, 1849: 0.32599381194671956, 10459: 0.2893079944079483, 5609: 0.28299353625300216, 6143: 0.26931984325927044, 3788: 0.3887086312617083, 7259: 0.20660502394428173, 2577: 0.32599381194671956, 8969: 0.2488724071565154}, {1804: 0.4010912347750958, 5464: 0.31099092523205313, 11642: 0.1943067331477183, 1845: 0.4935363339511106, 1844: 0.23136305098429408, 2228: 0.33428058545559247, 4526: 0.3673291392562727, 2124: 0.4010912347750958}, {8259: 0.314215361496283, 8674: 0.3920478748218956, 8193: 0.4434923678523068, 6371: 0.1551466084665012, 7964: 0.4045761111895005, 1455: 0.2164981229751517, 5306: 0.4207278170361252, 5218: 0.3731568798247268}, {8235: 0.6795183220569044, 9592: 0.4830005716683876, 733: 0.5522366320310341}, {11259: 0.19707131535486683, 4173: 0.39245281173737584, 6923: 0.24368566319445123, 5995: 0.5389450956679245, 3587: 0.5022065830844522, 9084: 0.2002362946340569, 11659: 0.4061934795766238}, {4745: 0.5592393503630324, 11342: 0.7758177479770505, 10365: 0.2921612070918231}, {7067: 0.29301510393241476, 1068: 0.5032371476090434, 5867: 0.4950208173696572, 495: 0.49768521659130743, 6018: 0.2618654979971282, 10785: 0.31557059218128336}, {8778: 0.43377159503249696, 6371: 0.2102209525637493, 10494: 0.5171461756785657, 1499: 0.5490927538589564, 6876: 0.32222068310938823, 7688: 0.30802624314057103}, {2171: 0.3306196711687353, 6742: 0.5510421023347907, 3316: 0.766187466936605}, {}, {2656: 0.9235754356217328, 7346: 0.38341676373905}, {2702: 0.27478053913795963, 1795: 0.3041350724697466, 9205: 0.20851267382340755, 585: 0.3175691428697682, 2722: 0.1515428217035973, 6451: 0.20927200157807369, 9304: 0.2379614506341942, 3940: 0.4012400600192875, 8929: 0.3103706717681055, 10124: 0.3499374720529485, 11086: 0.3499374720529485, 5023: 0.2490946799168014}, {10368: 0.21967584944612545, 9912: 0.11552854274984507, 319: 0.20118450291660714, 6900: 0.27030761082084026, 3878: 0.14311612027794618, 10365: 0.09547695982403923, 1047: 0.12874682882383678, 1566: 0.1309172950336209, 6234: 0.17306563417269824, 2441: 0.17767767423006275, 6371: 0.0869331660554142, 7114: 0.1494195058965897, 11669: 0.21246910453489037, 11259: 0.0862031882102302, 206: 0.22669578538472918, 6714: 0.27030761082084026, 9628: 0.20488987266667363, 11374: 0.27030761082084026, 6801: 0.21967584944612545, 170: 0.27030761082084026, 4616: 0.15348129942569486, 7148: 0.3670671401015521, 10905: 0.23574605686872366, 11019: 0.13324867863845746, 9246: 0.15845891304713375}, {9573: 0.5445527512474537, 6958: 0.31236783921967537, 5812: 0.09957872868383014, 7748: 0.257958377588021, 6076: 0.12415407759321448, 7133: 0.23414320640489633, 8238: 0.45669381358735667, 6903: 0.5006232824174052}, {11260: 0.5207687016532162, 10162: 0.3490950808088999, 9681: 0.28856770421854866, 8142: 0.1911700816276973, 4653: 0.3490950808088999, 9515: 0.17647781887371644, 1844: 0.2771715081602444, 8788: 0.367983759866864, 11384: 0.3490950808088999}, {2734: 0.15251432094087133, 6371: 0.10463162643750916, 8892: 0.15291080605323495, 11464: 0.1331724285871203, 282: 0.22280115510834467, 10785: 0.11959636634694029, 8950: 0.1374256588537082, 865: 0.13340982110301375, 3775: 0.17500765730643197, 10365: 0.08832603838891662, 4412: 0.07139020336807675, 5999: 0.23454485580998838, 3049: 0.2728480947790752, 5156: 0.2728480947790752, 8654: 0.25165881251498695, 5855: 0.283740883665514, 2081: 0.21190836622190587, 11259: 0.07974684285230815, 10188: 0.2254134999700584, 7045: 0.23125025857565693, 3675: 0.20054521125867747, 9528: 0.20829954326505984, 6923: 0.12829430301377823, 6774: 0.24214304746209572, 7970: 0.15802381750698877, 6530: 0.2728480947790752, 4367: 0.1382284588679606}, {917: 0.18080497071362825, 6210: 0.1655798890015675, 3796: 0.2244823238358035, 5812: 0.0699904906787858, 5749: 0.17061456207656508, 6130: 0.12426739152087558, 1163: 0.12240360778502411, 10636: 0.22299099560912186, 11555: 0.13791134050938067, 5773: 0.18397839293016105, 8919: 0.21895827266916734, 4826: 0.1197853717260026, 4721: 0.2565731885519006, 9990: 0.20383055019360588, 4376: 0.1358356400338177, 9730: 0.16385341691693092, 5029: 0.14128178802444505, 7133: 0.12649306408003527, 11640: 0.1160785945669936, 4599: 0.13143196469017032, 4412: 0.06455471581285668, 4947: 0.12753067596898074, 4785: 0.11127420880661183, 11298: 0.14248326470236866, 1568: 0.33380941438917305, 7639: 0.1883552250172843, 3016: 0.2063679013297211, 5968: 0.10711430609527368, 1566: 0.14248326470236866, 3764: 0.15890316211271316, 11454: 0.15343765388863653, 10483: 0.06817141210400161, 6958: 0.16875298544698789, 7346: 0.08478869190229088, 7748: 0.10711430609527368, 6076: 0.06707275403770903, 9084: 0.09532559993779606, 1742: 0.1459099793984494, 3900: 0.10486834881963542, 8366: 0.14128178802444505}, {4152: 0.17961143379707167, 4227: 0.361352105440907, 3487: 0.32049467309846685, 8950: 0.17501549486584217, 6330: 0.361352105440907, 7535: 0.41432813589413886, 2908: 0.2698713463693046, 4704: 0.1569474541266481, 11547: 0.15024834824233402, 7985: 0.2276554900847008, 4418: 0.16959888485214988, 2673: 0.32792795160185545, 9987: 0.17535388826149856, 4012: 0.25031946975512437}, {4624: 0.15451144033745487, 4499: 0.33921702071056126, 10365: 0.1133200071785674, 9084: 0.17596461504718583, 10279: 0.17239906221372636, 356: 0.292799704894227, 5812: 0.07632735551355185, 5409: 0.41740110553446425, 6210: 0.18057131663384346, 10061: 0.21932043965477385, 10483: 0.12583981948114087, 10288: 0.18514042147705934, 870: 0.3055449681624845, 9693: 0.2033292803578334, 5771: 0.17334850943714053, 10480: 0.12987010133046198, 7787: 0.41740110553446425, 1455: 0.1873236739222801}, {10782: 1.0}, {11590: 0.6132339147044437, 1566: 0.6851337494247687, 6186: 0.3931105585651718}, {7356: 0.25185673232249167, 10634: 0.3505368374489778, 6130: 0.3003731446725943, 3967: 0.5390030784828423, 4325: 0.164556063227214, 3404: 0.5390030784828423, 5030: 0.3396185371292347}, {11547: 0.38492546088474805, 4872: 0.5686862653119763, 9025: 0.7269307540668374}, {1112: 0.25434417471204435, 4251: 0.32803278429100635, 1763: 0.320723405633605, 6371: 0.1298136712810735, 3928: 0.34518753610362, 161: 0.4580012703863378, 2722: 0.19834059053748898, 4599: 0.18033046463839555, 1393: 0.32803278429100635, 1083: 0.3710772218941216, 6250: 0.24881110059849645}, {7346: 0.2781408086517127, 11464: 0.39503033955925954, 8950: 0.5303606198613514, 6180: 0.3971562617946451, 10586: 0.5723401418805548}, {11590: 0.24690851120160667, 10483: 0.13198473027151755, 3900: 0.23051259881173322, 9652: 0.3671507077251577, 10423: 0.4024670073795156, 8892: 0.20576031768419384, 11296: 0.26281942558973626, 1012: 0.3318344080707997, 9966: 0.29051698614673793, 8803: 0.11760258449053167, 318: 0.3258332858010959, 8592: 0.38180829641748465}, {10533: 0.36337753544302176, 4247: 0.4848350599573833, 865: 0.25702163165667596, 10363: 0.4588166492962106, 9432: 0.39805832586409057, 4418: 0.2565642814310813, 2035: 0.36337753544302176}, {11511: 0.33327230651115514, 8501: 0.30911804039751906, 11315: 0.1564761980052607, 10480: 0.14632055817009926, 4412: 0.10319356839076425, 8545: 0.4101435897699966, 1484: 0.46376641302790506, 10483: 0.1089750173735287, 8049: 0.2940975982033578, 9729: 0.17920203594419676, 9897: 0.23112463775201933, 10610: 0.2443869073316414, 6076: 0.10721876386246532, 11655: 0.1428987020210274, 5812: 0.08599563061841482, 5914: 0.26570304292869706}, {6290: 0.27841796465485436, 2992: 0.2539869129447317, 6205: 0.19306981237307974, 5099: 0.17027966155003396, 6521: 0.199067679623184, 7362: 0.302849016364977, 1095: 0.12266904665354662, 11264: 0.230978669589861, 3548: 0.2641267155512531, 2132: 0.229555861234609, 4325: 0.07008273296291079, 3388: 0.2539869129447317, 6537: 0.22540441473752923, 1384: 0.1362248714619208, 5730: 0.27841796465485436, 625: 0.302849016364977, 11470: 0.2641267155512531, 7215: 0.302849016364977}, {10400: 0.1901760964867463, 6771: 0.27307029001700434, 2479: 0.13549443132557018, 9484: 0.29703208567258976, 2146: 0.29703208567258976, 9929: 0.1287635431184416, 3953: 0.22107499054449775, 6751: 0.2350917424529583, 10380: 0.29703208567258976, 2224: 0.22107499054449775, 7193: 0.2174327234803506, 8012: 0.1731513992333269, 576: 0.18183922938637265, 7688: 0.13997236700231186, 6234: 0.1901760964867463, 8667: 0.20580102504195807, 11655: 0.09025720570306882, 3636: 0.1611105887567045, 4991: 0.15116554500957965, 3087: 0.25905353810854376, 3140: 0.21413789214233173, 2411: 0.24139451913593604}, {2973: 0.5669438290767366, 8466: 0.5669438290767366, 4683: 0.5976197698734673}, {6923: 0.284125338398347, 3271: 0.6623833917202132, 9256: 0.34896116437537483, 7688: 0.33952932483546383, 3173: 0.4934237304668355}, {1384: 0.1214019351656379, 10483: 0.06254211334577159, 9168: 0.17579548623474572, 5601: 0.12524094983903475, 3796: 0.12855046055848757, 4621: 0.21361383665727807, 11555: 0.12652322173767452, 9886: 0.2263500226304567, 2011: 0.1975681399646466, 9704: 0.19593318409706156, 6371: 0.11293014931985904, 6576: 0.2481226763603576, 10097: 0.21934079369454748, 9539: 0.1975681399646466, 11242: 0.20087765068409946, 5547: 0.1975681399646466, 7366: 0.11067446173755795, 6296: 0.26989533009025846, 9897: 0.17257550715063635, 10098: 0.2481226763603576, 4325: 0.06245687231771267, 8155: 0.19457432335539807, 7961: 0.20877176347536536, 7016: 0.1975681399646466, 3064: 0.2481226763603576, 11486: 0.1652264560155636, 6307: 0.2481226763603576, 1236: 0.18932693745880044}, {955: 0.5942072868770274, 5310: 0.5942072868770274, 8695: 0.5420658635668586}, {11655: 0.11378794263128729, 9515: 0.1651012220543278, 2421: 0.34426178223360837, 8921: 0.37447059944838373, 6919: 0.19982256060965137, 7907: 0.24614573365315134, 7532: 0.314052965018833, 5194: 0.2626846923887647, 9856: 0.27871091449526647, 865: 0.15355705493915256, 8682: 0.2262392383527459, 4355: 0.37447059944838373, 6171: 0.37447059944838373}, {356: 0.5256387041138514, 4448: 0.16381170623195793, 10480: 0.17920023348353717, 10365: 0.1563637167963943, 11655: 0.17500945244408878, 11185: 0.4618988052244581, 1800: 0.4365612712377869, 8087: 0.4286661961545654, 2171: 0.1482837686498755}, {11234: 0.37023444110739573, 6371: 0.24654834919738866, 896: 0.7047676533595393, 9681: 0.5526689107449642}, {8355: 0.49740263647302835, 4583: 0.31035829182172237, 2047: 0.4026922267824723, 383: 0.38457714374157664, 5239: 0.5410494950638108, 9912: 0.2312426924659955}, {6530: 0.4598356812955741, 3935: 0.28861032953750415, 10785: 0.15492175830997626, 1148: 0.2995550597603419, 6774: 0.31366527112153886, 1742: 0.2090210736933597, 3775: 0.22669998107610437, 5968: 0.15344493474852222, 10004: 0.31944228570472427, 1881: 0.21980472681180946, 6331: 0.38743716031588304, 3174: 0.2721070614143376}, {6923: 0.25596910153970465, 5539: 0.4920147748017062, 747: 0.2499861509051772, 6758: 0.2926468515360472, 4481: 0.6491064518428749, 1326: 0.3506092401112513}, {7985: 0.3923788131633353, 237: 0.426077724347659, 10514: 0.381064631103879, 3126: 0.37245918497838615, 6210: 0.3089349313575217, 9934: 0.44019832914639023, 6834: 0.3022336537283748}, {131: 0.374627328666016, 11315: 0.16217551489455942, 8922: 0.25109823114691787, 10754: 0.22781204705981795, 2006: 0.3444058679905935, 5644: 0.3444058679905935, 6153: 0.27007825204405556, 720: 0.28978496228709816, 5029: 0.17991216517225408, 10699: 0.2662845254276071, 444: 0.374627328666016, 1345: 0.2839629466397483}, {7346: 0.16058935574663988, 8803: 0.1496792495644908, 7436: 0.512241800637318, 3222: 0.2835654913802844, 6128: 0.19609655031843368, 2995: 0.32130175525962107, 6107: 0.5571907892378583, 4730: 0.3960503506595788}, {7336: 0.15575746929440237, 9773: 0.1400114739394016, 5863: 0.2203686035688984, 429: 0.2438218786023637, 6923: 0.11464605607793765, 3751: 0.2907284286692943, 4187: 0.165911785351575, 6371: 0.09350067018012659, 11555: 0.1362894920548256, 4839: 0.2907284286692943, 10371: 0.267275153635829, 7570: 0.23010259218868678, 10861: 0.12930790827544278, 7613: 0.20394099821497247, 1763: 0.15638993487861202, 1574: 0.19293003074154458, 5924: 0.1893650603850403, 9162: 0.20394099821497247, 10218: 0.17564577397136338, 5497: 0.267275153635829, 10430: 0.10853906518964715, 8185: 0.15108302499854662, 10638: 0.13593885758457996, 10152: 0.25355586722215206, 5914: 0.16426092508511728, 8803: 0.07809894540756145, 1598: 0.19110061637427755}, {10892: 0.3282861708285659, 430: 0.5040112595469505, 4946: 0.34920817935401466, 5664: 0.3344664580741353, 936: 0.5040112595469505, 5577: 0.26005751733695065, 1969: 0.28762721379356593}, {5812: 0.04026583801857882, 2309: 0.22019635281668304, 6304: 0.22019635281668304, 10784: 0.12264236472442497, 1082: 0.13573338000408391, 2038: 0.1618617360645427, 6942: 0.22019635281668304, 8881: 0.22019635281668304, 4019: 0.1298257109350146, 11166: 0.17895101326615775, 2897: 0.19204202854581667, 1271: 0.14473871351088935, 8493: 0.10192387314343707, 7925: 0.1611876123919691, 11590: 0.09545521826785931, 10170: 0.14612430340076166, 3409: 0.20243295194249442, 8831: 0.22019635281668304, 6076: 0.05020317133779816, 307: 0.1611876123919691, 6371: 0.07081697051030068, 2311: 0.22019635281668304, 6768: 0.19204202854581667, 132: 0.19204202854581667, 11642: 0.08669196373916897, 10811: 0.10726460798015053, 1384: 0.09904677987359423, 3809: 0.14098168130379687, 515: 0.22019635281668304, 5434: 0.22019635281668304, 3174: 0.1849724534268659, 6757: 0.20243295194249442, 8178: 0.15446395869257323, 4771: 0.10322502417487354, 10002: 0.15256478106585863, 7063: 0.1611876123919691, 9101: 0.14612430340076166}, {11640: 0.30344038916243055, 4680: 0.4993474452514213, 1908: 0.701860340199051, 4826: 0.4073918531359016}, {10237: 0.21811521824425167, 2679: 0.28775557698347703, 6067: 0.2074502239982369, 9295: 0.25096312460399667, 4771: 0.13489586003862108, 9687: 0.23385560848526304, 8189: 0.14547375427871403, 6629: 0.24132867115732679, 10494: 0.17498506390046933, 9205: 0.1495381212971057, 395: 0.1474443187056814, 10978: 0.28775557698347703, 5812: 0.052619942626413535, 3615: 0.14900227093104246, 4412: 0.06314320401034568, 2565: 0.19490176533117654, 1345: 0.21811521824425167, 11429: 0.25096312460399667, 4710: 0.1574187743965293, 10803: 0.23385560848526304, 10811: 0.1401748428818488, 11234: 0.13897148228595402, 8892: 0.13524654317378407, 206: 0.24132867115732679, 2539: 0.2645421240704019, 4624: 0.10651991112462754}, {5812: 0.14065787368356705, 10480: 0.23932772444826597, 10188: 0.5329440230157667, 6925: 0.4345944646355217, 7716: 0.6708471677820268}, {7399: 0.30668494137649055, 9848: 0.4256446690708162, 6143: 0.40287454342784446, 8072: 0.32091628756251983, 4169: 0.4497819749262402, 4176: 0.5071212636942378}, {3284: 0.35875294563010995, 6451: 0.24685395996711668, 11444: 0.3661083604393567, 7133: 0.20350489586600404, 4599: 0.21145071061614823, 5601: 0.21962624658696786, 6907: 0.43511528914605613, 9405: 0.34121130808787575, 6180: 0.253413702963131, 4624: 0.2279437099613563, 2148: 0.3464613537454985}, {5158: 0.6998416266908011, 5606: 0.7142980453219604}, {11259: 0.13218136037103062, 10767: 0.22205806134770004, 3222: 0.2109378971310323, 7970: 0.2619263963563003, 5159: 0.3810449844341033, 11649: 0.308490229111164, 7346: 0.11945875655721157, 1580: 0.562849645511937, 6527: 0.1511181417089203, 9693: 0.201907053599344, 11229: 0.33684430034810003, 2742: 0.26761767223489863}, {251: 0.5514151459285794, 8725: 0.4758607740542384, 9713: 0.45203074545884303, 495: 0.5149427790706335}, {4772: 0.7403323697090397, 11655: 0.1608509357067969, 10365: 0.14371366692509266, 10279: 0.21863836776978063, 6125: 0.5293526266762437, 6825: 0.27814456951757427}, {8803: 0.19302102644960764, 4046: 0.34900668686060216, 10640: 0.5686970329906096, 4325: 0.16627685983630014, 3724: 0.35748231565572747, 10784: 0.40020024181707026, 10211: 0.4493540551375492}, {2614: 0.5633190922486643, 3201: 0.604528240547717, 7366: 0.2842382224983281, 1908: 0.4862364029558187}, {6117: 0.16018203512869947, 7133: 0.16082696421555637, 1153: 0.3438652949323185, 1156: 0.3740393205639709, 8803: 0.10047891295225168, 11211: 0.31369126930066615, 1030: 0.3740393205639709, 2717: 0.41878934670303386, 10333: 0.2696543422322605, 5021: 0.3740393205639709, 6618: 0.23220813202874474}, {7356: 0.33842748149620594, 8478: 0.6543690788892813, 8950: 0.40362044692328236, 3790: 0.5425481390505165}, {6058: 0.22991975673543322, 11227: 0.3236242480540722, 11457: 0.6087587386526492, 344: 0.49473114210575636, 2475: 0.34740410368716435, 7336: 0.32614189461067844}, {6230: 0.4991214341477267, 5316: 0.7203806713721064, 4991: 0.3666160729934247, 9929: 0.3122853459749111}, {7346: 0.18671970714233915, 7675: 0.38653988467112, 4973: 0.4114399082746599, 1795: 0.49106564167183353, 6753: 0.49106564167183353, 8747: 0.4051529853535777}, {8793: 0.4890946797680962, 10782: 0.4705791532075008, 1247: 0.6346040395673094, 4152: 0.3696205726875975}, {7300: 0.44464224185389917, 11359: 0.1874597928734648, 5817: 0.4087726269097123, 6610: 0.3729030119655254, 11655: 0.13511054267557351, 3728: 0.2637017671657907, 5609: 0.24881453187926034, 11318: 0.33703339702133844, 6555: 0.44464224185389917}, {11640: 0.16661951400429878, 5771: 0.2968519907498997, 3359: 0.5493968178325723, 3227: 0.3018699213963244, 2538: 0.4089049707626291, 2747: 0.5050765746072532, 5030: 0.2623907371098116}, {11399: 0.7550261985961767, 4046: 0.3667320086217356, 3995: 0.46221734533617287, 4704: 0.286003844302931}, {8450: 0.7101258117234608, 4451: 0.5231861458680354, 3796: 0.36791075080514324, 9729: 0.2943454904323195}, {4785: 0.5103504888571782, 9576: 0.696190834842794, 10365: 0.3663128269974676, 2171: 0.34738395584893716}, {7806: 0.3425677255950614, 10279: 0.10875279788975294, 1442: 0.20839780721924672, 8803: 0.07073214088664087, 10785: 0.12592984697736076, 2507: 0.1898232013569898, 3279: 0.22082300059941778, 6742: 0.11298631795555926, 5427: 0.2296388155998745, 11375: 0.1898232013569898, 6845: 0.1847039462956305, 11642: 0.15312425950570227, 395: 0.17552975146618588, 9215: 0.20367396793385112, 7854: 0.3425677255950614, 4152: 0.11414284379228516, 5805: 0.1959726138390757, 3276: 0.24206400898004557, 10480: 0.0819246734904161, 10034: 0.22082300059941778, 6355: 0.12218281235398266, 9897: 0.12940635765900343, 7489: 0.14050310723427223, 7151: 0.10417050295510254, 11259: 0.06454117680842034, 2734: 0.12343377369673561, 10157: 0.11898477300206038, 1702: 0.1257539092828046, 9693: 0.1282642019503037, 7326: 0.26330501736067335, 9729: 0.10033496637211585}, {11623: 0.36891034029463243, 2970: 0.25774721501136827, 6375: 0.38069919629535515, 8732: 0.45393825922863207, 4475: 0.2217759225542307, 3915: 0.24519082759648878, 8339: 0.31451470585568286, 1479: 0.38069919629535515, 6128: 0.15975807283901433, 1204: 0.2603893063611653}, {3623: 0.28403764012525395, 5149: 0.1837874160659902, 6348: 0.3328325626034031, 3615: 0.19760993869501592, 4376: 0.17620907476925873, 865: 0.15649183882334688, 4017: 0.29807477101638613, 9828: 0.31014421570557976, 9380: 0.21355250382177154, 705: 0.24857187887622564, 404: 0.22632991817297643, 11655: 0.11596265886547308, 8819: 0.3020463941887261, 9979: 0.2751248406511257, 11398: 0.38162748508155236, 8803: 0.10251733640163617}, {9598: 0.21513411827353013, 3213: 0.13170234519612187, 10097: 0.17483699416774728, 9205: 0.11179888226937917, 7484: 0.21513411827353013, 6058: 0.08125318126589606, 3822: 0.08821883925647983, 4108: 0.13556197160854752, 6018: 0.085380872559743, 3912: 0.1299749009230345, 2746: 0.18762705173895222, 348: 0.1804240648532603, 11259: 0.07789385240436758, 6445: 0.15509559013019458, 4412: 0.06141853682651404, 1844: 0.1008519181882777, 10480: 0.06693678902466639, 10117: 0.16306903814312537, 6923: 0.08483614175867404, 10502: 0.21513411827353013, 4448: 0.061188701636521785, 10483: 0.04985244615053601, 768: 0.2229166346619674, 2425: 0.15091288812594736, 5192: 0.18762705173895222, 3902: 0.09568566431553098, 6451: 0.11220601336933184, 2713: 0.12472391596783751, 371: 0.11179888226937917, 4506: 0.1977790915633952, 6128: 0.09850598669508279, 6226: 0.11091579372977066, 8784: 0.16011998520437434, 2622: 0.11479846602441175, 888: 0.21513411827353013, 341: 0.14012694074747747, 4238: 0.12472391596783751, 8964: 0.18762705173895222, 8002: 0.21513411827353013, 10974: 0.21513411827353013, 2330: 0.1977790915633952, 1833: 0.1804240648532603, 9260: 0.12277191403734256, 1063: 0.11525789195966155, 3666: 0.14419609148694654}, {5183: 0.32037624295311146, 5590: 0.43588199864559074, 11642: 0.20462207333337362, 7366: 0.2131256936516896, 7151: 0.15804547943496702, 3209: 0.22354320560384852, 6469: 0.24624816031978364, 10365: 0.10845496796006984, 10157: 0.18052140443899276, 6592: 0.1761282475616252, 9405: 0.28799610269018316, 9194: 0.399481297736804, 9391: 0.3672548491936367, 5749: 0.1780738426283573}, {11561: 0.35967953335433356, 8847: 0.47084677613656695, 9256: 0.28060429247238494, 3881: 0.4014206681802883, 3574: 0.5793694092503495, 10638: 0.2709016657691024}, {2908: 0.24158058408669705, 5594: 0.24158058408669705, 1938: 0.3409736072531896, 1742: 0.1839541396366101, 6219: 0.31105332592035034, 9556: 0.37089388858602884, 11141: 0.37089388858602884, 919: 0.37089388858602884, 2716: 0.2935510833296122, 6849: 0.37089388858602884}, {10279: 0.11496031568813991, 5964: 0.2783342452369796, 1038: 0.2783342452369796, 6205: 0.1774413572476141, 2861: 0.2334273908598293, 7429: 0.1529326963761146, 532: 0.2783342452369796, 11471: 0.3253994399833816, 641: 0.2006581492740524, 8145: 0.2334273908598293, 11264: 0.16316426248641078, 4046: 0.21658691438187516, 747: 0.10719305968126919, 552: 0.25588081804840446, 8824: 0.21097396367125415, 4526: 0.2071585650238507, 1560: 0.2006581492740524, 3919: 0.2783342452369796, 9729: 0.10606200146124391, 10370: 0.2783342452369796}, {11640: 0.22450339439623554, 6018: 0.22581208272069322, 4991: 0.37673210951036995, 3900: 0.263877590598138, 11315: 0.24630980955122742, 7526: 0.7402581336944756, 4785: 0.2799964950921874}, {4862: 0.2507533546740525, 6371: 0.18980701621848875, 10279: 0.24376219024313145, 5777: 0.46096782476790954, 2603: 0.4254761276914483, 7922: 0.27959192385486087, 10335: 0.3718892503249441, 1204: 0.3385410962439448, 4238: 0.3421570449034825}, {6205: 0.32897258720307765, 10643: 0.499683562388835, 11655: 0.20400336540054542, 8803: 0.18035013893649654, 3337: 0.6501033030081087, 3117: 0.3812027335370272}, {7748: 0.2806216114509934, 4967: 0.43983367697888526, 7356: 0.2729752552738784, 3822: 0.31604635924454816, 7346: 0.22213222698076673, 988: 0.6463740545394534, 9729: 0.2936918656570199}, {9528: 0.43501884679015607, 9628: 0.5150124491930994, 4818: 0.2915561654070136, 1691: 0.368532869384608, 6710: 0.5698239260594394}, {8450: 0.6490394164052402, 4418: 0.28898716325706303, 4616: 0.5215362363469266, 11464: 0.28898716325706303, 5097: 0.3737922271459096}, {9105: 0.667283855648364, 1360: 0.4040655565885359, 10064: 0.3583409668524386, 7156: 0.5128889094580908}, {876: 0.7047639695833838, 7366: 0.28899860164153673, 6292: 0.6479101445619658}, {2551: 0.1931301399329525, 2486: 0.2820125078047282, 1520: 0.18368797236036957, 3912: 0.1703799846120734, 5407: 0.20643812768124867, 11655: 0.11148955942420115, 6186: 0.07836939021781848, 1484: 0.21376204184209086, 10279: 0.11647954745070015, 10691: 0.2369001338960939, 10564: 0.08306617974633423, 6570: 0.09268735107685312, 7346: 0.10574710999886607, 8501: 0.24117387804488635, 8803: 0.09856287170873165, 865: 0.11564328472859967, 7133: 0.12125788121062894, 3831: 0.18902209329796113, 4883: 0.24595436473555293, 3900: 0.10052815050697447, 10483: 0.09652986527562037, 10480: 0.08774531854236708, 6742: 0.12101385379460197, 6527: 0.102820523882838, 11248: 0.1931301399329525, 1557: 0.2291882830021278, 10160: 0.10039816405485269, 5183: 0.17383807859720238, 3478: 0.19782692946146824, 4278: 0.1931301399329525, 6252: 0.17770389877291562, 10365: 0.07656342780395174, 7366: 0.11564328472859967, 7125: 0.18902209329796113, 10148: 0.2820125078047282}, {4325: 0.3000571824858918, 11464: 0.53075944508398, 6018: 0.3955333917956158, 4826: 0.6868867699027473}, {2413: 0.7772799212705913, 8491: 0.6291549284473447}, {6186: 1.0}, {11259: 0.17725337801459906, 2603: 0.6782569359978753, 10331: 0.7131260539015438}, {6152: 0.41465981623433823, 7779: 0.29300547010450756, 11315: 0.10604816591621063, 9380: 0.17834858325339467, 6585: 0.23330614749592973, 10166: 0.2590172462763051, 10365: 0.08652819409214808, 3539: 0.23330614749592973, 4818: 0.13676386401860777, 3395: 0.318716568884883, 9395: 0.23721431404641807, 10461: 0.26729437132413214, 4127: 0.13411321703739376, 1806: 0.23330614749592973, 11568: 0.1879123092320062, 3393: 0.29300547010450756, 3604: 0.17144544027147582, 4649: 0.21150321526604265}, {40: 0.33760229048604334, 3374: 0.33760229048604334, 1455: 0.1515111017673161, 1984: 0.23119959504917273, 35: 0.33760229048604334, 6270: 0.33760229048604334, 7776: 0.33760229048604334, 6371: 0.10857569230251447, 8697: 0.33760229048604334, 289: 0.33760229048604334, 3143: 0.33760229048604334}, {10279: 0.14152261259006696, 4954: 0.34264510603884457, 7748: 0.12475756910982522, 1638: 0.14915508639034447, 7568: 0.34264510603884457, 3792: 0.23207938052541593, 10684: 0.2435516110811771, 9380: 0.24945767199193578, 1109: 0.22318082718182167, 1295: 0.44579156082400234, 1263: 0.24035965550010027, 9199: 0.22738241025858105, 747: 0.13196068370905883, 8366: 0.16455292552370937, 806: 0.21760737864474758, 10365: 0.09302454009148961, 2171: 0.08821758438787379, 7907: 0.22522630917575837, 3900: 0.1221416704474754}, {1047: 0.22924191008820882, 7639: 0.4009180647465466, 10667: 0.37229923271882503, 4325: 0.1449065187507009, 8234: 0.35822179194685516, 6726: 0.44247306480218834, 970: 0.31349318294354206, 5738: 0.31636638970162306, 11227: 0.19666373730564582, 7489: 0.25682810399667394}, {6720: 0.18749454613119848, 11019: 0.21276701060887632, 1028: 0.327161260860261, 11655: 0.17063396531794592, 7133: 0.18558431124745042, 10430: 0.16113809701088777, 5812: 0.07892712353878015, 907: 0.43161810598682204, 3604: 0.2321779393802989, 2224: 0.3212446510073309, 8803: 0.11594641449640972, 1202: 0.24889078881775695, 5914: 0.24386328401866944, 7640: 0.24889078881775695, 9912: 0.18447209332243222, 4656: 0.22511606880144885, 5029: 0.2072815890184705, 395: 0.2211586591835812}, {2047: 0.19497566587143744, 7599: 0.16363676788327194, 2550: 0.14739352090403385, 7399: 0.10619978729301903, 9324: 0.25939354421575395, 306: 0.12164858355198925, 7198: 0.20135241939655918, 5554: 0.13789183053122736, 10483: 0.06070456396254521, 6959: 0.15575186003919778, 6018: 0.06142155974412207, 4870: 0.12326536608072156, 11640: 0.061065592618100135, 630: 0.13637943147960674, 11234: 0.09724309943438438, 2429: 0.17560748204451457, 10019: 0.11432837898143546, 915: 0.15575186003919778, 4325: 0.046595257298108164, 10480: 0.08150790792743409, 8142: 0.09616548190566304, 4713: 0.09866377996631953, 9929: 0.08728636463048585, 865: 0.08256745542361477, 11545: 0.09752042872867699, 6527: 0.07341220929734953, 5968: 0.07331270149660021, 11454: 0.16824511703032827, 5812: 0.03681997362972004, 1702: 0.09616548190566304, 6210: 0.12866729459251824, 7748: 0.07331270149660021, 9083: 0.22847060160293747, 5805: 0.19497566587143744, 8902: 0.21969963424050323, 5030: 0.09616548190566304, 6371: 0.08425028614153496, 9773: 0.09696901383196004, 2722: 0.07604802419462786, 7010: 0.20135241939655918, 4745: 0.1046369381640769, 10767: 0.1078743603611873, 9912: 0.08605733120647546, 3469: 0.10281943447786705, 6958: 0.1155003257709666, 9223: 0.0753878664027111, 11486: 0.12326536608072156, 1157: 0.20135241939655918, 3764: 0.10875876916453499, 3874: 0.10967786939074661, 7000: 0.20135241939655918, 10754: 0.12244303427225713, 7151: 0.07966039922070656, 3597: 0.1336192977132319, 9474: 0.14986254469247, 9735: 0.10017617901794013, 10365: 0.05466506271521125, 6110: 0.15936423506527647}, {8189: 0.4207190749651531, 6923: 0.32817305776832184, 11309: 0.6763248516212186, 10157: 0.3760655551247219, 865: 0.3412584027444997}, {11259: 0.4859144246467146, 6592: 0.8740063912353572}, {4379: 0.8356388479260893, 1992: 0.5492792694401986}, {4325: 0.3774151804681999, 4616: 0.9260441574526327}, {2574: 0.5160733848977644, 6018: 0.18050502104924118, 6130: 0.2499524355469486, 4376: 0.2732209040923474, 9745: 0.5917323306816498, 8646: 0.2554647959517365, 3558: 0.3854230184650186}, {664: 0.5754029732878524, 5386: 0.33315395187313857, 7114: 0.23121843869019498, 10461: 0.4564003027384898, 1854: 0.5442027746646401}, {6117: 0.23752282674498917, 1662: 0.42040805070996723, 11227: 0.22662997027476825, 10561: 0.32513760628931926, 4947: 0.24043537158704192, 4087: 0.35223936505342124, 4599: 0.24779052591536765, 2622: 0.29596175874575104, 8142: 0.2648934169009893, 6193: 0.40600403187952716, 2722: 0.20947870876626667}, {6226: 1.0}, {4046: 0.5084830185420733, 5738: 0.6881201015835817, 6896: 0.517625101449826}, {11359: 1.0}, {6278: 0.2716324596307055, 10365: 0.08368648590799095, 2171: 0.07936206538029641, 10160: 0.10973868049480061, 3775: 0.16581492970936892, 5812: 0.07333583878500127, 9704: 0.19709977183183014, 9223: 0.11541092804030467, 8244: 0.2833827563620704, 11553: 0.2833827563620704, 4127: 0.16875497373174403, 324: 0.2746447663244482, 3519: 0.2833827563620704, 6007: 0.2833827563620704, 5738: 0.20261748491751233, 10480: 0.09590868087163901, 7067: 0.1052150939334158, 747: 0.11871411443438361, 8969: 0.19735807367852895, 4271: 0.23843978572234834, 4368: 0.20261748491751233, 6451: 0.16077154110503347, 6924: 0.268836663519764, 9820: 0.2833827563620704}, {11355: 0.1489887363037313, 2468: 0.283047329288641, 1095: 0.08812123490448186, 5968: 0.07921257342738328, 8892: 0.1510394249538578, 10365: 0.05906425770494883, 6018: 0.06636448680698669, 6226: 0.08621216332165321, 4790: 0.21755634399819326, 10477: 0.17680550882406718, 11242: 0.210666418425604, 4302: 0.17571920382792863, 9304: 0.16786547453018957, 8046: 0.18973957147785742, 5812: 0.039783077218533115, 10279: 0.08985723608927651, 7961: 0.16828605954970316, 3728: 0.12902506098217925, 10550: 0.2000059145667845, 3440: 0.21755634399819326, 10218: 0.13143830687232252, 10430: 0.10567144281704036, 7752: 0.18973957147785742, 2171: 0.05601216768466597, 6128: 0.09961507965030986, 353: 0.17680550882406718, 4238: 0.12612820032850738, 7922: 0.10306503024699158, 10638: 0.10172503940793946, 4046: 0.10567166433589065, 10634: 0.10724483590803116, 1844: 0.10198742432081347, 1837: 0.15925507939265837, 10480: 0.0676904398802492, 7114: 0.12025988216226832, 4325: 0.0503450311439604, 2278: 0.1721891420464486, 991: 0.17680550882406718, 2562: 0.2000059145667845, 8841: 0.15261203785977578, 9019: 0.18245548513537568, 5023: 0.135061608428367, 6058: 0.08216802242383242, 6371: 0.09103036501217832, 6205: 0.106603791098432, 6207: 0.21755634399819326, 4771: 0.10198742432081347}, {2712: 0.37526455379883805, 337: 0.3370156027251461, 2245: 0.3116578863899563, 7197: 0.3976387558688661, 4176: 0.41351350487253, 9827: 0.3470763928445805, 2267: 0.34181702729220903, 8682: 0.28645323970086045}, {8366: 0.2389378940243118, 9523: 0.2475321232609058, 9991: 0.3586857436982587, 6919: 0.2654915792694031, 1881: 0.25949607799300134, 10061: 0.2614264164098395, 9648: 0.4172622821102085, 120: 0.4573988240789538, 2354: 0.4043412016188436}, {6018: 0.23204891160788021, 1455: 0.3413930056760807, 7114: 0.32320439657408284, 7356: 0.2694263605867071, 7259: 0.4043265322295038, 111: 0.6993372061377104}, {6767: 0.5009334261645296, 3801: 0.5255719714472558, 576: 0.49397396988557923, 6186: 0.2242321823722442, 10480: 0.2510587898542718, 6076: 0.1839674030882848, 7356: 0.28578825987439266}, {93: 0.4491398440111899, 11642: 0.2300581444459712, 9476: 0.2589946263024526, 8803: 0.12065331319109257, 2132: 0.44292593631640415, 5520: 0.2875637563440387, 3558: 0.29254584449067805, 6161: 0.4491398440111899, 8892: 0.2110979461837294, 395: 0.23013669795045694}, {11720: 0.4338919176332457, 5738: 0.2852043512877415, 9879: 0.32888478235967045, 1070: 0.32888478235967045, 7748: 0.1579806626340303, 2314: 0.36388716078419553, 9929: 0.24471377890373863, 8892: 0.2039313454359575, 4412: 0.09521040794677385, 3227: 0.23840494669622195, 8838: 0.2713459059296777, 4325: 0.1004075620361586, 5758: 0.17250796224415693, 6186: 0.12057566265430474, 617: 0.24761177650788646}, {4836: 0.19679726911568945, 1851: 0.24215588452190775, 4367: 0.10288610816676293, 10452: 0.31505206938954516, 2417: 0.24215588452190775, 3708: 0.12723908574782214, 5130: 0.1916589203101908, 6205: 0.11865769966718993, 10222: 0.24215588452190775, 3796: 0.11533823305438046, 6469: 0.14926966897313157, 6548: 0.24215588452190775, 8780: 0.24215588452190775, 1280: 0.24215588452190775, 4895: 0.24215588452190775, 6086: 0.22262099215978287, 9037: 0.21119381267231568, 6128: 0.08522383088745551, 9051: 0.203086099797658, 11407: 0.24215588452190775, 9817: 0.1698681929354898, 6433: 0.24215588452190775, 559: 0.22262099215978287}, {}, {4010: 0.3706005315651357, 10365: 0.15011180199907068, 9450: 0.2842952296600052, 4934: 0.3930142646576441, 8118: 0.3786548507193644, 8802: 0.48222309213387793, 4437: 0.32791717291745637, 1358: 0.3484098509195272}, {7338: 0.2987754058723769, 8042: 0.24949678233723785, 3376: 0.39822294096216154, 2960: 0.16716691722065008, 7165: 0.2982861450476331, 4695: 0.2029727075936487, 6371: 0.14882039113055412, 2551: 0.2435732517144358, 9266: 0.28904931858412497, 2743: 0.2109357689430628, 4930: 0.1764037436841583, 8174: 0.33873241609627763, 11264: 0.20850011940474122, 11259: 0.08718176446639664, 9515: 0.1568124251470941, 3629: 0.2982861450476331, 6018: 0.1084955430253736}, {1355: 0.2539679720670517, 5864: 0.3080263143364642, 9432: 0.21278735522245887, 7653: 0.21823735136256497, 2620: 0.26518609166238005, 6672: 0.28653562716178027, 1767: 0.3350554854711705, 6755: 0.3350554854711705, 9548: 0.2539679720670517, 3946: 0.3080263143364642, 3900: 0.11943622122943479, 8726: 0.2722956936319775, 3456: 0.26518609166238005, 282: 0.22945547095789337}, {11640: 0.07830873749729106, 6753: 0.25463613082413716, 1923: 0.13140743795784207, 3092: 0.26588376392380486, 10480: 0.08033893733097981, 11555: 0.12104458628948293, 9522: 0.23737861158285398, 1850: 0.22519392057628382, 7557: 0.18614897842185724, 7675: 0.15405914415341326, 11359: 0.10885989528291279, 7758: 0.16671709132846918, 1793: 0.2165487447254938, 2660: 0.23737861158285398, 7562: 0.1768284675074161, 3902: 0.11484394009527153, 10861: 0.11484394009527153, 7852: 0.19217936271235353, 3780: 0.20984302537134644, 4325: 0.0882615414156451, 5771: 0.10723511334338663, 3212: 0.23737861158285398, 2492: 0.20436405371892363, 1919: 0.17134949585499334, 6562: 0.2165487447254938, 1844: 0.12104458628948293, 6355: 0.11981783858775834, 9627: 0.1957188778681336, 1095: 0.10458738901747637, 11655: 0.07846012897927622, 10365: 0.07010088435905223, 7260: 0.23737861158285398, 3768: 0.18353418686156348}, {2441: 0.39888989966915195, 11347: 0.2922500400670365, 7688: 0.2859679716040589, 6225: 0.48029986325105095, 9629: 0.4155852617775729, 8493: 0.280895156146446, 9838: 0.43749048452184797}, {9256: 0.5856887545677643, 1699: 0.8105360465598438}, {8259: 0.407168361697914, 9897: 0.3072261323365329, 4022: 0.407168361697914, 6761: 0.42809815830900244, 7607: 0.6251175256913286}, {7133: 0.18923848744825011, 2233: 0.34833881773296943, 6076: 0.13054979791970364, 11675: 0.44011671559519727, 1742: 0.2839978557347803, 1841: 0.23392926539519812, 4451: 0.29809856587484584, 11259: 0.10788116019324089, 5812: 0.08048120758012148, 9800: 0.26259402844475804, 7934: 0.36910764073502156, 5029: 0.211362987086895, 11642: 0.17327526937353627, 10160: 0.15668422141831229, 6644: 0.29206545730082956}, {2479: 0.22420924465685116, 7228: 0.3230799600980222, 9490: 0.3543444153382104, 11259: 0.15674745140586274, 6018: 0.14993375652207594, 11655: 0.1493530008395564, 9080: 0.2700655172966749, 7672: 0.49151348078267754, 10667: 0.3801997467707146, 7271: 0.3261728183629155, 1274: 0.2914994601677753}, {2528: 0.2968265910627371, 2171: 0.14652326627348555, 4947: 0.24670929751880513, 8892: 0.3480051705595009, 5788: 0.41659831011403076, 10064: 0.32314181240539847, 11562: 0.2886642967311848, 6186: 0.1581517711680728, 10279: 0.2350592072106669, 10579: 0.4235768471052432, 1420: 0.29574957804992474}, {641: 0.31098996491882425, 9729: 0.12634617011950194, 5097: 0.17554933463747568, 3824: 0.20438332012664137, 11315: 0.11032334665540262, 10289: 0.24271155247858409, 5061: 0.19663971145571113, 3666: 0.2222353313915815, 1193: 0.28917122226580005, 7896: 0.1347682778039437, 11067: 0.3315651733353794, 5030: 0.15835481278292665, 3869: 0.26945915757983424, 8464: 0.22706520651025486, 11068: 0.3315651733353794, 10910: 0.14373365219676487, 8719: 0.21794328570705504, 11259: 0.08127306760144348, 1194: 0.3315651733353794}, {6226: 0.2410199563908599, 3796: 0.35671875582470347, 2479: 0.21324899094544073, 7133: 0.20100668804591165, 6570: 0.15364590967365355, 4448: 0.1329630136892707, 5771: 0.1941491169958845, 42: 0.46748631606785274, 7324: 0.3543489266975903, 8366: 0.22450704707437366, 1774: 0.46748631606785274, 3822: 0.19169948729079894}, {4418: 0.3788261156482124, 3146: 0.925467867677148}, {8117: 0.4128137440089253, 2208: 0.4733343087014861, 4870: 0.28976918685910263, 11486: 0.28976918685910263, 10305: 0.3141089543488817, 10638: 0.22132175196963377, 3851: 0.4733343087014861, 7905: 0.2567371433323958}, {4360: 0.3237022836713916, 5019: 0.41278764630110737, 10836: 0.4490095773987793, 6635: 0.3765657152034355, 2674: 0.2308677854304257, 8803: 0.12061831941666999, 11259: 0.11006097344462164, 10453: 0.22928411270968563, 4164: 0.47475216573306256, 7067: 0.1532608812818286}, {4456: 0.3742973166626223, 1312: 0.4103010500531476, 11655: 0.17644013048351168, 1691: 0.3149485194126923, 9576: 0.23028238310052102, 9827: 0.48257990092694025, 6058: 0.16856314451656687, 3209: 0.24974486298245158, 5592: 0.3976495869443281, 5968: 0.16250020468177168}, {11259: 0.15229920968710145, 7845: 0.3493465957124836, 8142: 0.2967442173495651, 5215: 0.3240609837856737, 1500: 0.5049448477029308, 4325: 0.14378208152372887, 6743: 0.6213265396401527}, {8892: 0.4505069325034544, 5099: 0.5389341018435797, 6310: 0.5584367145799316, 6451: 0.38425416728763084, 10564: 0.21700393629527476}, {7346: 0.25134708527651145, 8725: 0.46722105679349446, 6742: 0.3742206486825654, 10658: 0.7605840077935981}, {4325: 0.16559784574933645, 7133: 0.30768861412737997, 7911: 0.5019805158671912, 6923: 0.28218988273990414, 5695: 0.5663746816003125, 10170: 0.47487811281490294}, {10539: 0.7098680018175026, 7880: 0.4544961036812227, 7588: 0.538071288710161}, {7748: 0.1561405418972251, 8892: 0.2015560022049281, 9539: 0.3139170124178696, 11418: 0.39424336985764635, 4367: 0.1822027904991549, 1801: 0.4288380490023825, 7925: 0.3139170124178696, 7403: 0.33171850622840393, 10019: 0.24349525640138198, 1696: 0.2015560022049281, 2272: 0.39424336985764635}, {6018: 0.18189209303911963, 8118: 0.40834901902211573, 11655: 0.18118754945208365, 11640: 0.18083794192548813, 11257: 0.3336682275603121, 6069: 0.3321086799680015, 6494: 0.4182797807208779, 1455: 0.2676017220725874, 924: 0.3757322019521, 3796: 0.28400638163638486, 7356: 0.2111904956048262}, {4451: 0.3381477289674386, 10348: 0.3502123775871327, 11259: 0.18076250205474964, 5097: 0.26432895084965397, 11315: 0.2161226487947276, 10811: 0.24319843741960512, 8415: 0.3077449919807963, 4452: 0.4992458364311847, 3383: 0.4589713095652482}, {5106: 0.6767890591450134, 1411: 0.7361769959877906}, {4325: 0.16802811640541954, 6644: 0.481847299748271, 1638: 0.31607535314264845, 1842: 0.5130750243662536, 8209: 0.39669457483185894, 10759: 0.4680529207566551}, {11655: 0.2350792841306556, 1163: 0.321888121917948, 4367: 0.3286983718110116, 4689: 0.4802816629757954, 1140: 0.6123078826823334, 6570: 0.254265794709385, 9084: 0.2506803425971648}, {7688: 0.07073331406036347, 10892: 0.12719930066380392, 6884: 0.1219857511890941, 4376: 0.06930645571822786, 3166: 0.1587071213406146, 2099: 0.15010151111852715, 2494: 0.151059750629518, 3831: 0.13089304524707304, 6355: 0.06965239383160893, 3764: 0.10548235215061506, 3748: 0.10821194998351079, 1566: 0.07269822612993737, 2487: 0.10987695247506805, 6672: 0.09866421190646558, 4734: 0.09691582355798949, 6018: 0.05957120188412559, 9474: 0.11171752733750345, 2882: 0.10987695247506805, 3489: 0.11610781544850132, 6445: 0.10821194998351079, 2227: 0.10821194998351079, 4710: 0.08211411977847549, 2475: 0.11144539032539857, 8838: 0.09386999125966104, 3249: 0.15010151111852715, 11259: 0.04786853309197598, 1745: 0.0996087286234774, 1384: 0.0878420804939272, 3203: 0.10987695247506805, 4325: 0.034735209799630226, 6487: 0.11610781544850132, 1135: 0.22107260363807707, 9337: 0.10987695247506805, 9331: 0.0912771970919634, 8552: 0.12588391369047505, 11574: 0.11610781544850132, 940: 0.0884984476087397, 4905: 0.10041072580834588, 6079: 0.12588391369047505, 5097: 0.07947222001595372, 10670: 0.17031721123359864, 10483: 0.04525322720172407, 3263: 0.19528656835969443, 5758: 0.059677778636828334, 11642: 0.05909541458170752, 7905: 0.08141525443225392, 3257: 0.13799271240450112, 715: 0.10821194998351079, 7429: 0.08247432437191109, 11315: 0.04994403024312461, 2496: 0.15010151111852715, 3491: 0.13799271240450112, 1455: 0.06736342130192766, 6451: 0.07828740647221938, 3809: 0.09610315126948474, 1818: 0.09458312308593715, 8862: 0.13799271240450112, 3902: 0.06676097181331413, 10475: 0.1219857511890941, 7950: 0.15010151111852715, 4135: 0.1219857511890941, 5029: 0.07208520520145154, 2171: 0.03864521188388107, 10365: 0.04075098051234285, 6896: 0.07421825431348496, 9993: 0.08009619005407773, 8142: 0.07168815847726571, 9107: 0.10987695247506805, 6925: 0.11033648316501689, 5609: 0.12406984343608479, 6923: 0.05919113703411748, 10279: 0.06199638527684888, 4412: 0.03293729504107075, 865: 0.0615512833937569, 7299: 0.11880072051398924, 2325: 0.13799271240450112, 5099: 0.08439596343482549, 4448: 0.042692050209093536, 4797: 0.1066919217999632, 2960: 0.07054845197703251, 4556: 0.1309095192280153, 6758: 0.08804410927589666, 8012: 0.08749992990945134, 3262: 0.15010151111852715, 1817: 0.0996087286234774, 11680: 0.08074327528426875, 5867: 0.08655541319243952, 3264: 0.15010151111852715}, {622: 1.0}, {2699: 0.7925147206915023, 2698: 0.6098527834545566}, {6099: 0.2646359389944122, 7366: 0.1335293796073642, 9312: 0.2993614472639803, 10253: 0.19673199093363308, 4603: 0.2468237814047385, 10073: 0.2730926143343594, 1007: 0.3553016828432969, 11142: 0.2993614472639803, 11315: 0.10834859983004506, 3915: 0.17588638161237768, 3157: 0.17445610187050575, 3516: 0.2730926143343594, 10075: 0.3256302801936012, 4660: 0.24236004993128485, 7217: 0.23145749920320124, 3651: 0.2577263321328221}, {9687: 0.5365964242926649, 1380: 0.4227428763751629, 2171: 0.22116768477370094, 10312: 0.42631769589205387, 7940: 0.45747556551229096, 8493: 0.3056254607370282}, {7334: 0.2821848127151509, 3267: 0.3795836281046334, 595: 0.24923568631352752, 10634: 0.23024386615670436, 193: 0.46707156876083034, 8430: 0.3163555481418696, 7896: 0.18984632885255665, 6628: 0.39171355845135, 4289: 0.39171355845135}, {6758: 0.30722022072988886, 1163: 0.2835247507262223, 4238: 0.39505932164678487, 11315: 0.22673595112935285, 11227: 0.2784392164750796, 7985: 0.3744170081826804, 1881: 0.3554092347277579, 7896: 0.276975042693247, 5594: 0.44384794789808335}, {4325: 0.12356080446182978, 60: 0.32481985336754915, 9060: 0.32481985336754915, 3052: 0.33352822813482913, 108: 0.3579272133711778, 1518: 0.24956621177833002, 6595: 0.377293777471176, 7106: 0.32481985336754915, 9562: 0.3579272133711778, 6018: 0.12519083733887892, 11019: 0.20230806344732596, 2074: 0.19876838330140065}, {4862: 0.11865684467180292, 4325: 0.08408204275087677, 1956: 0.1773618323888546, 10849: 0.24356632661512448, 8274: 0.24356632661512448, 6349: 0.18532901406020247, 2227: 0.20133590978541166, 11507: 0.2116865677793785, 758: 0.15483256454118766, 8220: 0.20785828190786948, 2171: 0.07190212259068252, 7905: 0.15147878144720964, 1844: 0.1309199873767895, 11315: 0.0929243652746472, 5064: 0.27927437132237953, 9138: 0.22103705876745747, 11130: 0.19590617375823347, 6609: 0.19125498664238394, 772: 0.27927437132237953, 10160: 0.09942337085188678, 9993: 0.14902456981267112, 6779: 0.20785828190786948, 7684: 0.27927437132237953, 2267: 0.20133590978541166, 3117: 0.15857264720253347, 6050: 0.2342158356270455, 7271: 0.18532901406020247}, {4037: 0.23867580034522554, 2261: 0.20016749749400017, 1969: 0.1362065909300419, 9004: 0.14926229006853428, 7957: 0.21942164891961285, 6860: 0.18890454092680997, 10638: 0.11160007908590885, 6018: 0.07280687251891516, 341: 0.15546074235565455, 10029: 0.16345193721407703, 6851: 0.12146686900200113, 4613: 0.20815869235242265, 10018: 0.20815869235242265, 9900: 0.23867580034522554, 1008: 0.20815869235242265, 5854: 0.1776415843596198, 10061: 0.12541050028998474, 11703: 0.23867580034522554, 5593: 0.23867580034522554, 8153: 0.17471489378126723, 7824: 0.21942164891961285, 4905: 0.12272021853243983, 5029: 0.11462239064948783, 6742: 0.10241772122847127, 8871: 0.1653684121448767, 9108: 0.23867580034522554, 4120: 0.17471489378126723, 5972: 0.23867580034522554, 9084: 0.07733798041313239, 9453: 0.20016749749400017, 1384: 0.10735904185310559}, {11655: 0.18428134623696682, 1170: 0.34787980171430505, 9084: 0.2556669589833526, 8377: 0.7890239146562956, 4785: 0.22938882884368697, 11464: 0.24824552209133802, 7067: 0.207003920581923}, {8142: 0.2878665782149571, 1351: 0.4898385107342628, 4599: 0.2692804209475813, 3775: 0.3242277506308361, 9741: 0.4228106569899911, 1359: 0.4486060599740721, 4583: 0.34574446988179675}, {3328: 0.15524921415481968, 1047: 0.13689899050437573, 9679: 0.26423665669810903, 10480: 0.08942883295788204, 1963: 0.2139233688784178, 2448: 0.2223301537888483, 4503: 0.2874233049860678, 4325: 0.08653546869698107, 5089: 0.3739463412397509, 3126: 0.14990938425173234, 10620: 0.2874233049860678, 9449: 0.2874233049860678, 9308: 0.25067333693224275, 10775: 0.19683560684615944, 7059: 0.22748668864428404, 9294: 0.15337295068239765, 3873: 0.2874233049860678, 7469: 0.25067333693224275, 237: 0.17149006354627414, 6834: 0.12164463317826667}, {11259: 0.172675093035699, 1610: 0.7044528875647784, 9515: 0.3105878880423997, 7283: 0.6143814818564443}, {10365: 0.3729152074519605, 5919: 0.7514323244970881, 6226: 0.5443194923498186}, {7970: 0.13487626780543557, 3810: 0.24217812236391692, 5953: 0.2066735953602534, 10209: 0.24217812236391692, 3: 0.19016493043793523, 4376: 0.12821456692009917, 4973: 0.17635096010143358, 1170: 0.1592850811478688, 1207: 0.24217812236391692, 10749: 0.2552817868944879, 8850: 0.34399336328160623, 6491: 0.2552817868944879, 7520: 0.19478960815153232, 3569: 0.23288092442139527, 6130: 0.11729535622737011, 9479: 0.23288092442139527, 51: 0.21479547784540848, 6330: 0.24217812236391692, 6049: 0.20018839744341826, 7918: 0.21479547784540848, 6610: 0.23288092442139527}, {2712: 0.4849945639059933, 2093: 0.5344277788421181, 2739: 0.3219796125543605, 9029: 0.612777570805965}, {1319: 0.6795037617621525, 4826: 0.2766753973561218, 6591: 0.6795037617621525}, {10077: 0.7071067811865475, 8209: 0.7071067811865475}, {6076: 0.1710446545976136, 9084: 0.24309325820293304, 10480: 0.2334232221221879, 4412: 0.16462331429911747, 8142: 0.35830332241345475, 6058: 0.2833476128019002, 4152: 0.3252205867296091, 6923: 0.29584217961855075, 6540: 0.6542965626578465}, {10785: 0.6467596691042449, 11590: 0.762693864155316}, {10365: 0.11817005389582257, 5771: 0.1807677497819796, 10061: 0.22870725849514345, 8686: 0.34449938061609486, 8167: 0.30938617895419307, 1998: 0.35373536075215545, 2573: 0.4001524885304691, 10243: 0.4001524885304691, 5896: 0.3186221590902536, 6186: 0.12095742486263615, 8160: 0.35373536075215545}, {10480: 0.18625344318583417, 1047: 0.2851195470940949, 11315: 0.19918069627118065, 4475: 0.29245990350615253, 747: 0.23054130624269137, 11568: 0.35293872616645644, 7638: 0.5220773946642218, 11545: 0.28992620795223106, 11506: 0.4737865589602028}, {6169: 0.43195205915882484, 2741: 0.43195205915882484, 4789: 0.2629235480036787, 2335: 0.43195205915882484, 6629: 0.39404845941559175, 6168: 0.43195205915882484, 5968: 0.17107511184026838}, {4785: 0.1439147814294911, 8113: 0.38048364668624746, 5594: 0.2478268432848008, 6069: 0.21191728914521143, 10559: 0.27430021881821015, 3792: 0.2577080701481987, 5867: 0.21940431516301626, 9506: 0.31909585841722304, 367: 0.3318349754819406, 11127: 0.3497897525517353, 2683: 0.24360632468369792, 6791: 0.38048364668624746}, {6760: 1.0}, {9266: 0.3296567981401367, 6720: 0.13543773148771915, 9987: 0.1716757565258432, 3912: 0.18836529509982936, 6018: 0.12373768428660274, 4704: 0.11810289283791439, 4412: 0.10105776734207816, 8797: 0.23205286597804262, 10480: 0.0970077140134352, 9897: 0.15323118665554972, 146: 0.31178174698999417, 9993: 0.1663709437447533, 9939: 0.2614784237405955, 2264: 0.28663008536529483, 69: 0.27191730648401835, 4229: 0.27191730648401835, 7174: 0.31178174698999417, 10007: 0.28663008536529483, 10155: 0.24117210592996716, 7346: 0.0898593997778941, 6527: 0.11367425796878138}, {4325: 0.06599757703753815, 10218: 0.17230319629130528, 7696: 0.26218884872351794, 7133: 0.12262661342336016, 1335: 0.28519580277880874, 9228: 0.28519580277880874, 11555: 0.13369587306890462, 5673: 0.2257236892900676, 5058: 0.1326744996044996, 6534: 0.3236060278331109, 3117: 0.2106820971409545, 3329: 0.15670282712419537, 8803: 0.07661270531277868, 4862: 0.12117271596079127, 4785: 0.14034593760717826, 9035: 0.2257236892900676, 5697: 0.28519580277880874, 3162: 0.28519580277880874, 7151: 0.11283108280256417, 10180: 0.28519580277880874, 3935: 0.19531015034659602, 5809: 0.17970978117948613, 4448: 0.08111572922168134, 6876: 0.14058784271042246, 4367: 0.12117271596079127}, {747: 0.23651547405788514, 7133: 0.2640591012430883, 6076: 0.14001693514112265, 4959: 0.5356063280214391, 9316: 0.49909541071284674, 8640: 0.5645866734561826}, {2171: 0.20972992871504045, 7551: 0.8146100821364417, 6018: 0.24849277688372623, 4019: 0.480286488379028}, {10430: 0.1648692377147354, 8015: 0.2991114770820801, 9257: 0.3495224406442739, 11642: 0.17386404262633573, 6128: 0.15542006249763085, 1384: 0.19864209801217447, 11572: 0.40598701272241144, 10540: 0.44161219126918855, 4448: 0.12560386435899734, 5749: 0.19685422145255155, 6142: 0.32326790976927117, 8258: 0.31836932218486874, 9704: 0.21703887659564308}, {882: 0.31791778781186536, 5847: 0.35175297664924143, 5684: 0.36579584881193344, 1217: 0.6195391513977868, 6116: 0.4194233543239935, 940: 0.24728808838715458, 7067: 0.1431621866203219}, {11640: 0.2338438361403805, 495: 0.4470196870356223, 3724: 0.3836134390432029, 7424: 0.6102674595218072, 9996: 0.4752944528773415}, {9627: 0.5827840841580058, 9897: 0.29043879969495917, 6681: 0.37238033666042697, 6592: 0.26054978343571367, 9713: 0.3007513628719954, 5771: 0.24542831811471108, 1596: 0.46772670278930745}, {7067: 0.12282220152180973, 6331: 0.33080514124460614, 2302: 0.35983314423909785, 346: 0.3017771382501144, 5307: 0.33080514124460614, 7525: 0.2847968450275153, 10388: 0.27274913525562267, 2171: 0.0926428255008198, 9740: 0.292432208547713, 8372: 0.21881565736535077, 5294: 0.23652429165847366, 4046: 0.17477848053595646, 3724: 0.17902297663923736, 3226: 0.35983314423909785}, {10430: 0.3423653694393996, 10666: 0.6951098965325055, 7133: 0.3943055209228193, 10288: 0.31264507857983315, 4325: 0.21221501815097013, 10600: 0.31835386385588377}, {213: 0.18294967535757187, 10854: 0.18779442245126934, 215: 0.2390310484054544, 10499: 0.33194212985132565, 12: 0.4193999420122983, 5029: 0.20141390087379188, 395: 0.21489814155280948, 553: 0.340841451853431, 342: 0.4193999420122983, 1638: 0.1825668409688539, 11053: 0.4193999420122983}, {7675: 0.46847681379316786, 10365: 0.2131690340685597, 11315: 0.26125802497422457, 3548: 0.6847897992482245, 1638: 0.3417941723868959, 11547: 0.2847320789917769}, {9457: 0.5600453774771633, 1018: 0.5643000511750974, 4044: 0.6065596651692595}, {10549: 0.30719480945779226, 7409: 0.5117002264437065, 6287: 0.3465832999649135, 4151: 0.5117002264437065, 4245: 0.5117002264437065}, {7346: 0.09447157457539862, 1384: 0.14744110126304324, 11636: 0.39205475273257284, 4698: 0.2663865953216507, 7819: 0.1749101936804663, 4475: 0.16014225268491117, 10600: 0.11379081260014584, 4930: 0.16257292323745653, 6876: 0.16158199421148753, 3344: 0.3277844347371132, 1598: 0.21545814354264517, 394: 0.3277844347371132, 1063: 0.22847412219121524, 11528: 0.15570135811319633, 4468: 0.2594312790714684, 5194: 0.22993514987713234, 3394: 0.3277844347371132, 307: 0.23994398516905638}, {6570: 0.1771055313285919, 746: 0.41682754902322955, 7063: 0.3944587245691363, 10140: 0.4699657282573331, 9336: 0.3944587245691363, 10160: 0.19183923138127423, 7922: 0.2552816559066808, 1707: 0.4010663961176808}, {6186: 0.2536921255008302, 4537: 0.26156746109668394, 6018: 0.18852837800634678, 8853: 0.42821010485045896, 7005: 0.42821010485045896, 8871: 0.42821010485045896, 3604: 0.3324558968064233, 8491: 0.35092146697768656, 7356: 0.21889572505040814}, {3900: 0.2140411066685925, 7744: 0.6004514055067717, 4967: 0.3426633066483214, 3643: 0.5520125513429771, 11259: 0.1471823086249523, 6052: 0.3243286373302328, 6128: 0.21132160029922586}, {7195: 0.6103504804443131, 6769: 0.3167969997099496, 4130: 0.37980191033772964, 8099: 0.4147104079347793, 1706: 0.40388237795847487, 6117: 0.21853320851796898}, {4152: 0.6162400545387331, 10430: 0.5307120461529605, 4418: 0.5818873767751256}, {7346: 0.16126645376177015, 895: 0.30349527126206194, 4436: 0.4300747041750808, 2722: 0.1624332680313593, 11547: 0.15595854668104478, 11464: 0.1760445017179529, 9934: 0.26510654016389973, 6769: 0.2669955755954555, 213: 0.18760619548685475, 10854: 0.1925742533342827, 215: 0.24511497769501528, 4118: 0.4300747041750808, 5066: 0.395380263152844}, {1047: 0.36870662060670967, 1384: 0.34820332074712135, 3630: 0.6291101755136457, 4238: 0.44878994310547426, 10634: 0.3815990688851749}, {4412: 0.11057822265626441, 11292: 0.35818979881954355, 3636: 0.2733300995421852, 1384: 0.22667153147061558, 3157: 0.2699778414912295, 10533: 0.2921508717781788, 1192: 0.3413176626158956, 11227: 0.20590897461004024, 4905: 0.25910421141069534, 7583: 0.5039259689300419, 11486: 0.30849700849835143}, {1484: 0.2961912364037856, 3192: 0.21194867504559037, 5466: 0.35923700987040597, 6076: 0.08909042220816636, 3822: 0.1602367155103327, 8892: 0.18365908240809695, 11673: 0.393254485561856, 3463: 0.35923700987040597, 4471: 0.31756602026569114, 1063: 0.20934922971007472, 11528: 0.1856154232770895, 9223: 0.14630345624345434, 10873: 0.242588467772002, 4108: 0.2462286430170085, 3597: 0.2593118230968518}, {10365: 0.13889470822259511, 5013: 0.3395041082359013, 10160: 0.18213361264584416, 2342: 0.4461890057155348, 11063: 0.5116025511156428, 11079: 0.4703311990361173, 6758: 0.2306538093971126, 2722: 0.19322520832814288, 1702: 0.24434027671307504}, {11227: 0.6421357553224829, 8803: 0.422158934504208, 4826: 0.6398777271906689}, {11655: 0.33836795563941463, 9515: 0.49095678934203024, 8258: 0.8027879904404203}, {267: 0.9300070306370867, 9450: 0.36754172955677944}, {6052: 1.0}, {917: 0.19871213073644295, 7896: 0.19412167986539758, 9912: 0.20412034368279114, 11347: 0.23000188539976058, 6058: 0.18037915674796823, 3952: 0.3620074332314801, 3708: 0.2509462916743112, 4278: 0.32706717073937575, 6355: 0.22161862297065812, 10480: 0.1485972779351807, 8185: 0.24818952064317795, 4412: 0.10479924048307145, 3469: 0.2438785633793071, 923: 0.40053497190185267, 5517: 0.3620074332314801}, {7574: 0.6425892584614263, 639: 0.7662108358082612}, {11640: 0.17657180963649768, 11315: 0.19372254446594947, 4785: 0.22021710613001433, 8142: 0.27806351230948034, 10555: 0.394342574617269, 5749: 0.2595286589105876, 4624: 0.2155205517482766, 6570: 0.19135232536453714, 5149: 0.28038693340872806, 6923: 0.2295901556254654, 6176: 0.5077708994691389, 4187: 0.33225488884739085}, {10634: 0.779891496823629, 7151: 0.6259147331563615}, {3728: 0.7350427025973589, 5919: 0.6780208148415287}, {9080: 0.3864156155846283, 6018: 0.2145284796198122, 7640: 0.4055366749064204, 8484: 0.47137350097924813, 9654: 0.646535019426462}, {6592: 0.38872291353319605, 11590: 0.3822056765278112, 4704: 0.3339776684348875, 5522: 0.768942282821655}, {4280: 0.3669484200194813, 729: 0.2915349373416861, 10730: 0.4515241477842735, 8892: 0.21221858073950825, 6186: 0.12547554151383777, 6076: 0.10294423074862917, 1555: 0.2967943085043005, 2940: 0.4515241477842735, 4412: 0.09907950934614533, 4577: 0.32094266544828, 3980: 0.31673643282417363}, {11259: 0.1358524390835367, 6946: 0.5542295725031411, 1163: 0.17724398112546927, 1384: 0.1916163772233262, 6355: 0.19767573766778562, 7693: 0.4259929243370671, 10483: 0.09871418579000889, 7012: 0.31183433100242636, 2011: 0.31183433100242636, 10739: 0.4259929243370671}, {1837: 1.0}, {6226: 0.15388408676515405, 9929: 0.16833967105311323, 1871: 0.2943466782672446, 4325: 0.08986317988387327, 11547: 0.1408193063475254, 5601: 0.18019714383008825, 4412: 0.12586817448824142, 3020: 0.3883264034475186, 11655: 0.11799821556829859, 2376: 0.4644674851789366, 10784: 0.216285455206352, 10480: 0.12082380400130252, 8012: 0.2263703598347932, 1018: 0.24469523152097325, 7259: 0.20640186358109017, 8950: 0.16403215659533385, 5968: 0.1413901942897776, 4704: 0.14709800062146308, 1702: 0.18546385405320692, 8499: 0.2943466782672446}, {6186: 0.3171206788434335, 11655: 0.3467567985583122, 6844: 0.8827197730330221}, {4537: 1.0}, {10859: 0.8482687526080007, 2656: 0.4526476430485421, 9084: 0.27486402927896214}, {1923: 0.6007039090932214, 8015: 0.7994715839853989}, {1678: 0.5794165023057171, 5030: 0.2767280737689273, 10157: 0.26183223430967756, 4983: 0.4014532969754067, 154: 0.5794165023057171, 2171: 0.1491768692650948}, {11655: 0.12027393436029414, 10459: 0.2945976301208215, 10157: 0.17886495189401624, 11547: 0.1435351537032454, 4818: 0.16984771929978956, 6210: 0.17123327701841928, 3328: 0.21379642068989488, 2979: 0.3958156835988957, 8493: 0.18321405875828883, 7780: 0.2270488754887617, 10034: 0.33195420206099174, 6807: 0.3958156835988957, 11019: 0.19511813471980974, 3456: 0.3132759160909066, 8215: 0.2813451753219547, 7067: 0.135104157120594}, {8621: 0.35643844168641853, 6853: 0.5567137810896396, 9521: 0.3282332404363358, 487: 0.5567137810896396, 11355: 0.3812533397238296}, {10782: 0.4724001616067399, 3671: 0.8813841882595387}, {4785: 0.2032984056990533, 5365: 0.37703494617253075, 1163: 0.22363186026112652, 1384: 0.24176576616509204, 9182: 0.49412366344366276, 7646: 0.46876019822685644, 6095: 0.49412366344366276}, {11655: 0.29481797885795397, 3117: 0.716737204188255, 1109: 0.6319573873881961}, {6291: 0.4350282116169836, 9328: 0.44448274759041806, 7338: 0.41906640864176925, 10327: 0.5020530543820947, 3597: 0.43071059648620325}, {6511: 0.23205962988075515, 6527: 0.1695281675153873, 5479: 0.4787973581964428, 10480: 0.14467250796632342, 6896: 0.22990906566717514, 10160: 0.16553423510585033, 1970: 0.3778804869504648, 6618: 0.28866264801743, 7748: 0.16929837773959963, 3227: 0.2554842475516487, 3118: 0.4649758808760004, 1566: 0.22520040924969723}, {2622: 0.559994176408252, 10064: 0.5958732783639858, 2171: 0.27018880157002695, 2074: 0.5082711579725452}, {9457: 0.21097544448036284, 6130: 0.14250243054620126, 9154: 0.31014250074934574, 2171: 0.11300252177816465, 383: 0.23979309385409897, 1109: 0.21973667429778745, 5061: 0.20007485858118465, 3893: 0.25571276965992323, 11264: 0.19776461864712894, 8397: 0.24695153984249868, 11259: 0.08269284666073282, 5952: 0.20514806669289765, 1856: 0.21424923587679442, 9325: 0.26700795939881017, 10122: 0.21257822830938775, 4143: 0.31014250074934574, 3648: 0.236650396908538, 11315: 0.14604141341484103, 10514: 0.18001843902487313, 11642: 0.13281860572074883, 1833: 0.2829276352046345}, {4152: 1.0}, {8081: 0.5170770404796088, 1326: 0.3436676439730119, 9925: 0.5549033892335642, 10776: 0.4883998614052955, 3822: 0.2609055167896603}, {11019: 0.6081830105317649, 9420: 0.740675383650571, 4325: 0.28550551965697357}, {10160: 0.2803918709363051, 7133: 0.3386488637366385, 6592: 0.34724866111482944, 3495: 0.5279006731552917, 8725: 0.4219581690775108, 10754: 0.47894456462896123}, {10073: 0.48848187187897985, 8706: 0.5354690403632465, 10568: 0.5354690403632465, 7214: 0.43351040872225405}, {8892: 0.16096182984097354, 10616: 0.19952411149295252, 4325: 0.06091412761241425, 10160: 0.09371099894978543, 8070: 0.26322865633555254, 10560: 0.26322865633555254, 11019: 0.16882047328793792, 6487: 0.20361490049511502, 1025: 0.20833736982100906, 11007: 0.19268801724067094, 10811: 0.1282270040797678, 5968: 0.09584192713012567, 8803: 0.07071162787537646, 3727: 0.2139228655215376, 8108: 0.20833736982100906, 6355: 0.12214737814578937, 7713: 0.24199380805468584, 6076: 0.060014224422843904, 9186: 0.26322865633555254, 9331: 0.16007016695268486, 9912: 0.11250302196116176, 6186: 0.07314948349438807, 7593: 0.26322865633555254, 4321: 0.26322865633555254, 3194: 0.2139228655215376, 8111: 0.26322865633555254, 3110: 0.26322865633555254}, {7346: 0.18219204720213958, 11023: 0.4493275649952516, 6834: 0.26753928548129574, 4110: 0.4791580703295593, 417: 0.39833197929953273, 8517: 0.5301536560252782, 11259: 0.15495098226306844}, {11655: 0.14234282695793418, 6186: 0.1301772715086437, 4261: 0.4684433385955152, 10598: 0.4684433385955152, 3708: 0.24614021767519118, 3197: 0.43065367176464403, 7970: 0.22753272245124848, 2743: 0.27781736546135344, 9929: 0.20307039857733644, 464: 0.2761898128102276, 4785: 0.17718480484830623}, {7133: 0.4587260450148454, 6592: 0.47037513485881566, 9877: 0.7538684554559827}, {2733: 0.4854734073385354, 9265: 0.44630991240321277, 5323: 0.4854734073385354, 3964: 0.3405512551786502, 1283: 0.2691514166475561, 6577: 0.37552757706687945}, {4418: 0.19694655077298387, 5529: 0.31014706486740207, 11547: 0.13410586867092478, 6128: 0.13015130544549947, 7067: 0.12622873131950185, 4826: 0.15057788070497807, 213: 0.1613190963190081, 10854: 0.1655910373406136, 215: 0.21076983408463348, 10474: 0.3399801727691745, 9256: 0.17911058525480822, 8365: 0.4196198037692468, 6570: 0.1215443018653806, 9730: 0.20597423466746062, 4704: 0.1400852316685495, 11315: 0.12304983165395425, 9777: 0.21803845278166772, 10861: 0.16448264792995096, 4278: 0.253258591971738, 5771: 0.15358507709815913, 11132: 0.3399801727691745}, {5609: 0.16955388398568286, 10365: 0.06680427471754874, 6003: 0.2063651830451431, 2568: 0.2063651830451431, 5812: 0.051086566342526264, 5875: 0.17387415496294417, 7067: 0.06455645284092545, 10288: 0.10330065228851784, 11245: 0.16494916025459613, 11521: 0.2568332099421851, 4412: 0.04150178833534215, 5334: 0.10738933151764346, 10371: 0.17387415496294417, 747: 0.0947657226549623, 5508: 0.16494916025459613, 5029: 0.11817138608211086, 3712: 0.18913153419211706, 548: 0.18913153419211706, 8892: 0.08889275569490335, 11642: 0.07446165158678396, 2171: 0.06335222659063931, 9450: 0.09724598461117727, 10253: 0.1142652435388558, 10480: 0.0588463499129086, 3878: 0.10013691924394989, 3750: 0.15370499670554963, 1815: 0.18913153419211706, 7486: 0.18913153419211706, 271: 0.14969178102542324, 1742: 0.093804534723125, 3881: 0.13104127628622614, 4012: 0.1142652435388558, 8803: 0.06610113603032428, 6592: 0.08338664629513197, 4670: 0.18913153419211706, 5030: 0.09032881344882976, 1428: 0.17261093084955825, 4160: 0.09160152311953235, 4165: 0.17387415496294417, 6018: 0.05769363915116234, 10483: 0.04382693781603153, 5149: 0.09108357580255937, 4785: 0.07153743305853011, 10157: 0.08546655467850413, 5758: 0.07519544438062445, 10494: 0.11501147586134093, 4818: 0.08115787489298688, 9693: 0.09213195228847056, 7019: 0.12011383396189604, 9987: 0.08004513099714754, 11227: 0.07728095528517805, 11123: 0.14969178102542324, 9294: 0.10092313658260892, 5682: 0.1801245031649789, 4704: 0.07164300518105313, 7508: 0.16494916025459613, 11077: 0.14076678631707526, 6156: 0.18913153419211706, 11259: 0.04635981460095184}, {9372: 0.8404448309010889, 7356: 0.3549347945630342, 11227: 0.40947988695433696}, {10160: 0.3466854507983379, 5914: 0.5502055653556824, 865: 0.39932846058849125, 9199: 0.6462351078519433}, {7911: 0.3863810010813366, 576: 0.33719640922363325, 4583: 0.315954889458775, 1919: 0.3655199252187357, 1974: 0.3915121075902786, 10430: 0.20563512501886666, 8493: 0.25495557975196365, 7569: 0.369184043010592, 4437: 0.32666382630104907}, {9111: 0.4272170970863498, 4388: 0.4647052060282565, 5329: 0.4272170970863498, 11019: 0.22907736290384648, 10453: 0.23729899360505294, 5155: 0.3677998501904782, 8344: 0.28258868524520175, 2518: 0.30838260329460665}, {1384: 0.26398671938721857, 1651: 0.3302068445676651, 9405: 0.3252031089657046, 2622: 0.24070844325292273, 7366: 0.18497650881930328, 6371: 0.1450747239631726, 4832: 0.45109138208518973, 4019: 0.2659592614806972, 4376: 0.2082826792641298, 4650: 0.45109138208518973, 5500: 0.2536303032829862, 7346: 0.13001017933373984}, {4127: 0.5983201848993125, 4862: 0.6041272534486706, 4624: 0.5263489507756411}, {11655: 0.7225554063335164, 4785: 0.6913130150504956}, {11259: 0.26940564416558577, 6186: 0.30542672976471824, 6076: 0.25058206058613314, 10480: 0.34196726068777467, 2582: 0.6728421077425287, 11227: 0.44909423645339064}, {7346: 0.24110445188101995, 11655: 0.25419729310537814, 7133: 0.4679735599871189, 10785: 0.30752089762716, 3900: 0.2982028050505005, 10067: 0.6621043062153744, 6076: 0.19072755146462367}, {7356: 0.09813326046038255, 11655: 0.12436161625832559, 3615: 0.14346991109004079, 6355: 0.1899148045918781, 10894: 0.3143874231179048, 930: 0.25471984274330933, 865: 0.11361711030614702, 2882: 0.2028211458987923, 6681: 0.17459036654050963, 3724: 0.13784762922794874, 10514: 0.1478490318757399, 9162: 0.1943608236215323, 5441: 0.25471984274330933, 7099: 0.2100167384923272, 4258: 0.18571025611158562, 9176: 0.2770713948688004, 6759: 0.21432265499367595, 1360: 0.17739617659476586, 6128: 0.09751192190381723, 7193: 0.2028211458987923, 3227: 0.15223881441501858, 9557: 0.21929347079149172, 11640: 0.08402942947441959, 4772: 0.3102201882739175, 4771: 0.12988726228952752, 5097: 0.14669724967494088, 9987: 0.11726344944311193, 4376: 0.127932332034277}, {2171: 0.24234778281181782, 8372: 0.5724079454650691, 11315: 0.3132037425613426, 1276: 0.6309175675989607, 5968: 0.34272895219872146}, {9547: 0.24164882218460487, 301: 0.29848294370020906, 3774: 0.27229119906487487, 7819: 0.17325079107389663, 9313: 0.2015619715642096, 6134: 0.24164882218460487, 10494: 0.19743569066785527, 8598: 0.25697001062473984, 9015: 0.3246746883355432, 151: 0.3246746883355432, 6742: 0.13932054138623795, 9523: 0.1615310598142785, 6117: 0.13904167148509436, 11185: 0.20013588910913568, 4728: 0.3246746883355432, 341: 0.2114758513419771, 3754: 0.18926533291393655, 6226: 0.1286604965995529, 9735: 0.1615310598142785}, {412: 0.4748416364503495, 9387: 0.6223043549153237, 11548: 0.6223043549153237}, {5247: 0.3356952005760691, 1875: 0.2998242570604272, 7301: 0.27563721489627274, 5221: 0.2514501727321183, 251: 0.18613452340768222, 3658: 0.16622563113295669, 7869: 0.21311461789857195, 5862: 0.2998242570604272, 3615: 0.1552515355237807, 11528: 0.14241995370228283, 8892: 0.14091888244931983, 371: 0.20271330128172932, 6527: 0.142221586011201, 9695: 0.29567614977742074, 10785: 0.11021710450432695, 6076: 0.06835775595685639, 10480: 0.09328726284974605, 8142: 0.14319541952181822, 10483: 0.09039226270433544, 8185: 0.15580985984744686, 9246: 0.17576207245769784, 5881: 0.26148870222688086, 5149: 0.14439192048042773, 7114: 0.12738798812938887, 11669: 0.1811410873295509}, {1923: 0.26010464095597247, 8892: 0.24021552706410682, 11259: 0.12527845730403456, 10288: 0.17424446662081788, 1967: 0.4045131664002241, 4325: 0.11827242832226742, 454: 0.5110914923858908, 8293: 0.35852190173986664, 6047: 0.5110914923858908}, {10430: 0.5530082985806561, 7133: 0.6369050281094049, 11547: 0.537153429543074}, {3604: 0.609936534582513, 10785: 0.4168171516666115, 2171: 0.2919268079189963, 3157: 0.6074698549526827}, {10365: 0.4093762995898519, 7067: 0.5146896222027597, 2475: 0.6614120820227648, 10480: 0.3606101716330418}, {7356: 0.18247582626472625, 7896: 0.20941103962543503, 7058: 0.47364383444052643, 11195: 0.4320818000878142, 11655: 0.15655225250768512, 2171: 0.13264516669396942, 1274: 0.30555058711584304, 9207: 0.47364383444052643, 6180: 0.21202659063779736, 806: 0.3271974313789749}, {2885: 0.2836321511438548, 8501: 0.19877079624928456, 6914: 0.2780027407226851, 1758: 0.2149435731534521, 10274: 0.21800585081753093, 9511: 0.20709026090034102, 6954: 0.30239735671871626, 6855: 0.2780027407226851, 8558: 0.19204679986644987, 6069: 0.16842570932713916, 9987: 0.12798202126966973, 8950: 0.12773504487701726, 7748: 0.11010330649575013, 9313: 0.1877319347821714, 11464: 0.12378173249338714, 424: 0.30239735671871626, 8793: 0.17346199920500074, 11234: 0.1460427260631944, 2743: 0.17934129924432907, 8993: 0.2780027407226851, 5952: 0.18388877582656188, 4526: 0.22506825357231255}, {11640: 0.34023730978511396, 7346: 0.32333672803836794, 11575: 0.7444825873326744, 4537: 0.47480270691106824}, {11655: 0.11000370095909744, 9515: 0.15961045642328045, 8721: 0.29420687566546017, 5541: 0.3157293394639125, 3213: 0.2216220639253752, 665: 0.29420687566546017, 813: 0.3328126800498014, 9780: 0.3328126800498014, 9033: 0.3328126800498014, 5327: 0.3157293394639125, 9777: 0.2134417464204499, 9291: 0.30360851642422176, 9897: 0.1779201992458382, 11315: 0.12045568398552947, 10910: 0.15693446502469435}, {7880: 0.44761123688157384, 6570: 0.2297738051538655, 9432: 0.44399440820995123, 11618: 0.35114267366381907, 3121: 0.4639388577210263, 6521: 0.45953966487071624}, {5919: 0.329815586204388, 9417: 0.6028904409287528, 5126: 0.4000835825879194, 8010: 0.4899620448414024, 11227: 0.24634680518135482, 6742: 0.25870517673362564}, {4199: 0.6885037210303455, 2345: 0.6329616221554638, 9450: 0.3540087724982136}, {495: 0.3857384631770375, 6018: 0.20296281944707878, 1643: 0.4729322958421613, 4325: 0.20032016543541348, 7420: 0.5266068560066303, 9730: 0.37058078184111154, 11315: 0.22138644133504967, 7346: 0.19176322549541747, 6371: 0.21398322152622057}, {7356: 0.10203373275993137, 1358: 0.1815297557472328, 6452: 0.42429615804039517, 5386: 0.22945142785893044, 9069: 0.3748060108233141, 9084: 0.0933477118445246, 7760: 0.2144151597937359, 4964: 0.19728811450703626, 3178: 0.29664740752425195, 7479: 0.2512496127756071, 9752: 0.19512425638631253, 10480: 0.08963442193980005, 10483: 0.0667569397650917, 4710: 0.1575984765616389, 2575: 0.2341225674889075, 3604: 0.15496746737012926, 8473: 0.2880840657574783, 10453: 0.14710844206411736, 8189: 0.14563982054819635, 8803: 0.07738858503574304, 4826: 0.11729997367301835, 6609: 0.19728811450703626}, {8803: 0.12112674618353825, 5523: 0.450902229268195, 4991: 0.22947312603963257, 5617: 0.2992228223167626, 779: 0.3568752194381716, 7782: 0.30540377843373745, 3666: 0.3022223514568613, 3808: 0.3568752194381716, 9260: 0.2573191559535732, 5072: 0.3664429940794164}, {4826: 0.4451663002510214, 747: 0.3236356219335866, 8493: 0.3889753596390344, 9210: 0.5013872386786773, 9290: 0.5425826718762156}, {11655: 0.376234775949837, 4734: 0.7994476725821174, 4785: 0.46832767606560655}, {6742: 0.40034983164265353, 11259: 0.22869184284271526, 4865: 0.49208454318219846, 1041: 0.7384259310737056}, {11506: 0.1886792792407386, 5335: 0.31015408664264416, 7905: 0.12930368990203353, 3496: 0.238391188271072, 217: 0.21915999670969938, 8012: 0.13896737021040523, 815: 0.17742975333315048, 10122: 0.15021689611799335, 10741: 0.2601133724952325, 6117: 0.10209083268687637, 6570: 0.07835059491832433, 9735: 0.11860358283308749, 6076: 0.08028369952753575, 8001: 0.238391188271072, 10480: 0.10956249917130371, 917: 0.09918804013036388, 4448: 0.06780350512952212, 2171: 0.061376317355732686, 7057: 0.1672273231901889, 3469: 0.12173306502100675, 7381: 0.15819856177177785, 6058: 0.09003705496937461, 7114: 0.10128658087696274, 3632: 0.1745065527251875, 9223: 0.08925546118007183, 10564: 0.07021761356974489, 6126: 0.238391188271072, 2971: 0.20791047080211122, 6018: 0.07272005301324987, 7346: 0.06870732266140311, 11640: 0.07229860574972988, 4771: 0.11175451299524808, 3822: 0.09775573529210176, 4412: 0.05231100503095847, 6577: 0.1844024079842199, 4565: 0.238391188271072, 10160: 0.08486870960291884, 3209: 0.13339981299684348, 8361: 0.21915999670969938, 7642: 0.21915999670969938}, {4187: 0.27364734662230983, 4159: 0.2827171107584561, 5617: 0.3182100107253361, 8372: 0.29159410291646287, 11528: 0.2277747505933428, 11331: 0.26707417784779275, 6357: 0.479514236583258, 7765: 0.40214870804155556, 1931: 0.3795207415188714}, {4905: 0.6046435378675767, 4596: 0.7964961971754669}, {5096: 0.5549345239640827, 1969: 0.5173056984749741, 9004: 0.5668905791756871, 7356: 0.32105694149158354}, {4599: 0.47776728929600343, 10279: 0.44169432994409785, 5738: 0.7029349013314576, 8803: 0.28727523503448266}, {7366: 0.2831339239788804, 9088: 0.5611305321244168, 5097: 0.3655696560619918, 6226: 0.2736129513272909, 11547: 0.250383173618269, 11315: 0.22974093280222396, 4325: 0.15978084791318084, 3632: 0.5054304474870206}, {10212: 0.269064182614049, 6371: 0.16524292337440016, 4964: 0.238210655472765, 1307: 0.269064182614049, 1283: 0.2508984817935264, 7331: 0.2917189700843894, 5863: 0.2636584642877529, 4325: 0.08049415792178395, 3582: 0.3033651322387249, 4108: 0.21918361484881552, 3523: 0.2917189700843894, 4718: 0.233143706927221, 1218: 0.24100367681741255, 4490: 0.24724412064545198, 6720: 0.15110139863532254, 6494: 0.2440037715166604, 3372: 0.21015014967612852, 4862: 0.1477886942548162}, {4485: 0.32759885960714497, 11334: 0.3408696786141053, 4325: 0.10941656446259095, 10706: 0.4346796406088096, 239: 0.3583937530209434, 7182: 0.4728225844027427, 1542: 0.4728225844027427}, {1841: 0.40534005174331755, 5418: 0.7626105779072745, 9729: 0.2906002607027212, 1326: 0.41191751285441586}, {11315: 0.33523355480943495, 6825: 0.5293884656920415, 6335: 0.7793371004398904}, {1136: 0.6151359085891523, 5919: 0.6069775561816922, 10430: 0.4142270263779417, 2171: 0.28566069186009263}, {5165: 0.826083245037548, 3658: 0.5635481099855046}, {10483: 0.07132071647695146, 4537: 0.13025952479663633, 5222: 0.3077786676219179, 5812: 0.09016622986770549, 11640: 0.09334224435876264, 11586: 0.22188537302862413, 8986: 0.2581212070549546, 1844: 0.1442824097646715, 1331: 0.26842606118207973, 2150: 0.21077553477648303, 4649: 0.2042447244587599, 6826: 0.17063563761087805, 3615: 0.15937039657246868, 3303: 0.28294993733843626, 11091: 0.3077786676219179, 9729: 0.11728208818595821, 4382: 0.19546436789435967, 8539: 0.21324687916866336, 7645: 0.238075609452145, 1326: 0.1662439873629466, 1202: 0.1774792908433646, 8950: 0.13000815333661608, 6920: 0.181463424909278, 10448: 0.238075609452145, 6076: 0.07017130387080464, 7133: 0.1323366449530167}, {4499: 0.5899077687064551, 4325: 0.12910933003739597, 10365: 0.1514696995545878, 5385: 0.3702415011349913, 4821: 0.28022569880969606, 5606: 0.3778894688049696, 6923: 0.22001099434004168, 4251: 0.45341596325409506}, {6018: 0.2687356939847205, 7613: 0.6179856709198814, 4486: 0.738833430020769}, {9084: 0.12564546253643552, 3796: 0.24028571149646022, 4325: 0.08973197611458758, 7366: 0.1590063317069587, 2074: 0.1878023921057149, 11534: 0.23902257060233045, 6371: 0.12470664425458357, 8035: 0.38775943165872473, 3902: 0.17246459608839707, 7412: 0.27200657989213994, 10160: 0.13804471062838575, 11259: 0.09504737239221416, 2856: 0.2525659032624164, 4771: 0.18177629281775506, 3639: 0.33818047797325995, 335: 0.33818047797325995, 11547: 0.14061370463384393, 10365: 0.10527260468766476, 2857: 0.2795455149758935, 4537: 0.1641093571999151, 2739: 0.20374543311949803}, {9401: 0.501193735274825, 4325: 0.23017073328300391, 11468: 0.8341620185918517}, {}, {10170: 0.6550549040471522, 1696: 0.46394638380715064, 4012: 0.596369705498229}, {306: 0.35777632797706416, 4418: 0.2424041018893416, 1969: 0.3379489753102918, 9004: 0.3703421224824366, 10559: 0.4269249703454309, 10811: 0.28847469762682615, 9907: 0.5444180116905225}, {10198: 0.4565049417733457, 10626: 0.5004162429860112, 6758: 0.2454077317546845, 5936: 0.345692052592103, 7715: 0.5004162429860112, 11486: 0.33323033418001535}, {4930: 0.229892323670558, 4417: 0.46351584177821825, 3906: 0.32514840015260943, 8646: 0.2001107152686156, 7814: 0.46351584177821825, 7183: 0.46351584177821825, 1053: 0.29927759689666444, 4789: 0.25937588995101307}, {3900: 0.25944972104547176, 11376: 0.7278365921612612, 6363: 0.6347753440080269}, {8189: 0.3291251902252113, 11259: 0.10803434004491985, 6834: 0.18653273261598508, 4947: 0.19106169750442326, 5812: 0.08059548239342516, 10480: 0.13713226922100286, 7748: 0.16047465438471592, 8176: 0.36963173459496973, 2475: 0.25152074686974557, 2759: 0.4407416348405553, 6923: 0.17380237087611977, 8153: 0.4197521494175267, 6925: 0.24901805782739006, 9693: 0.21469919041329816, 4821: 0.2213702591529018}, {3900: 0.13395568886571793, 3781: 0.279690741566765, 2713: 0.2178624017709586, 7133: 0.16157845266277202, 1163: 0.15635470363601237, 4160: 0.18200385616702403, 10590: 0.2848420211044575, 9328: 0.2573496160526327, 1598: 0.3213688186542139, 9256: 0.18200385616702403, 8985: 0.3757870761693771, 5609: 0.2102843064426664, 9648: 0.31515703945943074, 4556: 0.32773890886807105, 6527: 0.1370103203610017, 11259: 0.09211271539176365, 356: 0.2636081781924581}, {3203: 0.4791984582588315, 5473: 0.8516892794042294, 9084: 0.21211838428166283}, {4826: 0.6784502057294886, 1170: 0.7346463900037995}, {10627: 0.4164248642395735, 2734: 0.307384837173413, 4325: 0.1517373274571313, 11518: 0.45430941538826536, 6925: 0.3704710975878585, 1970: 0.5328826853229706, 1638: 0.2854309765945074}, {6058: 0.16298552748873094, 6130: 0.23715776198888444, 4818: 0.18517599235061963, 10279: 0.17823757455360553, 7554: 0.4315369220177406, 3157: 0.23119548091059508, 11259: 0.10577808604823187, 495: 0.2501833518118555, 11590: 0.1870714503438528, 1163: 0.17955068662251977, 2969: 0.28637182821119644, 8892: 0.20282448585907165, 1508: 0.2624189482833972, 6896: 0.21337504722840836, 9126: 0.21605427846298234, 4776: 0.4315369220177406}, {11227: 0.3782511174418682, 382: 0.9257030258965226}, {10932: 0.3071817897061424, 817: 0.4421867544539322, 4624: 0.19517653480225675, 9223: 0.15173223244117634, 4918: 0.40525954056411967, 11240: 0.2807872714063, 4187: 0.23127196130597563, 1780: 0.24980998329915355, 11024: 0.31347985502562575, 10064: 0.23010732365825673, 1576: 0.3725669569447939}, {11315: 0.5642441810777827, 11359: 0.5495138877937266, 11640: 0.39529469212257523, 11547: 0.47265748446565864}, {4818: 0.12939461078135003, 11640: 0.09145115423960959, 6747: 0.22856603055490668, 6371: 0.096978774159239, 3399: 0.30154315013222877, 3671: 0.238662110959555, 4325: 0.06978054055174349, 4216: 0.30154315013222877, 7366: 0.12365210550464653, 3782: 0.262987817485329, 6527: 0.10994131044480292, 37: 0.30154315013222877, 10799: 0.2772174436064547, 3227: 0.16568499138223297, 11415: 0.2207349021962752, 10480: 0.12206533892557808, 10608: 0.2450606087220492, 4638: 0.2772174436064547, 2245: 0.19820931209743914, 8142: 0.1440163591513965, 512: 0.2332522581845683, 4624: 0.1116237255556982, 9547: 0.22443248483842929}, {4019: 0.3658012668119084, 4309: 0.41172472625828616, 11023: 0.4410025396781013, 6458: 0.6204326109796822, 10782: 0.3424212584182551}, {6978: 0.5129869606593505, 1047: 0.24433438707963193, 4437: 0.3042346302835699, 6186: 0.14255564623538766, 9085: 0.5129869606593505, 4704: 0.19431940653005259, 1222: 0.4473964045380445, 6205: 0.25136639909155256}, {4367: 0.1794330517158559, 1063: 0.22625704573504218, 11458: 0.28921606675146366, 5910: 0.4223191072626747, 4875: 0.4223191072626747, 10019: 0.2397937858950487, 9508: 0.34321382347886437, 2507: 0.304460453273536, 11185: 0.26032583708047263, 4376: 0.19499764051908558, 10636: 0.3201127332263834}, {5334: 0.7483018869650858, 11315: 0.4385086527012361, 2722: 0.4977493821901124}, {11655: 0.6788605409284686, 6570: 0.7342672306254086}, {11331: 0.2632990237206514, 2102: 0.34605122849703124, 3135: 0.29422108577623785, 6734: 0.3200388542873991, 6226: 0.12682343096981424, 4418: 0.1310030096782293, 7732: 0.29422108577623785, 6072: 0.3200388542873991, 354: 0.3200388542873991, 5512: 0.3200388542873991, 2960: 0.15041984303975622, 7346: 0.0922392013949217, 2448: 0.24755921478104673, 11541: 0.3200388542873991}, {10279: 0.19256167277642075, 1026: 0.4066068673574272, 5749: 0.2078223370016538, 11102: 0.60656291663108, 10861: 0.20736054773425436, 11640: 0.14139311735861274, 7366: 0.1911791798652392, 4127: 0.19618034084837885, 4785: 0.1763428895898855, 4829: 0.46621747281200876}, {11227: 0.1709903842669683, 7400: 0.32164428739189965, 7259: 0.1709592233196474, 9087: 0.2438024474692608, 747: 0.12387277485450589, 1067: 0.2956970074176867, 10861: 0.14305842122753723, 514: 0.32164428739189965, 8345: 0.2613965690338664, 10285: 0.2545715416628462, 2446: 0.26974972744347375, 7067: 0.10978716140205502, 7831: 0.23544928905965343, 3937: 0.2956970074176867, 7480: 0.23544928905965343, 6018: 0.09811599917003398, 9328: 0.22027110327902585, 5985: 0.20767559748744574, 5867: 0.18547484291620378}, {9223: 0.18036263919146991, 11555: 0.2938083695042721, 7151: 0.19058451350436484, 8442: 0.3914948218129636, 4871: 0.48172810165436797, 4418: 0.19718802988411693, 11315: 0.16028781255233535, 5181: 0.3726304759122339, 5177: 0.48172810165436797, 6371: 0.15492774667908052}, {7346: 0.31325630859810977, 10875: 0.9115325274983813, 11259: 0.26641872388384896}, {7601: 0.5975232371872103, 6637: 0.3392530993779625, 953: 0.3066202762233067, 6779: 0.30107514370168514, 7173: 0.3718859225326183, 9223: 0.1514548732449232, 3975: 0.2837629507707115, 1300: 0.2658968783672729, 4152: 0.17535905871776955}, {7346: 0.1447578595165752, 5577: 0.2591543476554452, 10735: 0.478341626563973, 7137: 0.4295277448443997, 9996: 0.30960350463014435, 9006: 0.478341626563973, 6896: 0.3231039784899987, 11464: 0.20559279551929988, 6570: 0.16507504546670781}, {6186: 0.16066400562842922, 4514: 0.578149951343178, 7845: 0.3250701594527942, 1750: 0.44721553668508074, 4027: 0.578149951343178}, {11562: 0.20610669618574765, 10483: 0.09416124131651152, 668: 0.40634506810356374, 4967: 0.23189144600151027, 1566: 0.1968039190788097, 8493: 0.18808787086645626, 6239: 0.40634506810356374, 641: 0.2929443671686107, 10173: 0.37356491418347476, 7133: 0.17471757682728126, 2185: 0.33023190776177713, 3022: 0.3543897533723551}, {11019: 0.3000278948465021, 8060: 0.5595360652682763, 11259: 0.11466937177832123, 10475: 0.3801839063233103, 1005: 0.36186458173668795, 2622: 0.24962983069366856, 6128: 0.164640134917639, 3986: 0.4678102014850349}, {2645: 0.18169218908819135, 11607: 0.22818432282857593, 7005: 0.17197245271062525, 4363: 0.24820739496537264, 11259: 0.060840456153078176, 1560: 0.17893894611308267, 6626: 0.24820739496537264, 404: 0.14720312762702525, 11655: 0.0981251978003726, 5357: 0.25558539675294245, 3762: 0.26243760543690503, 6018: 0.07571443831908116, 2659: 0.14809203428138915, 1881: 0.12945581343095736, 661: 0.1631508359318083, 8179: 0.20816125069177924, 6592: 0.10943274129416065, 9515: 0.10943274129416065, 11547: 0.09000776892595141, 3192: 0.13462801315248368, 997: 0.24820739496537264, 11556: 0.24820739496537264, 8666: 0.24820739496537264, 401: 0.24820739496537264, 10430: 0.09266447985843088, 10402: 0.17411322355116587, 11528: 0.11790135343321521, 10542: 0.22818432282857593, 4010: 0.16636383163854107, 7067: 0.0847208745823143, 4826: 0.1010632810194949}, {1427: 0.43789067657177727, 10244: 0.5221322592476031, 6901: 0.5221322592476031, 10590: 0.3957698852338644, 825: 0.3261499057480665}, {2171: 0.22629699599358868, 5030: 0.41978848402909363, 610: 0.878958075382912}, {11655: 0.5264168758644243, 11259: 0.3263936641373913, 10660: 0.7850811733929857}, {10785: 0.1155032601955651, 2909: 0.2635100500771861, 5061: 0.18634353609813187, 3450: 0.2635100500771861, 9919: 0.1936816846909012, 6575: 0.3142042184099745, 3362: 0.24304595691563938, 8284: 0.3142042184099745, 3481: 0.2265177140044896, 3867: 0.2204088614215842, 10459: 0.23385586259725896, 8478: 0.2151759336541914, 11547: 0.14823972516966777, 3069: 0.3142042184099745, 4337: 0.3142042184099745, 3370: 0.3142042184099745}, {2694: 0.7065931206148272, 3187: 0.5556991289171799, 1455: 0.31509736958164514, 4947: 0.3043653851922323}, {1257: 0.40880616347344845, 1921: 0.43787742308372024, 4368: 0.4418906279623789, 4325: 0.15556971848429396, 6527: 0.31888857158702516, 6720: 0.29203115673465063, 1566: 0.3255959194886538, 6919: 0.35872942211248793}, {8896: 0.7562874167408724, 6130: 0.3809200009717424, 4599: 0.4028817496268705, 747: 0.3472975553566385}, {8803: 0.20408228943981327, 157: 0.7597096608467715, 2354: 0.6174071997904526}, {213: 0.27109119716052227, 10854: 0.27827004722955606, 215: 0.3541914624561125, 11678: 0.47105797087001006, 8430: 0.4209244583432752, 4390: 0.38864568598778837, 10747: 0.4165396388958275}, {10866: 0.7844183948769093, 3222: 0.589675883357906, 11259: 0.19227619292259435}, {6527: 0.38502607115739446, 11547: 0.3829516774549826, 3906: 0.7407910652786859, 9223: 0.39538782842518755}, {213: 0.2544057922215999, 10854: 0.26114279090758524, 215: 0.3323913153510409, 7250: 0.5361604866787669, 8307: 0.2978119199053281, 4074: 0.46159142043569956, 3980: 0.40911062389914254}, {9929: 0.20096861183976827, 11486: 0.41921715263073217, 6018: 0.18398820265550678, 8425: 0.3212054073398681, 11234: 0.2238930534804197, 1081: 0.34504435716987075, 747: 0.17854130417700462, 10359: 0.34504435716987075, 8812: 0.30764581490032744, 6925: 0.261930120261007, 7716: 0.40431964424873185}, {10430: 0.32455055119771264, 3775: 0.46763291611128827, 10160: 0.30948611961501205, 11259: 0.2130892399090638, 6067: 0.6267205479431067, 4947: 0.3768541732195966}, {9993: 0.5018174234488694, 6058: 0.3551812448539627, 10886: 0.7886859684482332}, {7005: 0.8078202313305023, 8189: 0.5894289387645757}, {5547: 0.2604827469571943, 11337: 0.3558421767062719, 6718: 0.3558421767062719, 4325: 0.08234595755461258, 3874: 0.1938293659320013, 2132: 0.26972403053461824, 10811: 0.17334197909707535, 4599: 0.15897664011266563, 11680: 0.28274495679564704, 9401: 0.1793072362542019, 6371: 0.11444179075531002, 3222: 0.26749952736408944, 11640: 0.1079188095390451, 4862: 0.1511886310554871, 11427: 0.4037670688608076, 7356: 0.12603232833198624, 5575: 0.3103441659350405}, {3129: 0.4831141847933254, 3227: 0.2654504654110579, 3790: 0.274313374410352, 9101: 0.4735635515212481, 11655: 0.1468007614605839, 10910: 0.2724749943434188, 274: 0.29780112336605163, 7142: 0.42134316431756746, 747: 0.18605862745824378}, {7599: 0.4590647971060051, 9515: 0.24904768669399313, 4325: 0.1307178246939989, 2171: 0.14543220154534334, 5554: 0.3868402317170468, 7936: 0.5193032457035284, 9516: 0.5193032457035284}, {2871: 0.34743336618972886, 3006: 0.3204518192959622, 8155: 0.2986596217302898, 6160: 0.4142727354847153, 9619: 0.361303788532512, 10480: 0.12889674086262168, 11464: 0.16957620754195876, 10365: 0.11247068763486175, 1508: 0.2519205426040365, 8756: 0.4142727354847153, 2739: 0.2176766598812746, 8307: 0.21154594769604906}, {1236: 0.5792905056509846, 10064: 0.3604037116895955, 576: 0.3885762734857157, 3222: 0.32302899851412226, 10061: 0.33351669179158144, 4734: 0.40982805755866236}, {11359: 0.4928695065542478, 7852: 0.8701032407185784}, {6146: 0.3875688539069738, 8427: 0.4369770642633319, 4325: 0.11594641928503133, 5771: 0.20808414763073182, 6180: 0.20619684110054776, 2102: 0.3667701814650137, 1696: 0.2354913196155789, 11331: 0.27906339511929434, 4947: 0.21720110625343297, 2081: 0.3263509106204352, 1638: 0.21810519694643102, 9420: 0.300795090371742}, {3089: 0.34171856351803753, 4390: 0.25481507231792727, 7138: 0.34171856351803753, 2986: 0.331136794559002, 1570: 0.27039317940786434, 11547: 0.1477572093779424, 4087: 0.25876914097027776, 240: 0.4074585168000768, 793: 0.34171856351803753, 2005: 0.234959262019422, 1747: 0.30326315604888393}, {4821: 0.8898984787615669, 2171: 0.45615863194490697}, {11259: 0.5172404888256747, 11655: 0.6411996663149417, 8803: 0.5668555941657313}, {4448: 0.3279300114217344, 10480: 0.35873586793499473, 9627: 0.8739396344518694}, {4221: 0.48188267697901704, 11533: 0.35691574161304795, 3724: 0.3029109090563593, 2480: 0.42184360810583854, 6459: 0.60884578547547}, {}, {6038: 0.2240616633788726, 8067: 0.22091432111916529, 4837: 0.3107974143523367, 7688: 0.14645909261481962, 7922: 0.14723700684718424, 1455: 0.13948145496039607, 10634: 0.1532082084590166, 8880: 0.3200359137894043, 10595: 0.2710588307005572, 11512: 0.3107974143523367, 10238: 0.3107974143523367, 6232: 0.22750918676094947, 1921: 0.2024369319702535, 2172: 0.3526556693310298, 7877: 0.2606529047709447, 11534: 0.19158166339699825, 11585: 0.16584569057818135}, {6056: 0.5067115532340098, 6371: 0.22926700396404234, 2734: 0.3341867331538384, 9256: 0.34526541895761925, 10943: 0.5978600099909218, 1455: 0.31992884993724546}, {4187: 0.1504280835085903, 4508: 0.2210672245970714, 7699: 0.31528084303836323, 3781: 0.19618935401130633, 2171: 0.06786561908252592, 10480: 0.1067043392057109, 8892: 0.1611865947613968, 2612: 0.22989278963976167, 10483: 0.061082438857794116, 6758: 0.11884122424375981, 746: 0.2038992255860511, 3728: 0.15632970482435765, 6825: 0.18019884752472606, 3708: 0.13850476017100705, 7059: 0.20862828930418886, 10236: 0.2635962252682171, 9450: 0.1355335828871305, 2582: 0.16137022491490546, 2656: 0.18300092725466915, 9800: 0.15727372359035294, 10564: 0.10101418477266937, 1696: 0.1238915284801986, 10785: 0.09689947368560584, 3942: 0.16248591838285095, 10061: 0.18019884752472606, 1420: 0.13698321582792888, 11359: 0.11113135266901501, 7356: 0.09336061935541379, 5749: 0.11750135238312805, 3038: 0.19003314045281303, 4412: 0.057841833694239, 4713: 0.12916358317301746, 6570: 0.08663458250187181, 11259: 0.06461255752587565, 7429: 0.1448347882974704, 9290: 0.17019578995759577, 7922: 0.12487593986451259, 4238: 0.15281980242664928, 11298: 0.1276667892864501, 6920: 0.15541386997323492}, {3724: 0.2089536541920599, 693: 0.281540718291729, 9084: 0.10460187999202497, 10480: 0.13067663562608847, 645: 0.3228159992198665, 11315: 0.107412189976357, 10732: 0.2066843602976054, 5812: 0.059031208138193965, 5927: 0.25549891546714654, 4325: 0.07470332161229847, 2949: 0.3999045728844121, 10785: 0.11866899986852601, 11227: 0.13190570735660206, 11298: 0.15634852930371892, 5353: 0.22366561305643884, 6186: 0.08970840764599783, 3900: 0.11507324837558144, 4704: 0.12228266641745288, 10211: 0.20188161198003635, 2479: 0.1472560452115757, 10365: 0.08764114628328695, 10784: 0.17979824374367642, 5758: 0.16698211020074089, 4104: 0.33241175288237357, 6418: 0.27073239357070156}, {8176: 0.6168080892156887, 11309: 0.45941124332060684, 5030: 0.2699851884041172, 3796: 0.26925008303222664, 1518: 0.34375958006633145, 7067: 0.19295378094202753, 6720: 0.24556505448066382, 747: 0.2177096115676603}, {11655: 0.2806685706722216, 10988: 0.7001280576447876, 10684: 0.6565408260994506}, {6527: 0.21786737971504105, 6130: 0.2524136533452896, 11507: 0.4529424105224454, 2507: 0.4307952708552422, 7346: 0.1722239698081225, 2171: 0.15384783772320526, 10785: 0.2196660798882412, 8943: 0.4529424105224454, 1389: 0.4374236995956388}, {6130: 0.26475414369376843, 2435: 0.48175184022189566, 7615: 0.2892155545668631, 1617: 0.39151411387052143, 1881: 0.32690196654560433, 6825: 0.25313307511888816, 1384: 0.21669736061378517, 3232: 0.28241111202234975, 622: 0.4040252934351385}, {4941: 0.31678014473808264, 9453: 0.31678014473808264, 11655: 0.1147760567521668, 6230: 0.26170797052569006, 9766: 0.329426812631591, 5812: 0.08986419972003225, 11567: 0.329426812631591, 2697: 0.29217911570048233, 9880: 0.30697062702292555, 6210: 0.16340598173209644, 6250: 0.2328355677194388, 10483: 0.08752859614352547, 5978: 0.2764994818481333, 2897: 0.329426812631591, 4537: 0.15986145264618729, 10480: 0.11752448728378795}, {7133: 0.23593931505701257, 5058: 0.2552719160085222, 11500: 0.4244583671126193, 2180: 0.5487300063686547, 3126: 0.28619731228531226, 4599: 0.24515152630009404, 6434: 0.3336844284134845, 5353: 0.38019191605592406}, {100: 0.9344543269087353, 9729: 0.35608301127903097}, {11511: 0.2119511424621156, 4826: 0.15843542236685387, 6925: 0.21984691324626116, 8868: 0.38911109019238804, 3298: 0.3393593133326361, 1656: 0.35772121387852257, 7088: 0.38911109019238804, 6058: 0.14696187753805762, 5365: 0.2729547451330241, 6186: 0.10813137013938058, 1148: 0.27657956070490525, 9540: 0.38911109019238804}, {9773: 0.2922788746958715, 9205: 0.31539112681104714, 2656: 0.3238531264416369, 8491: 0.34460255533178963, 3813: 0.5293067879301204, 818: 0.5579462806930875}, {7346: 0.2971177767837972, 1566: 0.4992919443320395, 2995: 0.594463205581842, 10638: 0.482027620578736, 8803: 0.27693221418381075}, {6180: 0.32535326529747743, 2851: 0.9455925405587066}, {404: 0.2718562500096281, 7790: 0.4583919697067407, 1028: 0.3474555230592078, 8303: 0.4583919697067407, 7346: 0.1321143000144144, 4865: 0.24177077099733016, 4963: 0.36280311819103206, 9998: 0.4214131541575631}, {10099: 0.8116028293759834, 1063: 0.5842095919692677}, {3222: 1.0}, {5368: 0.4405613801645601, 5142: 0.39674765769394305, 2530: 0.49642694461403747, 10775: 0.405370236418437, 1612: 0.4333030186431757, 2722: 0.2235636834188666}, {9868: 1.0}, {6527: 0.35076420372366207, 3194: 0.7818579652603085, 3157: 0.5154246749478681}, {8892: 0.35485307414651784, 5030: 0.5326288115739591, 2659: 0.45046715447881763, 6570: 0.24814083985655944, 4537: 0.31953403382259055, 6825: 0.3967086412890887, 7067: 0.25770441660310156}, {5712: 0.7681090214807019, 10365: 0.3110835241890513, 2171: 0.2267500183339594, 5812: 0.2095321323777426, 10480: 0.27402632532077864, 4818: 0.37792308716993533}, {10871: 0.4943808250936633, 7168: 0.5377625210892123, 6597: 0.5377625210892123, 2227: 0.3876865102920991, 6018: 0.16404179754194304}, {4862: 0.16275189147719032, 6747: 0.2903531557888748, 6371: 0.12319456680990669, 4101: 0.38305781937433886, 10799: 0.35215626484585083, 10608: 0.31130663173894874, 3830: 0.35215626484585083, 9050: 0.35215626484585083, 11317: 0.38305781937433886, 10209: 0.3340800142326953}, {1881: 0.4273859077300153, 1619: 0.7146600740390974, 8950: 0.34613493220469466, 7399: 0.43219541084545804}, {10694: 0.5923177966354737, 11513: 0.5165839931252788, 9610: 0.5165839931252788, 1204: 0.3397669552537169}, {5554: 1.0}, {6018: 0.12264915103820802, 10365: 0.10915749390608202, 10784: 0.2239396279954559, 6717: 0.35066039804985943, 6925: 0.22716808727569104, 5624: 0.5231037910659947, 3095: 0.2820444756267113, 1967: 0.3182252000443555, 6923: 0.15855216485254287, 11598: 0.3267567730798347, 9084: 0.13028217409305612, 6448: 0.27232817857003305, 6834: 0.170165507081633, 1923: 0.20462090798494645}, {8803: 0.10936196027950883, 9704: 0.2000806531347724, 213: 0.17758729609696605, 10854: 0.18229004036235924, 215: 0.23202488600530216, 9086: 0.4071070448177532, 3362: 0.3149089524499136, 7297: 0.330851157369435, 747: 0.1567864914166755, 11518: 0.28206732931083983, 9793: 0.37426542167867943, 2753: 0.3414237985396057, 9919: 0.25094882140300884, 6720: 0.17684696155274032}, {6153: 0.2050044484783524, 4367: 0.12081888931810926, 5526: 0.23848347897200592, 6354: 0.2614232522824633, 3935: 0.19473984098090338, 9115: 0.2155437056615485, 10480: 0.11511078623965344, 6018: 0.0867435346335519, 8344: 0.1729220428181839, 9044: 0.24800434512080174, 7041: 0.21164566464868273, 6619: 0.28436302559292076, 10365: 0.07720156864521824, 6128: 0.10007812303063722, 251: 0.17653600399929373, 2425: 0.19947577730975116, 4785: 0.10755795428070147, 8949: 0.24800434512080174, 4412: 0.06239876469551087, 10989: 0.28436302559292076, 7114: 0.12081888931810926, 9282: 0.24800434512080174, 10638: 0.13296252112437132, 10776: 0.1477751119866839, 10160: 0.10123496265059516, 6923: 0.11213591848529544, 3959: 0.28436302559292076, 6826: 0.15765376645470613, 10564: 0.08375851972306375}, {8268: 0.44432467289670896, 3690: 0.44432467289670896, 4043: 0.38751328268855656, 6320: 0.44432467289670896, 5140: 0.44432467289670896, 8072: 0.24522600995993277}, {6570: 0.27709309903252366, 10365: 0.2288898888172883, 1728: 0.5196969313365953, 7962: 0.7750767270568041}, {8803: 0.1951321887013325, 2742: 0.4690086726307135, 9372: 0.609195125192349, 11227: 0.29681085756627623, 11007: 0.5317319890719888}, {4325: 0.1528032392341689, 11585: 0.35235017612327735, 8682: 0.3989311078607803, 4862: 0.2805494434300138, 7639: 0.4227662046464717, 4488: 0.4575008104417879, 2603: 0.47603387389076157}, {6278: 0.3057491980933127, 2891: 0.3421649514400797, 9912: 0.19293202589985128, 7228: 0.38604257125689273, 321: 0.4514122114803808, 7784: 0.33044169734126694, 666: 0.4514122114803808, 8379: 0.29402594399449994}, {10785: 0.30997089606346656, 10638: 0.39427097081101353, 6143: 0.7600990079273108, 6205: 0.41318027943897595}, {892: 0.6566245335418072, 11257: 0.484752334271783, 2674: 0.44541272383271974, 4367: 0.36805828589637685}, {11232: 0.3824909218944438, 4429: 0.5521500188291342, 751: 0.4500050643999246, 3546: 0.5521500188291342, 6371: 0.20360955305862152}, {7469: 0.42518501087735955, 2475: 0.2782156257405212, 8856: 0.37711010677864165, 11199: 0.48751926532154793, 5968: 0.17750645599131679, 8621: 0.3121363492802424, 2674: 0.2506683572129967, 11511: 0.2655546651371561, 10480: 0.15168665235515058, 3790: 0.27681458953149973}, {7252: 0.48305552549623143, 3468: 0.48305552549623143, 9778: 0.4440870828580149, 2762: 0.48305552549623143, 6644: 0.3205600422718617}, {5720: 0.16447160922020476, 11545: 0.18432336481345102, 316: 0.16072670180811022, 8719: 0.1922776995991199, 10348: 0.20519717170311175, 10480: 0.09101435908698809, 2194: 0.2377269241733291, 9084: 0.09478481572037466, 10483: 0.0677846741892208, 11547: 0.10607660485640802, 1937: 0.29251916887130996, 8372: 0.17788181898555344, 6076: 0.06669224883421439, 3675: 0.18031459194940905, 2354: 0.2377269241733291, 10097: 0.2377269241733291, 4797: 0.2079221725193933, 2751: 0.29251916887130996, 9223: 0.10952138588248446, 7016: 0.27858849759900783, 1427: 0.31917348995341166, 10365: 0.07941587570526548, 713: 0.2551176432306763, 8894: 0.29251916887130996}, {7346: 0.16133721860332714, 5749: 0.24953152268097234, 345: 0.514627305047359, 11298: 0.2711193333551202, 4624: 0.2072186234558345, 1075: 0.7282978835309729}, {4930: 0.3324580676112517, 5126: 0.3419022911247244, 3754: 0.30033940722583446, 8228: 0.4461328724351389, 3166: 0.41871037199737554, 4826: 0.272932758789393, 3140: 0.4832447524446527}, {233: 0.5737411865534601, 7356: 0.23299918457762972, 11618: 0.33041903539749684, 10432: 0.5517153101980841, 8478: 0.4505173785531554}, {9432: 0.28435962178968016, 9693: 0.21811481861286175, 2687: 0.30663415535639327, 10279: 0.18493544125862402, 7024: 0.3755121693264925, 4325: 0.1036152563221183, 11313: 0.4477533499466311, 3313: 0.3001117792501959, 10331: 0.44155862457612893, 11259: 0.10975304768265398, 5431: 0.30663415535639327}, {9084: 0.4431329412903968, 9210: 0.8159556962608724, 10365: 0.37128088838877327}, {3822: 0.4405072726761961, 6128: 0.37806551016410406, 4021: 0.8142602856235437}, {1020: 0.9099212812727419, 2743: 0.4147809806234751}, {2610: 0.4810418652025261, 3619: 0.5356368056674474, 8621: 0.433300275771482, 1455: 0.30372145597169325, 1745: 0.449105872301443}, {8088: 0.40097948398196076, 6923: 0.1885422123322641, 8976: 0.41698760333623097, 5797: 0.4395497803577953, 8174: 0.3499923255746129, 1691: 0.2593329501239947, 10117: 0.3624091876061262, 337: 0.3398470105845619}, {4818: 0.20262545259529086, 11227: 0.19294601494067445, 4420: 0.4341085740656173, 4879: 0.4722014070261351, 9382: 0.4118256952374795, 1643: 0.335640029316444, 4616: 0.26811707343693125, 2363: 0.3198300751840639, 9704: 0.23207253996609686}, {2610: 0.5173905917654794, 8521: 0.4095493916833058, 9223: 0.2094736229663381, 7346: 0.1612492010227235, 3900: 0.1994362347189554, 2077: 0.4428113605451777, 4611: 0.48794503995324323, 2171: 0.14404406622132382}, {4325: 0.15311553595932773, 5029: 0.31775725244866293, 10808: 0.6616592632028077, 426: 0.6616592632028077}, {5788: 0.6600425519935342, 9223: 0.337594213263897, 11242: 0.6710990811561234}, {895: 0.4964093649991603, 10365: 0.32326564605394403, 2462: 0.6134277858574475, 617: 0.522286718154393}, {2694: 0.7071067811865476, 6828: 0.7071067811865476}, {2674: 0.29125681455523317, 9996: 0.3491764180519166, 1207: 0.4940313697414343, 3724: 0.28182270117141633, 10375: 0.4940313697414343, 1113: 0.47506554661312034}, {6570: 0.11300761981832368, 6262: 0.3063547524653795, 8892: 0.1616062124948638, 2965: 0.31610161497015543, 5407: 0.25169649556243606, 1651: 0.25169649556243606, 10634: 0.169496316652267, 2171: 0.08852506537845391, 2029: 0.3438393901189779, 4819: 0.28836383982133296, 4771: 0.16118718091054673, 10332: 0.2201446785896671, 9777: 0.20272448975038646, 7067: 0.11736303767578454, 5867: 0.1982735567537607, 7346: 0.09909881355908912, 4448: 0.09779520803912571, 11331: 0.19150760378262088, 2117: 0.31610161497015543, 1395: 0.27943427071125854}, {8587: 0.2700719564976197, 3754: 0.20352873898081025, 1057: 0.1487808433072523, 1839: 0.2250613728692382, 1283: 0.1487808433072523, 6628: 0.2250613728692382, 1373: 0.20758318977864024, 11118: 0.2250613728692382, 9608: 0.26835876937498687, 3174: 0.17327081485840684, 8317: 0.26835876937498687, 1416: 0.24671007112211252, 9223: 0.1004755498556636, 8817: 0.26835876937498687, 825: 0.218092048683899, 9841: 0.17327081485840684, 1248: 0.1487808433072523, 3874: 0.1461766297399368, 9987: 0.11357605140047258, 8922: 0.17987051970492188, 395: 0.13750550496354763, 8859: 0.26835876937498687, 934: 0.26835876937498687}, {2087: 0.7317377311641771, 10638: 0.5343908336292853, 4624: 0.42306776020361075}, {1021: 0.7121461506283829, 865: 0.3852644714459225, 11366: 0.5868723431775182}, {7356: 0.18496765153093445, 7067: 0.17825715014977667, 4826: 0.21264242783303264, 3267: 0.424419457246612, 1108: 0.40396862366617425, 836: 0.5222413376958169, 10715: 0.5222413376958169}, {3484: 0.24096453676397897, 1416: 0.3641476548874266, 1878: 0.2135679902369672, 2199: 0.21948724121612612, 4947: 0.1319801673843669, 11331: 0.16957019342120433, 7588: 0.3002400676187435, 10771: 0.2655249187235309, 11559: 0.32190703734395754, 4412: 0.06680700257954696, 10600: 0.1056909966459261, 9950: 0.18638173003194233, 1081: 0.22659763431425273, 7613: 0.2135679902369672, 11655: 0.09251190847846583, 9223: 0.11398920402631872, 4448: 0.1126596448108559, 6876: 0.19525900311141128, 3604: 0.1637722889348066, 6527: 0.11100193841428903, 5058: 0.14163267239756971, 580: 0.2655249187235309, 3824: 0.18767034990497453, 8892: 0.14309403997309664}, {8803: 0.2070481537492458, 2656: 0.4112827433784454, 4973: 0.48948882111997727, 2734: 0.3613171414558188, 3414: 0.64639630592764}, {3658: 0.2694208941170821, 7991: 0.3846219442441434, 6758: 0.21909273606871005, 4173: 0.3086235999904127, 6173: 0.44675676298017364, 10144: 0.3846219442441434, 1351: 0.394933604690612, 6018: 0.14823953265056924, 11458: 0.3327987859545818}, {747: 0.4501970208033307, 8129: 0.8099286499384355, 6371: 0.3759497632247006}, {9223: 0.18070790012829857, 4785: 0.18255845282187594, 3215: 0.4209385520683859, 8892: 0.22684800464474478, 3157: 0.258579398366585, 10776: 0.25081916061236303, 6076: 0.14316632597637677, 4826: 0.19652201860178894, 10782: 0.2663781763708043, 7688: 0.22744242725166108, 10785: 0.17742498237253485, 1170: 0.27685915957233126, 865: 0.19791767827510431, 6592: 0.21279680417368743, 6421: 0.3820028029492426, 9693: 0.23511420449599887}, {1271: 0.5485844884732622, 7202: 0.7278721458806416, 6876: 0.41140879701051497}, {7346: 0.1251386220188891, 7133: 0.18668960894176695, 8542: 0.3291097584723052, 6886: 0.43418872466771047, 4127: 0.18270291648245646, 7517: 0.29101993476080396, 3232: 0.2545288057529684, 7688: 0.20460558454430566, 8719: 0.2853994474724871, 9771: 0.3991624026025754, 11259: 0.10642809441270715, 4821: 0.21807894444946305, 1861: 0.28280747927383004, 6180: 0.17868498897401552, 10564: 0.12788935826936032}, {6186: 0.04393841440521212, 858: 0.15811252838560352, 10279: 0.10462280569837071, 2477: 0.12514119381293667, 1853: 0.11984736837248712, 278: 0.17251972814264527, 11570: 0.15811252838560352, 10157: 0.09295780165883652, 6076: 0.036048509667202494, 6451: 0.1072902937764477, 1566: 0.09963050751049475, 6371: 0.050850298457550236, 4745: 0.08216643686344985, 6615: 0.15811252838560352, 9378: 0.13789624715064214, 3619: 0.12514119381293667, 4412: 0.03469518033708519, 3895: 0.14535747504789806, 4448: 0.044970553262416404, 3966: 0.15811252838560352, 2470: 0.09322166849186085, 8803: 0.05526004847134001, 7905: 0.08576044059460494, 10754: 0.09614872167919188, 5533: 0.15811252838560352, 6723: 0.15811252838560352, 4351: 0.15811252838560352, 11545: 0.0765781787065165, 6916: 0.09377101233677866, 10430: 0.05902892298594631, 10288: 0.05390469922715343, 4352: 0.15811252838560352, 6720: 0.0686839507799056, 4680: 0.10266508295765138, 5736: 0.15811252838560352, 6592: 0.06971060397536213, 10483: 0.03663898767115372, 3750: 0.12849621168899839, 4325: 0.047603452863281735, 5777: 0.09492149020115531, 11379: 0.13789624715064214, 7741: 0.11574115835129292, 1442: 0.12514119381293667, 6972: 0.13789624715064214, 2172: 0.13789624715064214, 5682: 0.11574115835129292, 7697: 0.10208828538192743, 5484: 0.15811252838560352, 8259: 0.10298610501358745, 3195: 0.11984736837248712, 9881: 0.15811252838560352, 3241: 0.10123224023403458, 8185: 0.08216643686344985, 6790: 0.15811252838560352, 3883: 0.15811252838560352, 6691: 0.15811252838560352, 3092: 0.12514119381293667, 591: 0.15811252838560352, 3658: 0.08765920102197561, 3598: 0.12514119381293667, 11464: 0.08420389079333163, 7067: 0.053968705038454315, 10154: 0.16281244684383042, 2424: 0.14535747504789806, 3329: 0.08687603379982028, 10480: 0.049195102290782985, 1274: 0.09377101233677866, 5968: 0.057568995848899976, 6671: 0.12849621168899839, 5029: 0.07593244044423048, 9713: 0.08046661515415539, 239: 0.11984736837248712, 9741: 0.1443014292682333, 526: 0.1393303141653477}, {10430: 0.2860456021593602, 4418: 0.31362831555287246, 4730: 0.5446069418250727, 2527: 0.6425721095218351, 4947: 0.3321438786871704}, {5615: 0.23255371283404216, 3601: 0.31677935988204164, 5097: 0.1923096130328694, 5472: 0.2618550006868264, 2825: 0.33391951449875673, 9856: 0.27033791741254243, 6197: 0.2434528770525056, 9919: 0.22389647494304313, 6764: 0.3632208023515409, 4719: 0.3632208023515409, 11518: 0.25166039982261634, 9223: 0.13599261138469057, 8560: 0.31677935988204164}, {6128: 0.3802311693864506, 5142: 0.7241455861983294, 8425: 0.5753585211122629}, {7770: 0.5344995770119922, 5611: 0.3481446418567749, 7067: 0.1824412670486731, 1704: 0.3272137788452118, 7212: 0.37494227097172456, 1468: 0.49138110510227523, 3724: 0.2659224332751256}, {11653: 0.33396620167977853, 6128: 0.19320654824953132, 10480: 0.17080925595533056, 940: 0.24878208039965047, 3604: 0.22698115669001082, 11259: 0.10342993507592177, 1696: 0.19832201723375437, 10122: 0.2658869931374014, 11298: 0.2043653468129757, 6186: 0.11725911284397394, 5968: 0.15363525224887642, 6076: 0.09620320442018863, 4709: 0.3680058059509671, 1853: 0.31983848944662296, 7970: 0.20495348280391634, 11130: 0.2959957986252773, 4412: 0.12046438898702844, 8748: 0.3429197887640681}, {6334: 0.655179653486746, 10273: 0.655179653486746, 4325: 0.126761353455885, 10365: 0.1487150782792728, 11618: 0.2751295266108926, 11640: 0.16612757646839646}, {5368: 0.5216512901611416, 2700: 0.6443400438068807, 9990: 0.4856106516852446, 7151: 0.27728709740987834}, {10642: 0.5621306490418435, 8683: 0.42608817692727036, 11547: 0.20384616490661772, 6925: 0.3176025848347414, 8132: 0.5621306490418435, 10430: 0.20986298258040048}, {10785: 0.23456215210403894, 9645: 0.5050213243443441, 3157: 0.3418511832977518, 10365: 0.22538033189671217, 2341: 0.43218304954652065, 8185: 0.33159187232514387, 4130: 0.47491070226142124}, {8019: 0.4031802709847969, 8554: 0.4031802709847969, 5002: 0.29513506655439375, 6845: 0.28282403376091614, 11642: 0.15873328048108423, 4749: 0.3118714804877235, 10412: 0.4031802709847969, 4418: 0.16503567686960086, 3809: 0.2581379380033518, 7748: 0.1467985085285343, 11138: 0.327659913975592}, {1123: 0.669180953502212, 2889: 0.5296361031803661, 5149: 0.32226986559541226, 6158: 0.409663988418438}, {1704: 1.0}, {4382: 0.5003077561321647, 464: 0.46447114588928723, 6226: 0.31217990388695593, 1978: 0.6606832912972916}, {6180: 0.213185540366229, 11019: 0.2553599903583513, 3204: 0.5180220141641009, 8118: 0.35475612362958225, 1035: 0.5180220141641009, 8433: 0.4762327993040699}, {7067: 0.286518273186352, 7896: 0.34118928403892557, 2171: 0.21611615861555772, 6117: 0.35947869700739804, 10735: 0.6144664172277561, 6128: 0.29542186552446703, 1702: 0.4009026907364976}, {3874: 0.2823605315492849, 11483: 0.3549960514494293, 2611: 0.452093227645285, 6180: 0.2133297215050904, 1763: 0.2788451760359448, 5237: 0.4765548839409963, 10551: 0.4347374062983194, 11590: 0.22471465256370177}, {4695: 0.5550707961126125, 1598: 0.8318031084955806}, {11653: 0.40023723706440684, 1148: 0.35944293783370357, 8068: 0.3833060731268629, 8142: 0.2415159634978606, 6570: 0.16620174593974102, 2603: 0.36456388220012664, 7346: 0.1457458874061419, 11655: 0.15366041469099467, 2123: 0.31624611182894896, 3329: 0.2778543393722969, 10580: 0.3503705948510638}, {5738: 0.6681223496232226, 10781: 0.744051426941676}, {9494: 0.47954731141205037, 7352: 0.5498513516836095, 4815: 0.5054944408799968, 7001: 0.42532575981181747, 6371: 0.17683674801664095}, {11227: 0.6175773825788665, 11359: 0.6372065918253409, 6018: 0.4610487348007472}, {2166: 0.4757466105537697, 9205: 0.29479404172949475, 2462: 0.3802197995916423, 3775: 0.3051489139108912, 391: 0.4757466105537697, 3790: 0.3220977746068025, 8372: 0.34495922365524384}, {7346: 0.2352662437299507, 7489: 0.43558567158673656, 7896: 0.33179173446089694, 6186: 0.22684274208979946, 6210: 0.35313597371172717, 10291: 0.6845922265442485}, {8803: 0.22132904355769736, 4577: 0.5856352538326106, 4046: 0.4001911999896262, 4624: 0.30499153432308884, 3232: 0.4829911102902308, 6834: 0.3486998118281251}, {10910: 0.32249617431472494, 9223: 0.2785348961151779, 3213: 0.45542747890605434, 5126: 0.49368206030300454, 9729: 0.2834837623626035, 8803: 0.19984472157135433, 274: 0.4585762472962968, 2171: 0.19153389554641748}, {7133: 0.13158179858367836, 6142: 0.18192160934335264, 3596: 0.15911653488823702, 3604: 0.10275352669860989, 1170: 0.14255718307737839, 6117: 0.081803519017484, 7922: 0.09049300747468536, 11585: 0.101930048963249, 9084: 0.06189561437666864, 9735: 0.09503488401927213, 4120: 0.13982891243833995, 7114: 0.08115908673599892, 6572: 0.13982891243833995, 6479: 0.16019929883671521, 6971: 0.16832738307578177, 3915: 0.13423636395453606, 7366: 0.07832988467791732, 5714: 0.19101850371958132, 9767: 0.19101850371958132, 10365: 0.051859513369101176, 10217: 0.16659486169888874, 865: 0.07832988467791732, 8750: 0.24852080306602706, 7238: 0.12036605437458958, 10564: 0.056264090867321905, 8488: 0.12522848966896702, 7763: 0.15523851487977303, 1453: 0.15118525925745568, 11176: 0.17560890127814827, 6421: 0.15118525925745568, 5812: 0.034930279411699745, 6130: 0.10497716076942995, 10447: 0.16019929883671521, 10716: 0.19101850371958132, 6842: 0.16659486169888874, 3748: 0.13771003780305732, 11555: 0.08954684949172347, 6058: 0.07214504715673942, 4537: 0.080843743051233, 3666: 0.1280323263417832, 6826: 0.10590260991611843, 10776: 0.09926670571827202, 1097: 0.17560890127814827, 11227: 0.0780519890966435, 8087: 0.14217121967819618, 5549: 0.16019929883671521, 3310: 0.19101850371958132, 7676: 0.16659486169888874, 11372: 0.19101850371958132, 4763: 0.19101850371958132, 4732: 0.16659486169888874}, {11655: 0.2617193295306095, 6186: 0.23935107196820557, 6590: 0.8613056177149694, 6130: 0.36382233272622816}, {10160: 0.20080588080748102, 5720: 0.31714337072799875, 11275: 0.386278655837971, 1483: 0.5185493732962597, 2130: 0.3386236534442878, 7114: 0.2396518243547933, 4676: 0.5185493732962597}, {11259: 0.3838902050445513, 9521: 0.9233787470322523}, {6527: 0.08553645300105521, 11517: 0.23460636756758604, 7924: 0.21568050026128371, 7781: 0.23460636756758604, 11547: 0.0850756107549739, 1179: 0.21568050026128371, 3246: 0.21568050026128371, 5632: 0.21568050026128371, 1202: 0.1352847878150596, 10594: 0.23460636756758604, 810: 0.1589028983423767, 2480: 0.16254887352472674, 8372: 0.14266486387726304, 9798: 0.16675784298086765, 9316: 0.19066186454897244, 4711: 0.19066186454897244, 4325: 0.05429060198704032, 10638: 0.10969729288319174, 2995: 0.1352847878150596, 5130: 0.18568371028716998, 7401: 0.2046095775934723, 7831: 0.1717359972426701, 7759: 0.21568050026128371, 1863: 0.18568371028716998, 11327: 0.19675463295498136, 10655: 0.1746127876193586, 3181: 0.21568050026128371, 9816: 0.23460636756758604, 3675: 0.14461599764524488, 1087: 0.17782876564867903}, {8646: 0.09670560268126312, 8778: 0.14864770411146982, 6371: 0.10641166620343272, 8074: 0.1820412939624448, 10988: 0.16978842978908837, 9993: 0.11952882969342882, 2171: 0.05767086982060174, 9615: 0.13533062187038297, 9741: 0.1571313757756483, 2005: 0.12916803203171529, 6761: 0.15340077649700926, 9301: 0.18785858441571465, 11680: 0.15676693977325376, 11655: 0.10904434983060572, 3900: 0.07984821195144227, 9507: 0.20592873904234094, 2683: 0.14341627477332494, 1614: 0.22399889366896722, 11227: 0.11908079330246196, 5490: 0.291429279658873, 10279: 0.09251820058309257, 1518: 0.17721905395600315, 4537: 0.09480185767824179, 5030: 0.13918599639282347, 11378: 0.18785858441571465, 322: 0.18785858441571465, 1923: 0.14831416542636516, 11642: 0.08818903546386446, 395: 0.11477575730788939, 1163: 0.09319980077925856, 7366: 0.09185396790055918, 11537: 0.22399889366896722, 2413: 0.1571313757756483, 3338: 0.20592873904234094, 4482: 0.22399889366896722, 10019: 0.12718710052532686, 1063: 0.12000718664603427, 1711: 0.291429279658873, 8988: 0.14864770411146982}, {7074: 0.3770469564983682, 10488: 0.4076692071503975, 6355: 0.2390148481137841, 2972: 0.39042364991744766, 4412: 0.11302558517303682, 10020: 0.3713334368942791, 10288: 0.1756039344523382, 8043: 0.36611740216189603, 10480: 0.16026160319794674, 8803: 0.13836669483268918, 4049: 0.34887184492894613}, {4152: 0.20042350579652346, 1527: 0.313149068487301, 1204: 0.2652072899318871, 10335: 0.3790312309848773, 9144: 0.3576312744325564, 8865: 0.4623374792490275, 3905: 0.4623374792490275, 2245: 0.3039020905586123}, {8803: 0.19562897718894817, 6128: 0.2562958304554271, 3222: 0.3706166833827236, 4046: 0.35372219513332664, 11007: 0.5330857294898358, 9576: 0.3757549317473295, 6511: 0.36345001004550925, 4704: 0.27585786144931285}, {10873: 0.3967114384172858, 10861: 0.2842183689086359, 6758: 0.2880994887001971, 7133: 0.27476167932973494, 3988: 0.45421453831601954, 8072: 0.3526798596569655, 5749: 0.2848513195532686, 3: 0.43761907306453335}, {6923: 0.1914105243887718, 3939: 0.4853937666070502, 8227: 0.44623669634780055, 6371: 0.15610665488320893, 7720: 0.4853937666070502, 10104: 0.3612687906090215, 11298: 0.23508934416387656, 6916: 0.2878700716599854}, {2535: 0.36154737143252264, 806: 0.30292287774951, 10785: 0.1753417033412121, 8045: 0.4159959986169325, 10453: 0.24356862717276598, 2674: 0.24525096349307624, 1696: 0.2241844130520934, 1423: 0.38763861239575964, 1420: 0.24787410580119462, 11489: 0.4385045223379353}, {8803: 0.40358544887817654, 11315: 0.49989218980224964, 1817: 0.7663071081678104}, {11259: 0.15227510661495142, 4307: 0.5711132736933615, 5849: 0.5048649344708072, 9912: 0.2655107983511526, 10480: 0.1932888274903695, 5812: 0.11359985786963078, 4785: 0.23497441350259332, 6919: 0.3314957472396049, 10674: 0.3314957472396049}, {10964: 0.2836906487421485, 705: 0.2968946902642699, 7340: 0.23390319398728282, 4448: 0.08776793505187358, 9223: 0.11553632079806285, 8892: 0.14503618169669255, 5812: 0.056428766166892935, 10406: 0.25078293113545336, 1008: 0.2691287517231747, 5029: 0.148195497225719, 3279: 0.25879692136471566, 4325: 0.09290650116686906, 7717: 0.2836906487421485, 7067: 0.10532940902480376, 2970: 0.17521493710619998, 10264: 0.2836906487421485, 4624: 0.11423021115102093, 1638: 0.1343282844775515, 747: 0.11884309606936838, 7987: 0.3367025574809877, 10657: 0.2047793999493352, 10962: 0.30858437611958134}, {10480: 0.09310560995999521, 10157: 0.17592996906132619, 4163: 0.2751004824948791, 10024: 0.2751004824948791, 7615: 0.1796464044581543, 2582: 0.18319114736932043, 4325: 0.06924766394495074, 6210: 0.1294539882870639, 4606: 0.24318921213208536, 5812: 0.05472010046130431, 6570: 0.09834954731425181, 11547: 0.10851394325813842, 3173: 0.2049283059065374, 9377: 0.22682059422226794, 2528: 0.15607275861531492, 11590: 0.1297208599200874, 3109: 0.16745005234129998, 5720: 0.16825069858387212, 6949: 0.2751004824948791, 7978: 0.23147103564193164, 3708: 0.15723375204576354, 10288: 0.10201889349856097, 3796: 0.1425274555446839, 5867: 0.17255574962527548, 6753: 0.22682059422226794, 10483: 0.06934217303332327, 7896: 0.12162953225974099, 9974: 0.2227186141800889, 11655: 0.11830036755008574, 2734: 0.14027979972410887, 5058: 0.13920812816281533, 1384: 0.1346016874788755, 1455: 0.13429484339972755, 9401: 0.1507858747139355}, {7817: 0.4421996747738951, 9250: 0.4847349495065398, 4328: 0.4847349495065398, 6758: 0.2377175124268435, 10415: 0.5272702242391843}, {4930: 1.0}, {1163: 0.17594600701673313, 924: 0.266464213061081, 3768: 0.3005777146581509, 6337: 0.3436642408787164, 3892: 0.33469121625522086, 6253: 0.42287333865193805, 11227: 0.17279009402238787, 10365: 0.11480566087697074, 1461: 0.33469121625522086, 6186: 0.11751367323208424, 7459: 0.38875983705486805, 10170: 0.2806225954555737}, {9246: 0.4574346748338558, 10453: 0.3984639558009832, 9629: 0.5343826733256261, 7354: 0.5885704313303838}, {2403: 0.29243651702076456, 4439: 0.3247309292209384, 5953: 0.3247309292209384, 11315: 0.14517302464896226, 8803: 0.11720471236234216, 11343: 0.4363022020325677, 8072: 0.24079835009759037, 8601: 0.4363022020325677, 7217: 0.3101229299660579, 10672: 0.3659085663718726}, {11655: 0.1327021546045615, 5286: 0.401486125805431, 4418: 0.1787631592949939, 5588: 0.3808777636882879, 10021: 0.43671635354094684, 4745: 0.22694866154404944, 2901: 0.33781260030243626, 9773: 0.21031758274407347, 6226: 0.17305981937728107, 6570: 0.14353293155947813, 3728: 0.259001199928252, 8074: 0.3549142979725728}, {11655: 0.28449926898692385, 6758: 0.42211478675344116, 2692: 0.8607434418861424}, {11090: 1.0}, {8105: 0.21169603309310167, 7967: 0.23115351938105666, 2695: 0.24806336637736096, 7827: 0.26148546703453146, 3092: 0.22511813375027218, 8728: 0.28443069966162027, 5061: 0.16868590313022191, 9817: 0.19952324950635406, 4021: 0.2155950017803539, 3449: 0.24806336637736096, 3227: 0.15628243586900586, 11655: 0.08642810459178506, 1691: 0.15427558063029068, 7067: 0.0970850108505181, 6883: 0.28443069966162027, 3809: 0.23692797537788746, 10045: 0.22511813375027218, 3209: 0.15916277116030364, 9827: 0.20820828675396788, 213: 0.12407370376625983, 10854: 0.12735933799627974, 215: 0.16210724305922855, 11367: 0.24806336637736096, 595: 0.15177605613391762}, {2074: 0.2864822536202819, 7765: 0.4960713515090471, 4367: 0.2513163174665521, 11590: 0.2564180118530381, 8230: 0.4681585776760165, 8803: 0.15889734597174376, 11315: 0.19681468311697556, 10483: 0.1370680255876161, 4689: 0.36721392388030366, 5812: 0.10816471134503647, 917: 0.24610933419273234, 10480: 0.1840409893444554}, {11640: 0.1250687614433275, 10424: 0.3596626076626147, 6692: 0.26403511520024414, 7286: 0.41239094927051856, 4857: 0.37912306969361215, 2046: 0.44996797649790304, 7517: 0.27640972769272254, 5968: 0.15015212955661053, 4932: 0.41239094927051856}, {6742: 0.45479708781758466, 6180: 0.436174711540237, 10157: 0.47894231120318465, 7640: 0.6111673195327011}, {2787: 0.7071067811865476, 8298: 0.7071067811865476}, {9294: 0.3975768450250306, 7848: 0.416926319046798, 1163: 0.3100012176628555, 1271: 0.4897437201466065, 7206: 0.5763292725608683}, {10160: 0.17977103438955952, 4365: 0.3385019468096619, 10365: 0.10537267510549836, 9871: 0.3154271324503543, 6469: 0.23924978155225232, 5728: 0.3255068852710757, 11231: 0.3568174573547212, 8892: 0.1824220918345779, 9929: 0.16825367583066725, 7819: 0.20711034938656875, 2171: 0.09992764112730865, 11655: 0.153440793598522, 10910: 0.2189030791364219, 11259: 0.09513772287131071, 4656: 0.20243325052143613, 6834: 0.16426535687327856, 8787: 0.26891756870325956, 6675: 0.2888758641809571, 2960: 0.1824220918345779}, {2171: 0.386515091245685, 1638: 0.653505672660955, 4152: 0.6507967578586209}, {4160: 0.26818524640315833, 6308: 0.35452649501049066, 4152: 0.2400411432889765, 3567: 0.5537275513828731, 10533: 0.32102331857908445, 2081: 0.46924082755279006, 11486: 0.3389848978956833}, {6018: 0.21706026017012622, 3042: 0.4991526377545748, 7938: 0.5296056152956566, 5968: 0.25908287133899616, 7834: 0.5967624700863633}, {1702: 0.18250090671486316, 4499: 0.3105465487073912, 4448: 0.14140081840599883, 356: 0.2680524038179959, 4865: 0.20154380854571713, 6415: 0.3821225493899332, 5968: 0.13913136222798608, 6434: 0.23236991416033315, 10480: 0.17562017205629807, 1923: 0.19446977692049963, 7067: 0.13043026613471845, 11562: 0.1938205293358243, 2528: 0.19930101384967294, 4704: 0.1447479813595318, 11555: 0.17913379987441672, 10674: 0.20390574426473573, 6720: 0.215962805593336, 3605: 0.3105465487073912, 6587: 0.28964423254608507, 11347: 0.18402583335400172}, {43: 0.36155657529236745, 3499: 0.36155657529236745, 2172: 0.3153279210352294, 8629: 0.3153279210352294, 11316: 0.36155657529236745, 39: 0.36155657529236745, 1871: 0.2740554749771788, 3953: 0.2690992667780914, 4578: 0.36155657529236745}, {6422: 0.5970114705362056, 7346: 0.1720661742415056, 10218: 0.3606889848787218, 4127: 0.25121734085545816, 7097: 0.5970114705362056, 4367: 0.25365556098567255}, {6574: 0.4042037912198995, 3472: 0.38345593080038726, 7848: 0.24603354157179266, 4704: 0.16654789387612062, 10638: 0.20558215330653692, 279: 0.32723936872233494, 2712: 0.34798722914184715, 962: 0.27922762342513135, 9380: 0.24603354157179266, 3915: 0.23748529719320083, 10651: 0.3687350895613594}, {9223: 0.1333683777284884, 6128: 0.12536441941630164, 8954: 0.29874004404092125, 8892: 0.16742129341846312, 6076: 0.08121370052445935, 5812: 0.0651380704259444, 10066: 0.3562117726369238, 9950: 0.21806827396434803, 2564: 0.3562117726369238, 7493: 0.3562117726369238, 5544: 0.31066650529877987, 5962: 0.2112567477574037, 1592: 0.26075329769452266, 4330: 0.27554000256035055, 4905: 0.18315382840908748, 4448: 0.10131417578119703, 9420: 0.213848659755988, 5097: 0.1885986367247191}, {10365: 0.1315306100919679, 595: 0.25852385909252257, 1192: 0.32814505162027213, 5946: 0.32472672613357667, 3366: 0.36722822611847317, 985: 0.40631140061667415, 4325: 0.11211370325762011, 8493: 0.22425370838676217, 4831: 0.4453945751148751, 5030: 0.2313855299267826, 4785: 0.18324968773365605, 9773: 0.23331892283341524}, {1163: 0.13494108862028445, 10586: 0.19234345809017503, 6088: 0.24583137902980598, 690: 0.3243210211684987, 2005: 0.18701837033516985, 11259: 0.07949738512293869, 7114: 0.179276812028033, 9912: 0.13861368847498823, 8674: 0.2635719194220035, 427: 0.3243210211684987, 4397: 0.3243210211684987, 10061: 0.17041217191047237, 7801: 0.38791225050746625, 11715: 0.3339615118326124, 2551: 0.22210420626038646, 3038: 0.23381117125179207, 11315: 0.10791296349395137}, {1384: 0.26410227819793086, 10494: 0.46452217268070484, 1806: 0.4297970686612215, 11568: 0.34617244569042604, 4258: 0.3935374851738252, 11298: 0.2843679569162382, 3886: 0.4173384336434115}, {10776: 0.37158156761298433, 7125: 0.6235302090196012, 7688: 0.336949591305301, 3663: 0.599668400009072}, {6923: 0.15797309110536478, 9148: 0.2981588800022727, 512: 0.2381774153200156, 4771: 0.14434410779404275, 10804: 0.25823158481923086, 5108: 0.30791027990442515, 11088: 0.283070932361828, 10811: 0.14999283614158138, 8803: 0.08271453965046133, 5968: 0.11211056964708861, 1260: 0.30791027990442515, 6758: 0.138820025149627, 10711: 0.283070932361828, 11547: 0.11165790337322118, 2341: 0.20855288973403655, 8700: 0.2381774153200156, 9380: 0.17230156054404963, 7067: 0.10509931909976898, 11315: 0.10245253506856547, 10805: 0.30791027990442515, 9326: 0.26854084550702845}, {8215: 0.30000198881871243, 1817: 0.28008509160397305, 2267: 0.3042760844618647, 694: 0.42206336806530137, 213: 0.1841115089272139, 10854: 0.18898702289600358, 215: 0.2405490303077529, 11447: 0.3680983101732788, 5400: 0.3539670467107349, 2479: 0.1925288169117148, 11205: 0.3680983101732788}, {654: 0.5596897841831898, 7845: 0.31469075966510723, 4785: 0.2116980155666548, 9617: 0.329987864036694, 758: 0.3102977341682151, 7408: 0.4693886154811724, 5096: 0.3426349002695911}, {10910: 0.3527492004537537, 2479: 0.371188545752168, 9223: 0.3046639610900097, 6919: 0.5649247314316528, 3906: 0.5708125644302586}, {10775: 0.39875916869079303, 8950: 0.24595806720409932, 3724: 0.2896920703273103, 11214: 0.5822761440118014, 11618: 0.2924584625300394, 6049: 0.4197774992594281, 7259: 0.30948933725823635}, {7587: 0.60816988726726, 7570: 0.4813477243757347, 5274: 0.60816988726726, 6186: 0.16900634509082393}, {3117: 0.41837122815696015, 6186: 0.20475886498951798, 5022: 0.45107507913483824, 11664: 0.5484043195250489, 9704: 0.3621271570755082, 1881: 0.38430094987759145}, {747: 0.4704700201895295, 9080: 0.6712211496308499, 6058: 0.461384513298516, 6186: 0.3394767433557054}, {1881: 0.4917367702611825, 11655: 0.2864866322978896, 9320: 0.8222653819100709}, {4649: 0.5225568279805277, 2150: 0.539265800674023, 8211: 0.6603989383352566}, {4046: 0.2609412761314276, 9223: 0.2616904630528396, 8493: 0.2486690777021806, 2265: 0.5372247176208718, 11655: 0.21238400929654674, 10480: 0.167151997418202, 5386: 0.32888207493910376, 8321: 0.493886406624563, 6128: 0.18906973321534182, 4367: 0.22825363305180862}, {6179: 0.24972334724032064, 1818: 0.15735760362074117, 4412: 0.07129339930832866, 2171: 0.08364829348484759, 4997: 0.29868784108792545, 6058: 0.09431705469350764, 10634: 0.12310162464126942, 11355: 0.17101760972404575, 1852: 0.2177936989683377, 3469: 0.12751976567314377, 7366: 0.10240265005652768, 2246: 0.19316815732564804, 7638: 0.2177936989683377, 11315: 0.08309170450080453, 6851: 0.16534684268647226, 5968: 0.09092462493288685, 11642: 0.09831682989683721, 5001: 0.24972334724032064, 2074: 0.12094777882193407, 777: 0.20943261700858962, 10564: 0.09569788154028161, 2525: 0.24972334724032064, 9684: 0.22957798212445513, 8578: 0.24972334724032064, 8748: 0.20294725799602872, 8177: 0.20943261700858962, 3964: 0.17517663802911787, 1248: 0.13844917489544784, 6592: 0.11010111305295464, 5812: 0.045665242502028525, 4325: 0.05778884432022073, 4624: 0.09244133174632047, 6860: 0.257146410935075, 1384: 0.11232835188692188}, {7074: 0.6229055032649091, 5748: 0.7822970880696735}, {3724: 0.40304800044535116, 1484: 0.6140608074543835, 4826: 0.3298586199965992, 1868: 0.593021858702876}, {516: 0.36685277812844425, 981: 0.3867022826575096, 8648: 0.31307032147920993, 9258: 0.42063523477767867, 2147: 0.42063523477767867, 8042: 0.29506839099842513, 980: 0.42063523477767867}, {11640: 0.29968146907918997, 10638: 0.35513131588543434, 10417: 0.48234961908717555, 7819: 0.4052841782000509, 6094: 0.453158194874599, 10634: 0.37440142485670086, 8803: 0.2040283532219276}, {7356: 0.260750657619354, 10221: 0.8353612928039078, 1598: 0.483922106372563}, {2659: 1.0}, {1028: 0.35989187720301985, 10562: 0.39819426257759327, 11561: 0.29476097981254873, 4624: 0.1757587163460171, 5968: 0.17287500147988027, 3316: 0.283287106453873, 6896: 0.23476616018485416, 5700: 0.4747990333267401, 1140: 0.37578880344529736, 6825: 0.24947977222902712}, {8667: 0.45437276996211906, 1027: 0.6557950404142262, 5628: 0.6028915747338445}, {11515: 0.39027442259029127, 864: 0.3461467958908106, 11547: 0.16227409906432985, 6424: 0.4113912148858426, 10653: 0.4113912148858426, 415: 0.36367041060355754, 2425: 0.31390697518784255, 7297: 0.36367041060355754}, {7346: 0.36838214189058055, 8803: 0.3433550268308837, 1856: 0.8117360346647363, 4325: 0.29578122508509735}, {11655: 0.26155954539251425, 8723: 0.640661026164553, 8219: 0.7219003073609562}, {1829: 0.6073540141725545, 4660: 0.5963702053468366, 11259: 0.19640721278622542, 11464: 0.32798822603184596, 1455: 0.35959923525097437}, {6651: 0.6974479429818485, 8366: 0.3840490621587859, 6371: 0.2571892766631128, 3254: 0.5476553301620004}, {6018: 0.08192048794775215, 11590: 0.11641745360266773, 5149: 0.12933160541704894, 578: 0.21255070617474964, 5720: 0.15099589925684154, 9272: 0.20773273141117388, 1070: 0.20355921183584005, 11642: 0.10572976364695069, 5183: 0.16554081333319104, 10184: 0.2702665146471465, 4897: 0.23421500001454224, 5771: 0.11153086197445874, 6058: 0.10142841170819607, 5952: 0.16330736560864612, 6177: 0.19987790667386665, 10365: 0.09485688305288711, 8950: 0.11343853686014224, 360: 0.19088641233495707, 9084: 0.1132140406452876, 6720: 0.11665880591616593, 4376: 0.12399871012266882, 7772: 0.21824916083270823, 6117: 0.11500721578375922, 5386: 0.21389472240624938, 10107: 0.24688779951542525, 5248: 0.21824916083270823, 11454: 0.1400668569876263, 11167: 0.17821361283407405, 7209: 0.24688779951542525, 10869: 0.26855209335521785, 6904: 0.23421500001454224}, {4695: 0.2551707182894183, 9205: 0.23236422854786254, 9294: 0.23859860183545437, 8867: 0.3389247770551947, 2038: 0.25263171119540856, 407: 0.2578399865844145, 1192: 0.44735188446475915, 3117: 0.3750200102043554, 9100: 0.4878806367832758}, {7091: 0.8782292252933389, 7346: 0.2753275217138796, 4418: 0.3910347601269675}, {6186: 0.25276390250823116, 3878: 0.481578661668679, 10365: 0.24693915248474108, 8486: 0.7035797711670635, 4537: 0.38495319558205215}, {4704: 0.6104663132402407, 9897: 0.7920422213486275}, {7356: 0.14349869839723625, 5469: 0.4051570725384267, 9092: 0.4051570725384267, 5771: 0.16826350884365976, 10743: 0.48459822693930066, 9462: 0.3533536550205251, 2970: 0.23004914207141502, 5999: 0.292087911322715, 9492: 0.3533536550205251}, {4448: 0.11719963613001397, 8366: 0.19789070280274298, 5115: 0.4120636605526505, 10483: 0.09548639524566416, 4412: 0.09042055780910149, 1868: 0.3924399093278347, 7354: 0.23889373654263274, 3558: 0.34919175070712993, 8923: 0.40636270663998464, 5215: 0.21491719201042853, 10480: 0.12820940970448666, 1742: 0.2658958223500251, 9693: 0.20072924209077586, 10274: 0.29706704411879814}, {6128: 0.2670387393204097, 5012: 0.7587666671208898, 6186: 0.21085618323370559, 2623: 0.5554305776247261}, {4826: 0.31796222363145554, 2114: 0.7179065824155896, 6018: 0.23821046502832063, 2102: 0.5716344440450557}, {11154: 0.5519825506062939, 11464: 0.2590703738295558, 6018: 0.19306483308714129, 5968: 0.2998118993370702, 1527: 0.42867801281534923, 7067: 0.3191031172462437, 4826: 0.2577020435526414, 10910: 0.2743650191947592, 4818: 0.2715850794030893}, {8462: 0.5377634941386636, 10430: 0.18399982553831673, 6210: 0.2132131591638635, 10402: 0.34572905180359315, 9729: 0.18780710148474852, 7346: 0.1420468680747505, 5701: 0.5592323785640035, 5577: 0.2543009654560136, 1969: 0.28126038773321127}, {5812: 0.1378887033962356, 7896: 0.3064931964049712, 2698: 0.47156740635057626, 10480: 0.23461601364160523, 7796: 0.7540533384270566, 8803: 0.20256282050488025}, {7845: 0.24353858659019778, 9515: 0.24845688830795576, 1407: 0.43314287052471223, 10535: 0.3982009182784246, 3915: 0.2339583781105848, 2267: 0.31226357610699024, 878: 0.32831701378584927, 6923: 0.22222355865571805, 10127: 0.4726108110016934}, {10160: 0.17817210713827217, 8162: 0.43648415125246987, 10365: 0.13587367249172025, 2041: 0.3871316736832985, 11511: 0.27261168587374934, 6834: 0.21181333091514895, 3874: 0.27261168587374934, 1912: 0.5004749163287932, 5380: 0.4067301399214981}, {8773: 0.7850888892120672, 4325: 0.13964210394598833, 2952: 0.603436424854599}, {11618: 0.5470259821548307, 3206: 0.8371156281228674}, {11640: 0.07830873749729106, 6753: 0.25463613082413716, 1923: 0.13140743795784207, 3092: 0.26588376392380486, 10480: 0.08033893733097981, 11555: 0.12104458628948293, 9522: 0.23737861158285398, 1850: 0.22519392057628382, 7557: 0.18614897842185724, 7675: 0.15405914415341326, 11359: 0.10885989528291279, 7758: 0.16671709132846918, 1793: 0.2165487447254938, 2660: 0.23737861158285398, 7562: 0.1768284675074161, 3902: 0.11484394009527153, 10861: 0.11484394009527153, 7852: 0.19217936271235353, 3780: 0.20984302537134644, 4325: 0.0882615414156451, 5771: 0.10723511334338663, 3212: 0.23737861158285398, 2492: 0.20436405371892363, 1919: 0.17134949585499334, 6562: 0.2165487447254938, 1844: 0.12104458628948293, 6355: 0.11981783858775834, 9627: 0.1957188778681336, 1095: 0.10458738901747637, 11655: 0.07846012897927622, 10365: 0.07010088435905223, 7260: 0.23737861158285398, 3768: 0.18353418686156348}, {10430: 0.3613612120792374, 11234: 0.4674604779809046, 6527: 0.35290222799746446, 1455: 0.43439221192634253, 4017: 0.5810870843708831}, {7133: 0.1358529546901568, 7748: 0.14967082889353878, 715: 0.22778107710958, 11547: 0.14906650642300667, 8842: 0.31595663772405536, 729: 0.20400326102953237, 11555: 0.14811612976364585, 4169: 0.24440150341741326, 661: 0.2076835364638297, 10285: 0.2500699422216687, 7067: 0.10784579033917914, 11315: 0.10512984014894725, 7688: 0.1937107555918853, 8667: 0.21891305030290753, 5930: 0.31595663772405536, 11144: 0.25677428234226524, 5294: 0.2076835364638297, 6252: 0.1990930359926573, 4599: 0.141157310669575, 9912: 0.13503877977225862, 1017: 0.31595663772405536, 4983: 0.21891305030290753, 4624: 0.11695923784488468}, {3313: 0.6576719106103842, 4325: 0.22706487484098892, 4912: 0.7182681954587248}, {8803: 0.05831082582452453, 6527: 0.0791412609234613, 9080: 0.11926835400662247, 10564: 0.06393628389513631, 6018: 0.06621486715304496, 4012: 0.1706196462756891, 8892: 0.10202202696529887, 3241: 0.13897736967293808, 8962: 0.19955499846445987, 3433: 0.2170658602098306, 182: 0.2170658602098306, 192: 0.19955499846445987, 7281: 0.19566943993663186, 2475: 0.12387431312145376, 11249: 0.2170658602098306, 3438: 0.23684488240628973, 206: 0.18204413671908914, 59: 0.18204413671908914, 11229: 0.17640689835756596, 4699: 0.16790665629949386, 9885: 0.2170658602098306, 1379: 0.2170658602098306, 60: 0.17180093924273462, 5187: 0.259627038786942, 10335: 0.17795386247673847, 11561: 0.13475711016301256, 213: 0.09468796887777367, 10854: 0.09719543034843282, 215: 0.12371360828053271, 11447: 0.18931180098810538, 388: 0.13574793650530131, 10638: 0.10149569889879502, 11655: 0.06595838948411692, 7970: 0.10543342610622777, 5968: 0.07903398758434786, 4818: 0.0931447206923903, 1124: 0.18204413671908914, 1218: 0.15039579455412314, 3580: 0.1645332749737184}, {8501: 0.4035860809324, 1455: 0.2755501582196446, 5762: 0.3758770505566328, 5030: 0.2932405009886316, 10483: 0.18510838188421652, 3773: 0.49898286623597027, 7133: 0.2639995567296025, 527: 0.425408106684927}, {1702: 1.0}, {5720: 0.39596172226784726, 9210: 0.42017825790780977, 6128: 0.24784640213626788, 5863: 0.5338002985425631, 7235: 0.42824650247907087, 10365: 0.19119194533790226, 1384: 0.3167720338927789}, {1598: 0.6706341302074901, 2171: 0.20189910047356618, 2656: 0.41845672296658215, 9713: 0.39909216809772946, 7819: 0.41845672296658215}, {1163: 0.20724019068747335, 1276: 0.33384822412992554, 8245: 0.457905604843989, 4152: 0.21592077296227818, 6355: 0.23112975294523072, 6186: 0.13841494025315648, 8405: 0.457905604843989, 10684: 0.35403919621907626, 6130: 0.210395742258277, 3336: 0.37071582285591603}, {6018: 0.15255612056288997, 9773: 0.24084729574827712, 6521: 0.3287306773835852, 1906: 0.6506582747719822, 927: 0.4194216263379978, 1306: 0.4597658760578136}, {4862: 0.36600350096055906, 2385: 0.5310067841504671, 11248: 0.5899364244927707, 4152: 0.4858479675037025}, {11640: 0.34813793443182856, 4012: 0.6935245330247456, 8509: 0.6307326697598186}, {5771: 0.22055602224426274, 3822: 0.16738523362298705, 10610: 0.21212568863032644, 7823: 0.331733350397258, 11613: 0.2863400748477426, 213: 0.17806082103697932, 10854: 0.18277610486316945, 215: 0.23264356522753243, 3175: 0.408192569194852, 4973: 0.2592353236607634, 1709: 0.408192569194852, 7694: 0.408192569194852}, {4862: 0.13848867249883998, 1267: 0.2842750229170993, 1469: 0.2996564475411142, 358: 0.24706700939746473, 4930: 0.16166366786581543, 9979: 0.2349863840526833, 2656: 0.22629066736146908, 2674: 0.16759469682215625, 8803: 0.08756089826974495, 4325: 0.07542883527531108, 10191: 0.3259511666129389, 7114: 0.13848867249883998, 8923: 0.24706700939746473, 7892: 0.2996564475411142, 1359: 0.24259887922125972, 3536: 0.24706700939746473, 7067: 0.1112572327284059, 4412: 0.07152459468068893, 10600: 0.11315439107594794, 2220: 0.2996564475411142, 1163: 0.13561934746422127, 1384: 0.1466164767766625, 4537: 0.13795057467097527}, {11655: 0.3470441690670654, 6521: 0.750724897820351, 10910: 0.5621142877646625}, {371: 0.3412641061015276, 4302: 0.31335403914652604, 8464: 0.3456661211973643, 4872: 0.2704185706944681, 747: 0.1943905531342959, 8892: 0.2372343410512441, 6844: 0.3904374981578907, 2246: 0.3904374981578907, 1522: 0.4402114101880496}, {3435: 0.6666499350275322, 3724: 0.3802938374612512, 5933: 0.6410572995581458}, {3900: 0.2224810206455597, 6925: 0.3526309356918949, 2433: 0.5234302506768067, 6876: 0.30766513432842746, 10365: 0.1694441753483172, 5506: 0.5234302506768067, 6180: 0.25685213185358813, 1702: 0.29808217477713406}, {10785: 0.16957431331173495, 464: 0.27197464833852264, 11618: 0.23169306297181066, 1465: 0.4612940317555434, 4325: 0.10674872526562018, 6018: 0.14071546304729543, 4367: 0.19599254315700618, 11663: 0.4612940317555434, 5854: 0.3433318440250942, 2488: 0.28435075015986955, 900: 0.3868681777200211}, {7356: 0.18247582626472625, 7896: 0.20941103962543503, 7058: 0.47364383444052643, 11195: 0.4320818000878142, 11655: 0.15655225250768512, 2171: 0.13264516669396942, 1274: 0.30555058711584304, 9207: 0.47364383444052643, 6180: 0.21202659063779736, 806: 0.3271974313789749}, {10430: 0.8021686936550614, 6186: 0.5970974685256439}, {5096: 0.432239184573402, 6793: 0.70605725143311, 7214: 0.5255041718169946, 6186: 0.1962085890601554}, {9143: 0.4557251751016853, 6434: 0.2771279004291894, 6186: 0.12664297891481266, 6076: 0.10390203450656142, 9490: 0.3285437267542824, 8346: 0.4557251751016853, 4448: 0.12961789599590276, 1384: 0.20499027583227047, 9124: 0.4557251751016853, 2659: 0.27190675871468806, 6226: 0.18059254215993711}, {802: 0.5532642778484325, 1384: 0.2707021985014755, 1910: 0.5532642778484325, 9205: 0.3127445240752534, 6902: 0.46551955931100897}, {4872: 0.16206483742640546, 5378: 0.2780984084958092, 677: 0.3025014192960876, 1555: 0.19883919830672414, 10080: 0.3025014192960876, 6715: 0.17443618750644577, 11189: 0.3025014192960876, 3027: 0.3025014192960876, 8442: 0.2458393829190815, 3004: 0.22143637211880313, 213: 0.13195647140504635, 10854: 0.13545085164969387, 215: 0.1724063934094713, 11304: 0.3025014192960876, 8049: 0.18917734654207544, 3392: 0.25369539769553084, 9991: 0.21808087214365715, 1808: 0.21501754067239626}, {10327: 0.3667826481883979, 8375: 0.324723767363426, 865: 0.19443947537338768, 8646: 0.20470957412932408, 6371: 0.15249640831363034, 6186: 0.13176816237144118, 462: 0.4741681645845165, 632: 0.43591667185291266, 6118: 0.4741681645845165}, {7269: 0.16516127451342494, 2910: 0.20226079645035025, 464: 0.14219264378747754, 2528: 0.12578631153808417, 1992: 0.1585262286205661, 8646: 0.10411955503848246, 11454: 0.12578631153808417, 11130: 0.16917788085420774, 9035: 0.28195298612174546, 3236: 0.15571704983433293, 9773: 0.11614557476753151, 11707: 0.2411718089881055, 11432: 0.1369379223764205, 2005: 0.13907072235168846, 263: 0.2411718089881055, 7293: 0.2411718089881055, 6128: 0.08487749742114795, 1195: 0.2411718089881055, 9111: 0.22171630271922788, 262: 0.2411718089881055, 8892: 0.11335194201460193, 9623: 0.1497223745853301, 10312: 0.20259255267077014, 10331: 0.18280529018147262, 1209: 0.13907072235168846, 3754: 0.14058830071291745, 6916: 0.1430305675762377, 1913: 0.20226079645035025, 6740: 0.20226079645035025, 3708: 0.12672201026367683, 2035: 0.13981925637568585, 3313: 0.16164815005635516}, {11395: 0.8972738289706014, 4818: 0.4414744339646815}, {11655: 0.1334456542218861, 6018: 0.13396455455573444, 5639: 0.4391631747150615, 7122: 0.26045232401592855, 9846: 0.31215650492101904, 2722: 0.16586585766493797, 5252: 0.4391631747150615, 6952: 0.4391631747150615, 2878: 0.4391631747150615}, {1054: 0.3800481588735534, 4624: 0.18303451315091637, 2538: 0.28286217948197206, 7346: 0.10953463367490604, 10480: 0.11824811254432231, 10418: 0.4944540544913632, 5720: 0.2136855937075651, 6618: 0.23593849149316445, 4022: 0.3220611149551269, 9084: 0.12314678331232778, 8142: 0.18151018227129284, 4964: 0.2602677259809103, 11547: 0.13781735580197488, 6186: 0.10561284212595347, 3724: 0.18908028278405792, 5812: 0.06949687135463849, 10873: 0.23593849149316445, 7366: 0.15584421339789084}, {11559: 0.43878085208987244, 11162: 0.43878085208987244, 7946: 0.4963576429281597, 10476: 0.539912803803376, 2674: 0.27760760485719704}, {3107: 0.47006865637985373, 4939: 0.6076939384271438, 9450: 0.31245871100777456, 5168: 0.5586708238341304}, {7608: 0.5417240876716108, 9272: 0.35034517445803437, 9218: 0.3950080205625677, 10976: 0.4163809362405528, 6205: 0.22193236855270992, 5740: 0.4529181769211709}, {8637: 0.527997425243765, 10724: 0.527997425243765, 8105: 0.39297783446179435, 9930: 0.48540348667407107, 4947: 0.22888712200183536}, {10160: 0.27832055050002674, 8953: 0.6187595064415187, 865: 0.3205825820650503, 10480: 0.24324474048319433, 9912: 0.2568216229354324, 4437: 0.35637143496578666, 8185: 0.3122689014584574, 7356: 0.21282620552738013, 10288: 0.20486176416637936}, {5749: 0.35343813861271606, 5023: 0.4922320619068015, 8803: 0.2129940156209448, 7544: 0.563580822462109, 6018: 0.24186538690389822, 8995: 0.4596742541032536}, {6469: 0.11590984902005172, 580: 0.16399475599576413, 6504: 0.18803720948362035, 6825: 0.09880281318995994, 8646: 0.10561773981721867, 4704: 0.07122847509702733, 5968: 0.06846461468126724, 2974: 0.1319048721144127, 4367: 0.1039424189570392, 2960: 0.08837841767412422, 11642: 0.07403081267081493, 395: 0.0963491951546846, 11471: 0.1488256567414178, 2192: 0.1454521640287508, 6958: 0.1078624186265565, 4127: 0.07912445586000026, 4012: 0.11360409900861608, 4096: 0.12736081246623496, 9223: 0.0704025512570127, 2074: 0.09107151203220192, 1508: 0.11434601359280198, 6720: 0.0816832073513267, 3900: 0.08720681574731083, 4325: 0.04351396513518764, 9101: 0.1247832032535616, 6925: 0.10624061128654826, 1970: 0.2257272472549722, 4901: 0.13028306477440446, 5812: 0.03438511002421736, 3887: 0.20332064833142502, 3878: 0.09955752191906972, 8995: 0.10901449431950619, 2217: 0.16399475599576413, 7688: 0.08861000062109023, 6210: 0.08134651787549617, 8825: 0.13556074789688716, 2622: 0.10033919016408543, 8493: 0.08703813864532983, 7133: 0.08085100121350258, 7848: 0.10522254938819446, 6834: 0.10353855061997896, 10782: 0.13501972186095113, 6923: 0.07415072740519682, 5210: 0.18803720948362035, 9256: 0.09107151203220192, 4376: 0.08682252720403888, 3551: 0.18803720948362035, 1384: 0.0845812378703395, 8587: 0.1454521640287508, 6076: 0.04287111991108518, 10365: 0.05105012336092683, 7789: 0.16399475599576413, 1358: 0.11848745823272512, 747: 0.07241754891251544, 9658: 0.224906596702033, 1064: 0.1425299117205813, 5945: 0.12359997540692594, 2051: 0.15281565175081863, 8179: 0.15769901097492767, 3615: 0.09736725705303839, 8049: 0.11759409401801692, 6371: 0.07867890910736922, 9927: 0.172868110229274, 6682: 0.18803720948362035, 8803: 0.0505128027048099, 7366: 0.07710736209949434, 9405: 0.13556074789688716}, {11640: 0.21967027568175893, 11259: 0.17754535568532934, 2683: 0.4637502272858479, 10638: 0.33867854560509403, 7356: 0.25654060147396934, 5570: 0.7243218248064782}, {11555: 0.3002803524534319, 8892: 0.39168936387560294, 2385: 0.39484676814677877, 3775: 0.34456673113601305, 9259: 0.5888750198256404, 7780: 0.3674332057133737}, {3192: 0.2586783734879111, 6029: 0.47691326428915376, 8747: 0.2982504547545061, 9199: 0.3164839818434814, 7399: 0.25153950161868777, 1691: 0.2586783734879111, 2007: 0.3774621520997915, 8803: 0.12811414130480037, 7507: 0.47691326428915376}, {11655: 0.5006163272142108, 6825: 0.8656692745653818}, {282: 0.7388227677347569, 6773: 0.673899783259168}, {2734: 0.15251432094087133, 6371: 0.10463162643750916, 8892: 0.15291080605323495, 11464: 0.1331724285871203, 282: 0.22280115510834467, 10785: 0.11959636634694029, 8950: 0.1374256588537082, 865: 0.13340982110301375, 3775: 0.17500765730643197, 10365: 0.08832603838891662, 4412: 0.07139020336807675, 5999: 0.23454485580998838, 3049: 0.2728480947790752, 5156: 0.2728480947790752, 8654: 0.25165881251498695, 5855: 0.283740883665514, 2081: 0.21190836622190587, 11259: 0.07974684285230815, 10188: 0.2254134999700584, 7045: 0.23125025857565693, 3675: 0.20054521125867747, 9528: 0.20829954326505984, 6923: 0.12829430301377823, 6774: 0.24214304746209572, 7970: 0.15802381750698877, 6530: 0.2728480947790752, 4367: 0.1382284588679606}, {1881: 0.2759081341912062, 5552: 0.4863274123791557, 7133: 0.22745697952199925, 3010: 0.3872391015887032, 11259: 0.12966877497154938, 11227: 0.2161554546047381, 6742: 0.22699922998509586, 6620: 0.4863274123791557, 5968: 0.19261053327485364, 6186: 0.1470062366912229, 4046: 0.25694753672453174, 5812: 0.09673514426849082}, {4689: 0.3592506648808172, 8339: 0.30817277885150157, 1592: 0.32559044151761946, 6803: 0.3614715706419671, 2711: 0.2802713409134034, 9919: 0.2741742399951968, 729: 0.28718366383022453, 7748: 0.16194683933677737, 6226: 0.17625721218522214, 6295: 0.2923645368113328, 407: 0.25648340768698513, 6923: 0.17539682350801647, 11432: 0.25254995716396206}, {6720: 0.2870361402015462, 10117: 0.5008524646636358, 10785: 0.24290129376077202, 9014: 0.6074613954705382, 10417: 0.4196398090869802, 4704: 0.2502980383269758}, {11590: 0.18528293900831241, 3822: 0.17526610109251856, 5334: 0.24268507640812056, 9735: 0.21264417352746695, 2528: 0.22292189115300906, 6371: 0.13745897464708354, 4616: 0.24268507640812056, 6494: 0.2998215996647614, 4042: 0.4274111792851414, 6193: 0.40705653155486654, 3469: 0.2839563239841704, 6511: 0.21331188165060455, 3256: 0.37276234979882683}, {5771: 0.2079726691471125, 8526: 0.46037405016056193, 4599: 0.15146053746260454, 10785: 0.16214111023147781, 10064: 0.25044254277982997, 5720: 0.1906163777531778, 9223: 0.1269311461255077, 6052: 0.18311799869106657, 10861: 0.1507860578209211, 5812: 0.09157277386829893, 11533: 0.19873849662317078, 2734: 0.15892728838817446, 4508: 0.2843208924749404, 6076: 0.07729379530768182, 3270: 0.24097393810474893, 10480: 0.10548220166959642, 4247: 0.2622406369792696, 6492: 0.4410734443871837, 4418: 0.13877210240226717, 6570: 0.11142321916340901}, {7346: 0.35623596412857367, 8372: 0.5777166128075393, 7353: 0.6752803289314474, 11655: 0.2886796329944417}, {10430: 0.47435349890742917, 7232: 0.8803344580750431}, {3900: 0.6044568320473238, 6186: 0.4712203805764493, 4704: 0.6423264677099175}, {5621: 0.6366001945310402, 11655: 0.2306535345400112, 4865: 0.5208776211833956, 9025: 0.5198323220785417}, {9401: 0.18443685132360743, 9863: 0.43778986299395656, 6570: 0.15651166604197222, 10785: 0.13455180752560356, 2969: 0.24289559424839208, 9558: 0.36602209710488204, 7346: 0.10549214721138345, 10910: 0.15867074419299373, 5215: 0.1909036123642805, 3351: 0.29746189851406135, 2171: 0.09423623647353627, 7114: 0.15551383006243374, 11669: 0.2211350119108549, 4448: 0.10410443992732446, 6128: 0.12881704430319635, 1384: 0.16464082904679186, 5968: 0.13326916471443398, 1841: 0.1945467583033315, 10365: 0.09937114713275423, 10784: 0.20386266600749434, 6212: 0.36602209710488204}, {11655: 0.16204505793155216, 3495: 0.35743861531425397, 11534: 0.32872581727643313, 11130: 0.3740884597395394, 2429: 0.46509688908688684, 7489: 0.28456669187653216, 3157: 0.28570553391061054, 7399: 0.2812704226301224, 2425: 0.3740884597395394}, {9929: 0.27347422407460553, 5983: 0.46179410648495056, 6649: 0.4695297289687457, 9341: 0.4781774191698089, 3340: 0.5126853239494921}, {5096: 0.2287253657729307, 3254: 0.33288874155040593, 8312: 0.28319940195434445, 1248: 0.20713874357309736, 7644: 0.21902238907771532, 11385: 0.4860908238205656, 3149: 0.37361999757160785, 6584: 0.37361999757160785, 4789: 0.20907164467530626, 806: 0.23727894211218511, 9420: 0.2242995371749312}, {4367: 0.2007468405522182, 5812: 0.05849213581847928, 6018: 0.09757416588251219, 4930: 0.1586465920034041, 895: 0.1734968439231353, 6896: 0.20577084409058513, 1384: 0.14388022169254663, 11263: 0.2789696916620798, 5749: 0.14258522894764677, 2722: 0.12080973779754967, 3902: 0.14226839905646335, 10160: 0.1138749657886131, 9084: 0.10364665681648344, 6462: 0.23414904992011437, 7133: 0.1375347567801731, 2174: 0.22736171290590804, 3372: 0.1932506943868412, 10910: 0.1386629427389545, 8189: 0.1617080933308228, 10145: 0.2940640578172671, 10288: 0.10905138923837722, 7151: 0.12654834947548216, 7970: 0.15536659742399822, 6078: 0.31986804719535294, 7784: 0.23414904992011437, 11046: 0.26826006843918115, 1566: 0.15492075631661648, 767: 0.26826006843918115}, {4991: 0.5710654625273037, 4710: 0.6138598418739767, 10430: 0.5450324137543865}, {5919: 0.5291851285265712, 1763: 0.5203507097711394, 11240: 0.6702225291550471}, {3353: 0.8041841829774803, 4325: 0.1550244506407528, 6232: 0.4903846642581344, 3902: 0.2979565414835059}, {5786: 0.6321245590798279, 5801: 0.7748667897181639}, {6186: 0.7189861142308922, 8803: 0.6950244366518075}, {11640: 0.17016310069726145, 8784: 0.41760137239205264, 4325: 0.12984060449914728, 5215: 0.2926392049928435, 587: 0.5610811350295203, 4382: 0.3563319389332004, 4624: 0.2076981904697591, 2171: 0.14445623622098186, 7313: 0.43401231867813067}, {7880: 0.44761123688157384, 6570: 0.2297738051538655, 9432: 0.44399440820995123, 11618: 0.35114267366381907, 3121: 0.4639388577210263, 6521: 0.45953966487071624}, {8114: 0.566466212312869, 8348: 0.5207689685471023, 1084: 0.3973663164724504, 10061: 0.2976456388991877, 2130: 0.3400730958536579, 2722: 0.21394645442310695}, {11655: 0.5047659705705622, 371: 0.8632562278686197}, {4046: 0.4730094380140625, 1702: 0.46509896236913834, 4862: 0.4137563987390214, 6692: 0.6234987323606099}, {8803: 0.10747130106438654, 9040: 0.3166422392998388, 5868: 0.4000689423197853, 2171: 0.10300195466986183, 6742: 0.17167282710442902, 4017: 0.24017793261505885, 9256: 0.19376422142320937, 4656: 0.20866118982351814, 5029: 0.19213032291911264, 4583: 0.2294886413943476, 2722: 0.15110050674451503, 137: 0.4000689423197853, 7381: 0.2654893905414811, 3980: 0.28064149015270534, 6369: 0.3094647787047445, 5749: 0.17833579263598984}, {4382: 0.221131021395044, 2171: 0.08964606192210514, 1112: 0.2854542299485419, 7133: 0.14971392372787715, 10288: 0.11870825784749608, 8129: 0.31387163236692794, 5221: 0.29201540298450895, 2185: 0.2829727583735479, 1874: 0.2829727583735479, 4785: 0.13170127968132872, 9730: 0.19393267244543935, 546: 0.3201044227208104, 10932: 0.2639263832482075, 9236: 0.34819344245711187, 11655: 0.1058032740441121, 2758: 0.25915335652900334, 3831: 0.23338061803783225}, {2711: 0.3043481717626932, 9283: 0.26041850524254456, 10492: 0.3712400325988646, 11259: 0.09099814664566928, 3222: 0.18893144735562198, 10160: 0.13216370432192856, 6180: 0.1527792348417344, 10754: 0.2257522217560869, 3485: 0.3712400325988646, 408: 0.293825045648979, 8803: 0.09972693476089442, 6076: 0.08464003479442972, 1163: 0.1544628034218414, 4710: 0.20308955103934087, 3764: 0.20052209519536732, 7137: 0.244022228500913, 8892: 0.17448464986519566, 10863: 0.3237732506725676, 6130: 0.15681473786250968}, {8803: 0.1458344086154869, 11590: 0.3061816784827179, 10861: 0.24145706749619, 3636: 0.29445779548625095, 9515: 0.3114025809651362, 417: 0.34208255084757533, 8532: 0.49908374261249283, 10253: 0.3279839111687061, 2104: 0.4114949928092048}, {3454: 0.3425484451183668, 3754: 0.25814769507693386, 10719: 0.5761464901024319, 8876: 0.483190172388428, 8297: 0.3356663681751241, 7246: 0.3713904936848375}, {1047: 0.2518864092070702, 4437: 0.3136380821975476, 2746: 0.354507414236614, 3580: 0.3081068668850682, 5006: 0.2697438125809618, 5123: 0.3217163769848145, 10785: 0.14942435520053207, 10289: 0.29755051073321587, 10963: 0.40647997864046675, 6567: 0.40647997864046675}, {7819: 0.6166753300349728, 6825: 0.6072329000529527, 4947: 0.5009787844006351}, {6018: 0.07746279526916786, 5595: 0.3303820819701187, 8803: 0.0887511837130015, 11642: 0.130072415386776, 105: 0.23345343207081706, 10430: 0.09480423802059144, 5771: 0.10546192465102838, 10155: 0.19642898189610136, 2663: 0.22147022130368627, 1011: 0.19642898189610136, 2797: 0.20098478719063181, 9336: 0.18588772706775197, 10811: 0.12370165347295053, 1812: 0.19642898189610136, 10480: 0.07901049097800959, 6076: 0.05789621964796329, 11359: 0.10705983997128321, 5812: 0.06041476721426299, 4448: 0.10668596027156912, 7346: 0.0731883579326213, 7795: 0.1719971297316537, 3277: 0.23175735478545284, 8251: 0.1719971297316537, 11259: 0.08098306891569414, 8493: 0.1529263358854131, 10365: 0.06894172956735871, 7151: 0.100465003194327, 6058: 0.09590919789979656, 10483: 0.05884456521147364, 8049: 0.15880745617774134, 2228: 0.1719971297316537, 9912: 0.10853259761603702, 3615: 0.13149169213573086, 6683: 0.2129679979577626, 10634: 0.1251796731553227, 4930: 0.12594735876408283, 2722: 0.09590919789979656, 6527: 0.09258499723465742, 9987: 0.10747319264225753, 4325: 0.05876436371258001, 5432: 0.16851614231044657, 6371: 0.08166884223070536, 6355: 0.11783658795163243, 7114: 0.10789240866692736, 8714: 0.2539388661838715}, {11640: 0.13143345705518647, 5381: 0.39841648005593394, 3576: 0.43337734775165954, 1283: 0.24026882899680688, 4325: 0.1002884846687139, 7970: 0.21050043763730575, 8041: 0.43337734775165954, 11070: 0.35220072682731424, 2212: 0.39841648005593394, 2622: 0.2312559957057257}, {4336: 0.5531951910502764, 4624: 0.27974601012403955, 4870: 0.4626377558570202, 812: 0.633785103386084}, {11359: 0.6035404841242735, 6069: 0.797332354807603}, {5467: 0.33579205734549716, 4596: 0.22743769363880303, 5529: 0.2816148754921501, 4160: 0.16263318560639536, 2037: 0.3087034664188236, 11528: 0.15950507050235618, 5123: 0.2657690656026707, 1551: 0.3087034664188236, 9693: 0.16357493179734467, 4480: 0.33579205734549716, 10425: 0.33579205734549716, 1163: 0.1397138724002252, 2739: 0.17643954621369493, 4376: 0.15504545676806164, 11590: 0.14556600832173036, 5738: 0.22072168664766245, 8969: 0.21499233846275786}, {10365: 0.20268418075860772, 2713: 0.33267486791170975, 2960: 0.26970063576880116, 10592: 0.573825106633153, 6896: 0.28372997299095987, 11315: 0.19093171192207256, 10893: 0.573825106633153}, {7051: 0.5892147855273365, 747: 0.30488594890028453, 8366: 0.3801880486859733, 5029: 0.3801880486859733, 717: 0.520369666633108}, {6018: 0.1690811787999816, 11259: 0.13586544750868979, 1360: 0.35488189437136974, 11315: 0.18442924952086304, 6720: 0.24077991862406675, 1566: 0.26845409193800635, 6308: 0.46171198949520864, 8917: 0.46485386973510134, 8631: 0.48341201678820256}, {495: 0.30677373318405854, 6646: 0.40520922356208294, 6592: 0.13731708479164303, 6226: 0.19772772921381773, 10480: 0.14314090038696659, 7403: 0.3134409333842454, 7151: 0.12321898844144148, 9064: 0.25311393374441704, 8725: 0.16686044374000483, 6058: 0.11763135392116604, 10288: 0.10618235580140933, 2171: 0.13623489227311814, 10638: 0.14562908732934524, 8262: 0.2612024132224015, 1780: 0.1919855558580215, 5812: 0.056953265982570514, 11259: 0.07634309419223394, 10365: 0.08455611315974203, 11342: 0.2921255083057262, 917: 0.12958690682730908, 1248: 0.17267252402169098, 9929: 0.13501485882214345, 5611: 0.2028637095083837, 10564: 0.09173770683783289, 11464: 0.12748837315433373, 9641: 0.23607730110438482}, {2857: 0.64255035443528, 9704: 0.3366868682768562, 10782: 0.3780905313761844, 2171: 0.17637634057105933, 3724: 0.3408296036641015, 10211: 0.4284216527926979}, {6018: 0.48036145985484435, 6069: 0.8770706173884305}, {6128: 0.2551256968586329, 6527: 0.20314788778402057, 4536: 0.5122381923381855, 5831: 0.5571868643120929, 4596: 0.3773921764164635, 7756: 0.42234084839037084}, {9223: 0.21159290641471473, 3112: 0.47395998405082235, 3531: 0.6412538373913367, 6963: 0.5651405944580543}, {7354: 0.751561107941631, 11227: 0.5297029447529643, 11640: 0.39315479311536566}, {8049: 0.5032539368722663, 5385: 0.5340203418997016, 6527: 0.29339783833260513, 11259: 0.19725281648624235, 641: 0.5801437617607297}, {6923: 0.39420849825756865, 3271: 0.9190210334380343}, {11454: 0.16963424182824355, 3597: 0.21583357938310094, 6922: 0.24207109294837043, 2480: 0.22534651675931802, 864: 0.2515840303245875, 2341: 0.22029198891352203, 7259: 0.1728714896287791, 1566: 0.20494283505611238, 776: 0.2465295024787915, 2171: 0.08373698291487353, 7896: 0.13219817264625738, 4418: 0.13313285546106512, 9004: 0.203398803400225, 7748: 0.11842108923679151, 8587: 0.2515840303245875, 595: 0.17355353926529424, 8366: 0.1561952258020042, 9773: 0.15663283449909055, 5945: 0.21378698741634244, 10910: 0.14099257241995414, 4013: 0.29900452960933044, 8803: 0.1136715177111956, 5577: 0.1678169617829831, 6130: 0.13738482185029288, 7570: 0.25741905449621566, 4600: 0.22273494817429768}, {2656: 0.4677082631509606, 9084: 0.28400938281910315, 5968: 0.3191321951822644, 5914: 0.3806341084874027, 1591: 0.673691300600782}, {7346: 0.13764595178831748, 11640: 0.14484060819008288, 10160: 0.1700231337481943, 8892: 0.22446727801132593, 6076: 0.1088859004833253, 6371: 0.15359526894488365, 9952: 0.4775849318016431, 10365: 0.1296592826002211, 7206: 0.36942561543449953, 4325: 0.11051862622603917, 411: 0.4390578059589762, 6526: 0.4005306801163093, 11659: 0.3139244939206751}, {11655: 0.6109922065604263, 11642: 0.7916366107769532}, {4278: 0.3429411925111211, 11259: 0.12274836444841769, 5872: 0.48491043383065524, 7900: 0.500769614536502, 9693: 0.24394071792981717, 9084: 0.16226408619767127, 10279: 0.20683273423695772, 6186: 0.13916052744019355, 1209: 0.28876671911849644, 11310: 0.38735963118258476}, {3900: 0.5554817420172409, 7489: 0.8315287332891699}, {5029: 0.4703133089132931, 9996: 0.6036743685952988, 6295: 0.6437256000502828}, {11534: 0.6043227232218786, 2354: 0.7967396351366566}, {4912: 0.4892302390398377, 6260: 0.6683312242521534, 6767: 0.41490810358141106, 4862: 0.28395757867040916, 4624: 0.24739948867521225}, {11331: 0.20731053255630355, 9828: 0.3024927800850608, 6094: 0.22207923627684453, 8081: 0.3024927800850608, 7016: 0.2724661240962127, 7346: 0.10727630344094446, 6291: 0.24947967561966145, 8950: 0.15722552266722625, 6128: 0.13099568734102035, 2569: 0.2724661240962127, 7939: 0.3722125162210852, 5526: 0.3121592042433889, 6230: 0.2578903797490138, 3213: 0.22786372376016564, 2171: 0.0958300249478361, 10365: 0.10105178077120194, 3723: 0.2945947364678584}, {4382: 0.3214654826594782, 4947: 0.21942940069721406, 10480: 0.15749285201821805, 9781: 0.4653463910318881, 751: 0.3597921940713286, 11298: 0.24515683889755166, 5881: 0.4414600688904889, 10430: 0.1889747759374456, 5431: 0.34664660239431266, 6128: 0.17814403641200652, 4624: 0.18737528621979052}, {10623: 0.880668461166529, 3604: 0.47373311210699376}, {10792: 0.6526639712460022, 4862: 0.27730094635007796, 11227: 0.26668474611364557, 6742: 0.2800634021809291, 6923: 0.2573719762804243, 3115: 0.5304124136607299}, {7346: 0.2352662437299507, 7489: 0.43558567158673656, 7896: 0.33179173446089694, 6186: 0.22684274208979946, 6210: 0.35313597371172717, 10291: 0.6845922265442485}, {770: 0.31548921473231895, 2475: 0.22153901086667277, 1263: 0.272318732053702, 9376: 0.38820442069099864, 8251: 0.2629374822034704, 2070: 0.31548921473231895, 1742: 0.19253973281573045, 8476: 0.38820442069099864, 10365: 0.10539341452660159, 3192: 0.21056258159403876, 6138: 0.38820442069099864, 3558: 0.25285574548855483}, {11259: 0.14133898456546118, 4152: 0.32520789850357706, 4862: 0.24498863297483753, 9001: 0.43706541331727494, 1384: 0.2593668461758303, 11257: 0.3226630570627636, 6259: 0.5028869726073382, 6369: 0.4460264475133203}, {9823: 0.21345447543074003, 8747: 0.14520310090205552, 2233: 0.18376728043270477, 5080: 0.23218501005507824, 10336: 0.1590068604805316, 10935: 0.20249781505704295, 6445: 0.167388006341662, 237: 0.13853233693338726, 9615: 0.1402763258561934, 3804: 0.15123298622989048, 7468: 0.15562464578134297, 5812: 0.042458123786469654, 8778: 0.20046281288497392, 3738: 0.18869405547856688, 5149: 0.11181763556196637, 1063: 0.12439289043663422, 11680: 0.12489799765542724, 4991: 0.11816357653266657, 6421: 0.2390867440645435, 2702: 0.1590068604805316, 11292: 0.21471775668345677, 8646: 0.10023974209495035, 4127: 0.09770147424495294, 11259: 0.0569130582335339, 11315: 0.07725608541698209, 10507: 0.27771067524411314, 3328: 0.12541272654048385, 4826: 0.12299859896770562, 6720: 0.13122315411185895, 4367: 0.12834625474907174, 6592: 0.10236859438166306, 10691: 0.14991455648663676, 7133: 0.12988622430930075, 6530: 0.2533416878630263, 1148: 0.16503674580836658, 1863: 0.18376728043270477, 10754: 0.14119242882146432, 371: 0.12065972990327013, 4382: 0.14745627621181331}, {5762: 0.3678347784222162, 6570: 0.13369172412972602, 1839: 0.34114388909668136, 1383: 0.40677328628477183, 10394: 0.302753152796714, 6779: 0.302753152796714, 6434: 0.24736010415406065, 10776: 0.21138812898965037, 9576: 0.20988509472050046, 10555: 0.2755144919085909, 8725: 0.2179283874586398, 3328: 0.21971507508044777, 2171: 0.10472805849878822, 3874: 0.22157184650917772}, {371: 0.38365923951331127, 6018: 0.22520678944370537, 5489: 0.6787167663985548, 7991: 0.5843210084653945}, {1456: 0.436013829897116, 9131: 0.3802650647768986, 1047: 0.20767228030362783, 9223: 0.16324687061882848, 213: 0.19019694721071237, 10854: 0.1952336115580432, 215: 0.24849989816290663, 7234: 0.436013829897116, 9290: 0.2815204130342453, 8657: 0.436013829897116}, {9084: 0.16464989423058934, 1742: 0.19370890917633002, 2991: 0.4431626801723981, 2546: 0.28156577620097806, 7016: 0.28589808625530494, 10365: 0.10603340444985189, 1691: 0.27561175515641195, 3055: 0.3905617493582538, 300: 0.3905617493582538, 8696: 0.3719619859210799, 11331: 0.21753047177889198}, {4600: 0.3678217799662485, 6114: 0.3872094213515408, 8153: 0.39316721669117743, 10533: 0.3113839361265143, 2705: 0.4937724376006381, 7819: 0.28660420044208007, 1326: 0.2901103271087421, 4818: 0.23047428598539085}, {6076: 0.24908678544479873, 7067: 0.3729111515774438, 865: 0.448003807675781, 9929: 0.4736081972250065, 8892: 0.5134901072517412, 11655: 0.3319771376613728}, {6018: 0.1460504835478552, 691: 0.5432659473251438, 8178: 0.3358582729565732, 5854: 0.356348767586318, 7346: 0.13799134189503504, 9987: 0.20263291170027156, 11019: 0.23601719678678137, 7023: 0.5726607359837752}, {1702: 0.15839085264743352, 2544: 0.3048869414578754, 4517: 0.22711688412669756, 3391: 0.3048869414578754, 1170: 0.1902366079532163, 3620: 0.33164063402844074, 590: 0.33164063402844074, 9008: 0.2624833419777072, 6618: 0.2058865148628192, 8062: 0.27813324888731006, 4766: 0.33164063402844074, 8416: 0.33164063402844074, 4164: 0.2695204836068658}, {6269: 0.35612431425641017, 11019: 0.17555219463334062, 4518: 0.2986666963234863, 5825: 0.35612431425641017, 1248: 0.1974389580064314, 1969: 0.2032316586851444, 5444: 0.2986666963234863, 9987: 0.15072059510164862, 3801: 0.23196046765160633, 2201: 0.35612431425641017, 11490: 0.26993788735702434, 8259: 0.23196046765160633, 10376: 0.35612431425641017}, {1745: 0.48638996940425416, 1305: 0.5956570935956507, 7428: 0.6392319019824582}, {9586: 0.8182545257979583, 5149: 0.45183344984157875, 4704: 0.3553956451835276}, {5968: 0.1278641463281273, 9478: 0.30627572469819664, 399: 0.35117728158521105, 1082: 0.2164726109241679, 3056: 0.3228475533635123, 1116: 0.2853976897067253, 9440: 0.2945178251418135, 7420: 0.27794599647649787, 7031: 0.35117728158521105, 2218: 0.2945178251418135, 9441: 0.35117728158521105, 3874: 0.1912883696065408}, {6278: 0.2716324596307055, 10365: 0.08368648590799095, 2171: 0.07936206538029641, 10160: 0.10973868049480061, 3775: 0.16581492970936892, 5812: 0.07333583878500127, 9704: 0.19709977183183014, 9223: 0.11541092804030467, 8244: 0.2833827563620704, 11553: 0.2833827563620704, 4127: 0.16875497373174403, 324: 0.2746447663244482, 3519: 0.2833827563620704, 6007: 0.2833827563620704, 5738: 0.20261748491751233, 10480: 0.09590868087163901, 7067: 0.1052150939334158, 747: 0.11871411443438361, 8969: 0.19735807367852895, 4271: 0.23843978572234834, 4368: 0.20261748491751233, 6451: 0.16077154110503347, 6924: 0.268836663519764, 9820: 0.2833827563620704}, {5097: 0.24080452193858437, 11585: 0.1865408030377719, 9331: 0.21258091843165, 9727: 0.30488300162086657, 9729: 0.13321104115238308, 6058: 0.1320316743560909, 2171: 0.09000314313972807, 10448: 0.27041042923641784, 7927: 0.321379473454561, 3210: 0.30488300162086657, 6186: 0.09714604963605938, 3209: 0.19561946661575708, 1455: 0.15688669706448286, 4902: 0.321379473454561, 7925: 0.2558990001422522, 6180: 0.14386547254020524, 8067: 0.24848119188654294, 7967: 0.284099905009414, 10776: 0.18166665461209655, 6226: 0.13853000154372871}, {11185: 0.5247364104966923, 6806: 0.8512647646291058}, {9586: 0.8182545257979583, 5149: 0.45183344984157875, 4704: 0.3553956451835276}, {2171: 0.22011059288276838, 7346: 0.320575591241558, 9693: 0.41646321149711674, 8307: 0.4365647577046703, 10274: 0.6163401701468166, 4704: 0.3238472134573418}, {2673: 0.3906868026916531, 5592: 0.3380464780286368, 4419: 0.3673930879482084, 4543: 0.3215193244006429, 1249: 0.4936222541779753, 2245: 0.3244660918137255, 1307: 0.38183094335480894}, {2722: 0.26770393857495534, 4135: 0.5760334699283639, 6226: 0.36543301282518087, 2494: 0.5482770501197829, 2734: 0.3322756757485017, 6371: 0.22795593334915007}, {7346: 0.09998405437460066, 4323: 0.2325502283481183, 9762: 0.26664325781544096, 7429: 0.14650900162682906, 6527: 0.09721696272207192, 2910: 0.22362264445479632, 5912: 0.24513295113511865, 11480: 0.16801930830715137, 7688: 0.1634770749460297, 11359: 0.11241597215950638, 4475: 0.13027113994615036, 7313: 0.20625619248146954, 5924: 0.17367725904411263, 4704: 0.10100443790489133, 3762: 0.21669787240475727, 8546: 0.1572103718754022, 6018: 0.08133820710653117, 7970: 0.129514204548021, 5483: 0.26664325781544096, 81: 0.26664325781544096, 8950: 0.11263222956968849, 4616: 0.1514006711403304, 3584: 0.24513295113511865, 82: 0.26664325781544096, 3874: 0.14524218031375125, 9759: 0.26664325781544096, 6242: 0.24513295113511865}, {3140: 0.8072819001404711, 6226: 0.34107079874907376, 4789: 0.48162915603839795}, {7346: 0.05699075131880057, 4448: 0.05624106062788476, 4241: 0.18286278854701563, 10494: 0.12024548716798433, 10514: 0.10551600728546064, 3126: 0.10313317707253379, 2898: 0.13122116455269808, 10443: 0.19773864562156962, 10234: 0.13122116455269808, 2336: 0.17245576568964638, 6570: 0.064989568763661, 6851: 0.13092672788484355, 4964: 0.1354170160618882, 3781: 0.14717288575772317, 237: 0.11798003959644428, 9576: 0.1020283183914192, 3809: 0.12660303570819378, 8424: 0.22437012408742923, 2171: 0.050909892916687724, 9934: 0.12189000582579992, 5888: 0.12660303570819378, 11585: 0.10551600728546064, 4695: 0.1128447323787362, 10795: 0.1565040444846213, 7133: 0.08502236084554594, 7848: 0.1106513145031687, 11589: 0.17245576568964638, 8853: 0.13700478137632677, 5464: 0.12460060207457119, 7542: 0.16583520321151948, 2862: 0.18178692441654454, 8841: 0.13871026290178137, 7151: 0.07823069371991481, 395: 0.10132015577627054, 7489: 0.10551600728546064, 9871: 0.16069989599381143, 6222: 0.1565040444846213, 6765: 0.17245576568964638, 1414: 0.15295650258135182, 2697: 0.15295650258135182, 5520: 0.12660303570819378, 11681: 0.18178692441654454, 4800: 0.19773864562156962, 7485: 0.18178692441654454, 7870: 0.18178692441654454, 2908: 0.12879645358376127, 2005: 0.11402516902293709, 5940: 0.18178692441654454, 6494: 0.13871026290178137, 4616: 0.11227646970247877, 11375: 0.14255475691321884}, {6681: 0.46302202773846424, 7151: 0.29070914453449564, 751: 0.5222994162525982, 7819: 0.3921027253378839, 11232: 0.44393897098421103, 4704: 0.2783446743145296}, {11257: 0.33050998449887786, 8892: 0.27760161320314275, 7361: 0.5906355201712722, 3122: 0.5906355201712722, 407: 0.3405875204959598}, {9934: 1.0}, {3739: 0.7363247031023744, 1090: 0.6766283555994386}, {10365: 0.23572308862537114, 1772: 0.7572430359937878, 11259: 0.21282707170105597, 7137: 0.5707208139528291}, {11655: 0.329013652033214, 6182: 0.9443251647476981}, {4583: 0.3792478352344341, 6128: 0.232681995017816, 8491: 0.37539972242632846, 6058: 0.24970534374820164, 1518: 0.40204436980871144, 2193: 0.6611450544891557}, {10430: 0.5416806010745381, 4537: 0.6140668898917068, 7151: 0.5740243733137589}, {10913: 0.5727516159269458, 6058: 0.23530237954566582, 10155: 0.4819163110943211, 371: 0.32376016075535696, 3222: 0.31706235845475084, 11227: 0.2545679777963378, 1691: 0.337921602703824}, {271: 0.21111819694095138, 5901: 0.2667421565151543, 6343: 0.24522387160437326, 4325: 0.061727191817831456, 10600: 0.09259990264308306, 1704: 0.16329612363507315, 5771: 0.11077918726818638, 10748: 0.2237055866935922, 6467: 0.2667421565151543, 7354: 0.1546436547641233, 2472: 0.20218730178281114, 10323: 0.216778246227021, 8546: 0.15726868162411783, 10472: 0.23263648185173244, 9084: 0.0864323054372796, 67: 0.2667421565151543, 1432: 0.2667421565151543, 7520: 0.18711504036502394, 793: 0.2237055866935922, 10782: 0.14721693104250677, 2423: 0.2237055866935922, 7536: 0.2667421565151543, 1380: 0.17078283640517145, 2654: 0.23263648185173244}, {3284: 0.4133271142158705, 3948: 0.5452952031110184, 6527: 0.19881224024820873, 4704: 0.268737848569678, 1136: 0.30231723136608873, 3893: 0.4133271142158705, 11259: 0.13366245151554873, 3900: 0.19437974108237976, 11533: 0.3196613107281631}, {11640: 0.17657180963649768, 11315: 0.19372254446594947, 4785: 0.22021710613001433, 8142: 0.27806351230948034, 10555: 0.394342574617269, 5749: 0.2595286589105876, 4624: 0.2155205517482766, 6570: 0.19135232536453714, 5149: 0.28038693340872806, 6923: 0.2295901556254654, 6176: 0.5077708994691389, 4187: 0.33225488884739085}, {6570: 0.25193549205611104, 4692: 0.6428689144446291, 6210: 0.3316136690849962, 5176: 0.6428689144446291}, {2171: 0.35674534234701316, 4983: 0.7379119485790363, 1763: 0.572903758809455}, {9675: 0.413050249528048, 10157: 0.26259633891749373, 9592: 0.413050249528048, 1499: 0.375202774760194, 4182: 0.459928609631925, 3663: 0.48735069238799583}, {11655: 0.11000370095909744, 9515: 0.15961045642328045, 8721: 0.29420687566546017, 5541: 0.3157293394639125, 3213: 0.2216220639253752, 665: 0.29420687566546017, 813: 0.3328126800498014, 9780: 0.3328126800498014, 9033: 0.3328126800498014, 5327: 0.3157293394639125, 9777: 0.2134417464204499, 9291: 0.30360851642422176, 9897: 0.1779201992458382, 11315: 0.12045568398552947, 10910: 0.15693446502469435}, {9450: 0.3081876563718282, 1419: 0.4871150505259864, 10606: 0.5510342512949659, 5771: 0.2489281574261764, 4862: 0.2546649758062078, 6300: 0.4871150505259864}, {1593: 0.4545453677251998, 10549: 0.5450427774625193, 2171: 0.23374533557713728, 3887: 0.6645900973618527}, {11547: 0.17697418483585264, 4283: 0.40928875468401454, 9825: 0.4880278882411263, 5296: 0.4880278882411263, 4293: 0.3966145850544533, 2423: 0.40928875468401454}, {2722: 0.19846669248391827, 395: 0.269253181068745, 11547: 0.2814738003949332, 11158: 0.45829246066416385, 1242: 0.4830895422618526, 316: 0.2887288967289151, 4325: 0.12160219413098106, 7819: 0.2804034438914372, 1574: 0.3487135970988508, 6876: 0.2590366329964639}, {3822: 0.28812716154430545, 6210: 0.3039676194179326, 11359: 0.2962303599280914, 7067: 0.23983240716111356, 4826: 0.2860953700254334, 2158: 0.6459564504050835, 10253: 0.42450455696973516}, {8189: 0.3571087125084448, 1508: 0.4295530227258869, 3329: 0.3881264593996709, 10610: 0.3670859134152066, 10432: 0.5924131914768141, 6371: 0.22717826121323634}, {3697: 0.29367148394765114, 1133: 0.48722550611288407, 9804: 0.46221612467401285, 7074: 0.2981900583985012, 8705: 0.40735365659411615, 3648: 0.2857515920702163, 7380: 0.35526938365331123}, {11655: 0.15076270015731372, 2171: 0.1277397365625136, 1702: 0.23696147678147975, 6920: 0.3805864179361266, 9164: 0.38378834104482595, 3497: 0.43271460252199784, 5183: 0.3058383418576625, 5777: 0.29786100602374055, 5823: 0.4032175517136872, 9223: 0.1857633300451369, 6210: 0.2146399495234659}, {7346: 0.05494723637490394, 11655: 0.057931069464509904, 10350: 0.17526859840655443, 4804: 0.1906483394234084, 5558: 0.28161071433932505, 1304: 0.2480392082133822, 11666: 0.1906483394234084, 11691: 0.1906483394234084, 4367: 0.11964949963383574, 6018: 0.058156333085574274, 7165: 0.20802019943644093, 9307: 0.14747194777977224, 9623: 0.11835679389306443, 7133: 0.0819737176721767, 6076: 0.06963584224431639, 6128: 0.06709637418092494, 9521: 0.11240447849211023, 5450: 0.17526859840655443, 5758: 0.07579850004711076, 2929: 0.2480392082133822, 7067: 0.06507418546441944, 6148: 0.1906483394234084, 9974: 0.22732544611342373, 3838: 0.14747194777977224, 10218: 0.11518163286491317, 11315: 0.06343538022375657, 4418: 0.07803898158001397, 9912: 0.08148244425815514, 4019: 0.11240447849211023, 6371: 0.061314084716131596, 9713: 0.09702473747525629, 8917: 0.1598888573897005, 6969: 0.17526859840655443, 3628: 0.1906483394234084, 1951: 0.17526859840655443, 10228: 0.1906483394234084, 8010: 0.15493768666428367, 10802: 0.1906483394234084, 3928: 0.1253163144082777, 8741: 0.17526859840655443, 7640: 0.10993657339142375, 8362: 0.13955794564742974, 3489: 0.14747194777977224}, {6186: 0.09375411779035621, 3902: 0.15005478190750796, 4325: 0.07807232563315654, 10600: 0.11712001696245888, 4367: 0.1433421674399362, 4159: 0.19891285124213998, 8803: 0.09062957074036601, 9961: 0.274180330113957, 7615: 0.20253986616548447, 8892: 0.15856768240175198, 11618: 0.2204625158831317, 5812: 0.061693424131825825, 5606: 0.22850950936959244, 1749: 0.49834304541079966, 11259: 0.0826970465746211, 10369: 0.40352519013641974, 4624: 0.12488759446494181, 1204: 0.1935256840977394, 4862: 0.1433421674399362, 5562: 0.33737450044707296, 6180: 0.1388422947616502, 10564: 0.09937293602371357}, {6758: 0.3150050837841285, 6454: 0.6423337171231495, 6551: 0.6986982131341914}, {878: 0.2456298358711656, 9515: 0.1428734268050802, 11546: 0.297913364502868, 8118: 0.2219221156605058, 7805: 0.2717716001870168, 9846: 0.23033788404370223, 11705: 0.2372140674879692, 3403: 0.3240551288187192, 962: 0.20580123829532593, 6434: 0.19705893459320994, 9095: 0.2564796483595534, 9375: 0.2633558318038204, 5200: 0.297913364502868, 9897: 0.15926317820107325, 7826: 0.297913364502868, 10651: 0.2717716001870168}, {9223: 0.20695607811281602, 6226: 0.2849821682037745, 11259: 0.13549127539789882, 7151: 0.21868510929267757, 4785: 0.2090754272255663, 10564: 0.162813140246036, 5058: 0.2571448994877672, 7133: 0.2376704511965298, 11575: 0.3668140131092073, 6355: 0.2564983456481298, 9675: 0.39289821828479793, 5334: 0.3138562484922997, 7318: 0.40462724946465944}, {1662: 1.0}, {9027: 0.714903575524601, 8646: 0.20894745428459865, 10365: 0.13139665824336547, 8259: 0.3152417076917563, 10349: 0.4449409816384029, 1508: 0.29431239496365924, 4152: 0.2098074355536466}, {11655: 0.24960070965880754, 230: 0.8214238275802652, 10910: 0.3560875997677016, 10480: 0.25557765494297824, 9084: 0.26616548387559885}, {2816: 0.7423813951620327, 4537: 0.360256553800396, 3597: 0.5648761630314526}, {9693: 0.42016601756765365, 2520: 0.6537878034065833, 7346: 0.24859215719044345, 6197: 0.5781210644679323}, {9084: 0.31466253257553284, 9729: 0.37004434281082166, 11590: 0.42096903148425036, 8892: 0.4564182680812263, 11234: 0.46898886854455984, 865: 0.3982104408740416}, {2171: 0.1962848728927851, 6527: 0.27796341871108904, 5623: 0.6649092368631627, 2581: 0.6649092368631627}, {11655: 0.23495480841628508, 4616: 0.33745487166300764, 4891: 0.5183280025864628, 747: 0.22888563266621237, 6146: 0.4597215880174137, 2956: 0.5463734601876565}, {7346: 0.18422477006472618, 1455: 0.28686282290537735, 3893: 0.6303543062402516, 2463: 0.6974413262301914}, {1202: 0.3361752276386463, 8993: 0.5359541338208236, 10483: 0.13509327191390963, 1691: 0.31621120716239587, 9800: 0.34783519294913595, 10564: 0.17171664913641782, 1518: 0.35451430914419896, 2674: 0.2997535054070818, 10480: 0.18138949116854985, 11019: 0.28738306694751503}, {7356: 0.19527896818995696, 7595: 0.5513545137611598, 6358: 0.4808582790593947, 7970: 0.2678044135027905, 10064: 0.31306039426456544, 9710: 0.5068763417730595}, {8646: 0.0681318465497343, 6199: 0.15781358915744867, 8484: 0.1376182099184693, 6088: 0.11962077608303687, 11640: 0.07069688331388295, 10685: 0.18875688140770036, 1204: 0.09052546283475295, 1047: 0.0751662118858951, 8929: 0.12207332856689, 9260: 0.09006045418295082, 353: 0.12825326725736264, 3764: 0.0852416462918102, 1812: 0.12207332856689, 2711: 0.09944271724303841, 2578: 0.15781358915744867, 5777: 0.09474202461859983, 8779: 0.12207332856689, 1053: 0.1505116686689845, 2171: 0.06001657018607575, 1414: 0.12207332856689, 10061: 0.08292202703486047, 11555: 0.07398084186009721, 9743: 0.1376355303174502, 7346: 0.04548385059545714, 9741: 0.14402860232448925, 4310: 0.1954995295148981, 598: 0.15781358915744867, 7983: 0.20532021321723276, 4818: 0.08810456832840566, 5105: 0.13235171377450747, 3661: 0.14508265146597807, 7114: 0.06705113128386277, 4608: 0.14508265146597807, 7479: 0.1376355303174502, 6837: 0.15781358915744867, 10430: 0.058917318542855014, 1529: 0.15781358915744867, 1742: 0.07827161328297355, 3222: 0.08031447902634802, 7366: 0.06471373190870616, 1384: 0.07098631574226232, 10634: 0.07779452514328233, 11547: 0.057228146116437555, 4451: 0.10688983839156627, 4448: 0.05839741909520292, 6250: 0.09727941263745328, 1596: 0.16250462160258958, 5812: 0.02885831820945097, 9235: 0.15781358915744867, 8150: 0.15781358915744867, 9328: 0.10807520841736416, 1283: 0.08749346605990939, 9079: 0.15781358915744867, 10354: 0.15781358915744867, 4411: 0.15781358915744867, 6371: 0.05075415712626198, 6076: 0.03598035369141691, 9083: 0.1376355303174502, 4862: 0.06705113128386277, 11259: 0.03868317764196005, 4785: 0.05969168028114004, 6205: 0.07732951649148022, 1652: 0.15781358915744867, 835: 0.15781358915744867, 11642: 0.06213168280845887, 4537: 0.06679060406926317, 11432: 0.08960692840784244, 1963: 0.11745747147745175, 4656: 0.08230974164845732, 1205: 0.11745747147745175, 8012: 0.09199559609451055, 1018: 0.09944271724303841, 4127: 0.06640661390204516, 7504: 0.14508265146597807, 7838: 0.0800463449113815, 10564: 0.04648365444997823}, {11655: 0.3369544849592382, 2557: 0.7835676533452387, 11315: 0.28359850237230555, 9450: 0.43824114041138984}, {8991: 0.5143345658670785, 6371: 0.16541425558379, 7346: 0.14823765605278574, 6197: 0.34473859706609533, 2683: 0.32930496314335583, 4660: 0.3828088437930853, 4367: 0.21852816784602164, 1541: 0.5143345658670785}, {10811: 0.6391133001787676, 11533: 0.7691125987361047}, {4023: 0.4738053332814517, 11547: 0.17181664132057733, 3804: 0.3086116344324066, 5680: 0.36650198778957005, 9223: 0.17739629488119515, 6511: 0.23646622287090893, 2626: 0.4738053332814517, 10785: 0.17417353900910157, 2375: 0.35264375341116755, 7688: 0.2232743774045554}, {2479: 0.3658794880639992, 2171: 0.20650501798089055, 1163: 0.3337252688507376, 2739: 0.42144945225906266, 6287: 0.5432653476168262, 4947: 0.34770387813415066, 4818: 0.34418084939900084}, {4710: 0.5925183890433591, 6421: 0.5803458975817312, 10279: 0.3028546266777816, 3596: 0.46946743540460695}, {955: 0.5942072868770274, 5310: 0.5942072868770274, 8695: 0.5420658635668586}, {10961: 0.6443424329782338, 6186: 0.23296041920582977, 1605: 0.5403832820583171, 1064: 0.48840370659835874}, {5712: 0.5860271207745781, 2777: 0.5860271207745781, 4418: 0.27504903023939364, 2674: 0.3454930251273541, 10365: 0.18242508198959548, 4947: 0.291287001850622}, {9817: 0.4615853070551838, 6206: 0.520797566939687, 3822: 0.2698279816384537, 11359: 0.2774165395966618, 7529: 0.6049312543286269}, {6094: 0.45906891661177224, 4152: 0.3335419466877965, 10782: 0.4246459705698934, 4624: 0.2848183403333948, 3088: 0.6089686220878421, 6186: 0.21381541011238583}, {10365: 0.15240535458422724, 11334: 0.4047038966950115, 2267: 0.4047038966950115, 3549: 0.5613674502560396, 3790: 0.31874576323969095, 6186: 0.1560002607939639, 553: 0.45621679357055456}, {251: 0.34470298248440373, 10634: 0.27370927403959827, 4599: 0.24806230088700415, 4325: 0.12849012084583217, 5914: 0.31371236660537904, 10430: 0.20729243423008575, 5708: 0.45124137732783104, 11019: 0.27370927403959827, 9945: 0.5552452803370398}, {11640: 0.20529334305828084, 6006: 0.5677018575520867, 1559: 0.5903659574783152, 2889: 0.5357586028040774}, {11640: 0.06789489694458618, 7356: 0.07929064433101018, 6843: 0.18775113961102372, 3408: 0.26776623122612436, 1881: 0.1167627328340903, 6128: 0.07878860929959845, 11334: 0.1613940680211988, 6876: 0.14357848162519624, 2512: 0.2442698643540377, 2873: 0.22387077858997784, 6399: 0.291262598098211, 4586: 0.291262598098211, 11559: 0.23670572373013232, 11227: 0.0914757430788671, 3804: 0.14581753734981256, 4537: 0.09474763621631467, 7067: 0.07641403334583396, 2722: 0.0845528970434201, 5601: 0.1351561156178804, 5812: 0.053261248180875534, 6527: 0.10619306624751218, 8325: 0.20581095910050076, 10480: 0.09062338989340556, 10811: 0.10905453699161219, 6186: 0.06221219243875915, 8326: 0.20581095910050076, 10610: 0.11633906801948052, 11501: 0.22387077858997784, 3335: 0.20581095910050076, 2054: 0.20581095910050076, 6117: 0.09587247903862045, 4967: 0.1277577178603355, 10483: 0.05187696876673707, 3822: 0.0918014324698688, 11359: 0.09438322731084109, 4475: 0.10937423194752087, 3495: 0.15005193750705165, 4325: 0.051806263670383684, 10600: 0.07771704545278879, 8950: 0.09456479467987927, 9382: 0.19524664192936358, 2171: 0.057637885246787854}, {6618: 0.4459546356047787, 10785: 0.26406615912398934, 4325: 0.1662322867287917, 10585: 0.555657149630758, 5446: 0.5346465486786033, 8803: 0.19296928415081874, 10430: 0.26818167137526316}, {4376: 0.3950400464836979, 1841: 0.5916386008838329, 9912: 0.3656651062517693, 1084: 0.600163442479014}, {10365: 0.2627939274866169, 5357: 0.7661191838722762, 11545: 0.4688145444797318, 5968: 0.35243959860279284}, {9773: 0.3500866633179909, 3490: 0.48240496879118433, 9523: 0.3616654730253382, 1829: 0.7168841328093477}, {4821: 0.6453325977778901, 9468: 0.763901720279016}, {3822: 0.6030335158007378, 11359: 0.6199930422279396, 7067: 0.5019553828877081}, {6117: 0.45737354446016076, 11185: 0.5060153901580642, 4789: 0.45935897451546553, 747: 0.3161456474757199, 11259: 0.20121711980314605, 6451: 0.42814810353838473}, {11259: 0.19041294317091187, 3140: 0.5600268888774499, 5623: 0.6774931122568204, 10365: 0.2108976396646626, 10634: 0.38293349714497055}, {7346: 0.07604861613148936, 8339: 0.18281955906817085, 5664: 0.2805237240246628, 4108: 0.21631885897426065, 11655: 0.08017833023910405, 6570: 0.11282826187124409, 5975: 0.2638629954379033, 5812: 0.06277582350980641, 11315: 0.08779646071508121, 10480: 0.08209828268423278, 4704: 0.09995127480985565, 10121: 0.20000493279692347, 4560: 0.2638629954379033, 9502: 0.21443838552751393, 479: 0.1875534087923649, 2898: 0.17510188478780628, 6058: 0.09965740427289625, 4418: 0.10800828112588086, 2532: 0.1807008406426287, 2854: 0.21443838552751393, 9084: 0.0854993725522673, 2960: 0.16134972219310373, 3754: 0.20011905275259978, 4680: 0.1713306123979059, 3490: 0.22781280440623455, 11259: 0.06467794807886063, 5758: 0.10490738777279374, 11185: 0.16265036078324768, 10365: 0.09320061887259676, 9246: 0.15468097003810807, 5549: 0.2212909536772501, 10672: 0.2212909536772501, 1062: 0.2638629954379033}, {8185: 0.47044547106032397, 10762: 0.8322469420779575, 9084: 0.2933361316999985}, {5601: 0.1617353506250663, 6186: 0.09685723542003989, 7356: 0.0948836099908364, 7970: 0.13012281742448434, 11130: 0.1879246292183302, 6371: 0.0861576579945154, 10177: 0.21771617858907114, 11327: 0.22467349151330182, 8179: 0.22467349151330182, 8649: 0.26789626724938637, 747: 0.10317314902879732, 6176: 0.23364302788999922, 8142: 0.1279466803427234, 4448: 0.07619537476347138, 7133: 0.11518827304422263, 1063: 0.14352516130318269, 9299: 0.26789626724938637, 8022: 0.22467349151330182, 310: 0.21203164002195696, 9667: 0.19610479072102885, 6058: 0.10118071525779725, 4412: 0.05878540681597077, 10600: 0.09300055375509861, 6094: 0.15983932790917502, 2944: 0.26789626724938637, 7922: 0.12691303953583372, 6076: 0.06107840585658663, 2879: 0.21771617858907114, 1982: 0.26789626724938637, 4400: 0.26789626724938637, 8803: 0.07196549730813809, 9912: 0.11449794280471842}, {2433: 0.38253300286976044, 2974: 0.31996374936745964, 10641: 0.4193289285557948, 6205: 0.2235036578699011, 8725: 0.24436844138073435, 4276: 0.3288318654865337, 10785: 0.16767409421733512, 11579: 0.45612485424182914, 10480: 0.14191860120705269, 11533: 0.26738813752768414, 4704: 0.1727800466233223}, {8892: 0.16643741806319776, 9016: 0.2802738349642757, 7033: 0.35411844282685667, 7346: 0.10206136513734097, 4418: 0.14495296796440488, 5146: 0.35411844282685667, 11298: 0.17150915031814457, 3887: 0.25922094336749824, 6128: 0.12462769734127525, 4367: 0.15045625872136703, 6076: 0.08073643651087775, 11642: 0.13941749176233828, 395: 0.1814482731874932, 4012: 0.2139433293556906, 1150: 0.29698445512531, 1808: 0.25170684111350244, 4656: 0.18469510577414586, 2722: 0.13374563856033278, 11547: 0.12841442930749147, 2477: 0.2802738349642757, 5415: 0.2552928810500742, 2171: 0.09117151555008397}, {7639: 0.36293008745835914, 5577: 0.29248229562500566, 10627: 0.3599975070449207, 11244: 0.5668529474660068, 2528: 0.29564956926519137, 6899: 0.49437508172160566}, {1818: 0.5498752988254508, 6923: 0.34411788385208053, 5995: 0.7610650680163865}, {10480: 0.2624804549617438, 5812: 0.20070367097479688, 2171: 0.21719609568609713, 1493: 0.7075001851564684, 5888: 0.5401245903782638, 8803: 0.22662042739194518}, {8803: 0.2522018940606324, 895: 0.5092269680906052, 8995: 0.5442909614045932, 6672: 0.6171138054968004}, {4624: 0.3620389663973109, 9341: 0.7413281454608319, 3878: 0.5178196786905234, 4325: 0.22632531435615758}, {458: 0.7131070636613548, 6527: 0.41622623358800404, 10785: 0.32256179649971295, 7399: 0.46280441405447165}, {10279: 0.23940849365241015, 11545: 0.2807352836983857, 8803: 0.1557097898267159, 6916: 0.34376414006840617, 1704: 0.3548480863751654, 7795: 0.39259998019054404, 6158: 0.3548480863751654, 3157: 0.3105414891482569, 11486: 0.3548480863751654, 6018: 0.1768162755717346, 6742: 0.24872816800993255}, {6896: 0.34882169034127114, 9084: 0.2285926440008626, 6234: 0.45167949560427295, 861: 0.5250661995095542, 5771: 0.29298428625483686, 11185: 0.4348648991126523, 4704: 0.2672315070625138}, {4730: 0.4298662917093962, 9897: 0.29722367253386234, 6172: 0.5559785086209268, 9848: 0.4426989157936731, 6371: 0.19449755225895152, 3796: 0.288048212248206, 6825: 0.3177696606508259}, {8969: 0.24155633086565348, 8803: 0.1013499476142486, 5183: 0.23256395402979885, 3724: 0.1877039752152966, 3051: 0.29183827495854386, 6230: 0.26140267581415216, 5149: 0.18169458595247834, 2645: 0.27617693213983957, 1455: 0.16931870255348905, 3560: 0.30661253128423127, 6570: 0.16132633571009397, 10004: 0.285975006589149, 10873: 0.2342211049586043, 2957: 0.34684620487793244, 9696: 0.3772818040223242, 2734: 0.17686452496952382}, {1455: 0.22516096607955513, 3177: 0.4207646089319339, 606: 0.4207646089319339, 8736: 0.4077350742411104, 383: 0.3566157377561319, 8195: 0.3362778246536222, 9704: 0.24657583370343944, 1204: 0.2877931073332555, 9987: 0.21233667173672935}, {6355: 0.5455777373588598, 10160: 0.418565142770728, 4376: 0.5428680511186291, 865: 0.4821228399799874}, {6501: 0.5436771583481259, 7067: 0.18557385992511036, 8118: 0.37232549182828517, 7964: 0.45595949012754866, 2546: 0.3919505208704254, 2184: 0.41210065601726}, {1452: 1.0}, {4160: 0.16067964570871596, 575: 0.29905673958863105, 4418: 0.13580028525442114, 9004: 0.2074740711186538, 2995: 0.19130718679538, 10774: 0.3968080848691493, 7356: 0.1175023772824396, 5809: 0.2090502549581996, 4325: 0.11340262960115402, 1633: 0.3317585436508451, 4362: 0.28933986823639657, 9561: 0.3317585436508451, 5984: 0.30499533922477945, 10927: 0.30499533922477945, 352: 0.2625766638103309}, {11227: 0.4944185941584111, 9205: 0.6288027461695443, 4930: 0.6001311191398593}, {1923: 0.6060944283805684, 4785: 0.4504641663330018, 4325: 0.35856096962495526, 7356: 0.548785577249947}, {8189: 0.20268692110237763, 10979: 0.4009265603819869, 9800: 0.23921136569154958, 163: 0.349664056505754, 10514: 0.2139398180523686, 3126: 0.20910849174162127, 1593: 0.20072882365420042, 3459: 0.4009265603819869, 2734: 0.18794886181531673, 3469: 0.2047308014930997, 11257: 0.2243519509605749, 1881: 0.20910849174162127, 10327: 0.3101281708248463, 11547: 0.14538851756682272, 1742: 0.198849597532333}, {2644: 0.503948827672405, 10077: 0.27532497173166826, 4316: 0.35609855930028433, 8605: 0.33781994993431663, 10160: 0.1378975436804069, 3636: 0.210096999350279, 4492: 0.3873460484016085, 6349: 0.25704636236570055, 10009: 0.3873460484016085}, {11640: 0.18763767501904544, 10160: 0.22026105741074717, 8591: 0.5687892845698934, 1150: 0.5188782795788279, 11476: 0.5687892845698934}, {1454: 0.6288612554070756, 7549: 0.5273998603281008, 1702: 0.30034278137437226, 11292: 0.4469936070925863, 11640: 0.19071926402018557}, {4695: 0.2551707182894183, 9205: 0.23236422854786254, 9294: 0.23859860183545437, 8867: 0.3389247770551947, 2038: 0.25263171119540856, 407: 0.2578399865844145, 1192: 0.44735188446475915, 3117: 0.3750200102043554, 9100: 0.4878806367832758}, {9084: 0.21100581185710307, 2734: 0.23463756373228817, 10782: 0.2762414027904026, 3708: 0.2629954176514561, 7675: 0.38853214935362856, 7067: 0.17084346954209303, 11329: 0.3354801793958132, 4619: 0.46014399165899206, 4662: 0.5005214208534737}, {3402: 0.5775012076376643, 2507: 0.5122936997734456, 10480: 0.22109780988366706, 9683: 0.5959557691759682}, {4290: 0.8450309670232542, 5771: 0.43183227732754476, 11640: 0.31534671241325823}, {3042: 0.5043023232919474, 10634: 0.3543876398477655, 9223: 0.2691649640795377, 9474: 0.5350694797958385, 1063: 0.3851547963516565, 10638: 0.33614762262413933}, {10785: 0.3029510853205242, 2974: 0.5781057925886477, 2997: 0.7576373357211007}, {6180: 0.7366980771897756, 9897: 0.6762218149874251}, {5758: 0.21488606992576387, 9365: 0.27844380938614033, 9987: 0.17581840792401016, 2171: 0.10695571546567076, 10365: 0.11278370758378965, 9084: 0.13461014998140144, 11545: 0.2012019189382613, 11315: 0.1382266839147687, 11259: 0.10182891442931236, 1384: 0.18686312531704202, 1524: 0.3091929758941799, 4771: 0.19474586304432195, 6681: 0.26177125463901213, 2882: 0.3040989424633005, 3469: 0.21213470644854018, 6076: 0.09471404834140304, 11464: 0.17004815927392022, 237: 0.2478622291811894, 9800: 0.2478622291811894, 2986: 0.33761163826064555, 1248: 0.23031625661678778, 6834: 0.17581840792401016}, {10157: 0.2463886949221107, 1380: 0.34909295926323275, 1384: 0.24525550533802437, 5812: 0.09970458871766269, 6570: 0.17920108119081568, 2171: 0.1403780333911107, 3529: 0.5452410250244089, 7122: 0.3233633881269144, 11364: 0.5012560326452423, 6058: 0.20592999471884424}, {11655: 0.17470061420591526, 6287: 0.38941074962440164, 8491: 0.42471775392045585, 6469: 0.35439897344966964, 1098: 0.4821709982028722, 8814: 0.5285511224921075}, {266: 0.5062296506899552, 11279: 0.5352527766014672, 9515: 0.22319277959915643, 407: 0.29191522629731237, 6922: 0.3767765188942811, 10127: 0.4245538181470571}, {4695: 1.0}, {1095: 0.17643020081176397, 10182: 0.30778530660225856, 3728: 0.19855421795796346, 4599: 0.14957282224208535, 6058: 0.16451109329387034, 10163: 0.2919866260374681, 11568: 0.19739101693453914, 6130: 0.1414193610074647, 6919: 0.178650234377616, 6570: 0.11003450557333647, 2112: 0.30778530660225856, 7346: 0.09649162570107102, 1646: 0.3347933294162226, 10160: 0.11918845682715037, 6355: 0.15535590987009068, 8228: 0.19626153667529467, 5720: 0.18824064712661936, 11655: 0.10173146895128093, 1818: 0.21096255759557603, 3900: 0.11934277124895959, 7133: 0.14395223135483443, 4367: 0.1422454910473666, 8519: 0.30778530660225856, 526: 0.22676123816036645, 11315: 0.11139746725372122}, {2622: 0.4446893990932167, 11536: 0.610030463837829, 11259: 0.20427147624465244, 7366: 0.34172915350038635, 8838: 0.5211607440748554}, {5029: 0.17081777666637857, 5758: 0.14141633410965235, 10610: 0.18484178803552886, 7748: 0.1295073333386095, 7133: 0.15293736207621336, 5917: 0.23603930255142633, 11259: 0.08718659866792454, 4332: 0.23380111449087634, 5097: 0.18832252598752572, 10483: 0.0824231432136049, 10402: 0.24951061685565765, 5873: 0.35569027419532423, 3768: 0.25282409641758025, 10782: 0.2554026515410585, 6128: 0.12518088435547625, 4463: 0.35569027419532423, 10564: 0.10476780792570345, 2227: 0.25642605376872063, 1393: 0.2890653462782889, 5812: 0.09607596569069733, 6186: 0.09884394884498404, 318: 0.26473309718862303, 4325: 0.08231080557281346}, {10520: 0.29435365130675234, 1358: 0.25338241555852126, 4325: 0.09305350761113663, 7896: 0.1634431251697995, 3764: 0.2825807851833486, 1804: 0.3267923842177406, 5812: 0.07353168315982479, 9991: 0.28989321122550854, 4905: 0.2067548085477492, 7688: 0.1894902324253996, 1763: 0.21630630776534962, 7489: 0.21457280591693767, 9617: 0.23708194122441442, 1384: 0.18087482575938446, 1861: 0.26191491839576414, 9701: 0.28207521385632, 1047: 0.1915253008371013, 5962: 0.23847903599068296, 6371: 0.1293228030048554, 7067: 0.1372535544888012}, {2171: 0.15973895292660237, 5788: 0.45417345341181087, 6355: 0.2879063127645391, 6896: 0.3067791753270559, 10331: 0.47028640417475026, 2656: 0.331075465987679, 4821: 0.3116272306461965, 5594: 0.4041220567837001}, {9528: 0.5481441727828683, 10480: 0.26637767363251313, 2171: 0.2204209479115683, 9987: 0.3623374399973219, 4599: 0.3824882774327578, 5771: 0.355556607706275, 6205: 0.41951078946219716}, {10365: 0.16545163649784903, 10426: 0.6094219168012018, 8493: 0.2820875157300511, 10784: 0.3394286237502498, 4127: 0.25643955091906334, 11655: 0.18518106951352975, 2702: 0.41734936150959945, 2659: 0.36360935743813877}, {3284: 0.5142131083819759, 9834: 0.5247558809461383, 8770: 0.6783923234976856}, {4821: 0.4616530495920686, 10683: 0.5741392591308674, 6923: 0.2785895502762631, 6285: 0.6161399479045901}, {2960: 1.0}, {4991: 0.5710654625273037, 4710: 0.6138598418739767, 10430: 0.5450324137543865}, {1735: 0.5025128965405181, 7340: 0.380898647555247, 8929: 0.38870810971347153, 9189: 0.40838638272134214, 9297: 0.5025128965405181, 10160: 0.17889764046028703}, {11259: 0.12793365726437206, 3469: 0.26651731466218465, 2527: 0.5694814102741188, 9168: 0.3399534241862677, 8600: 0.5219237626701462, 4624: 0.19320311146097252, 1108: 0.40372297029335436}, {8484: 0.35944063758517414, 7780: 0.3076162934592409, 6570: 0.17625241571432917, 10430: 0.20020805840887285, 4745: 0.3625755031324115, 11259: 0.1314500401634716, 10365: 0.14559148523542303, 4537: 0.22696241939348383, 10627: 0.34057444867684217, 4592: 0.5362693526652357, 4865: 0.2828458250240735}, {10861: 0.3662796295928636, 11655: 0.2502382533766868, 2479: 0.37565853687873796, 9773: 0.3965987192680794, 8178: 0.577686502559916, 4821: 0.41362842479384415}, {434: 0.47928678005370084, 10785: 0.17110250268228352, 2595: 0.4279028877888473, 1755: 0.4279028877888473, 797: 0.3528063142510027, 6923: 0.1835463484054111, 6741: 0.4654511745577696}, {4475: 0.32066259956772464, 11359: 0.27671207821242005, 5149: 0.31608714144434247, 2171: 0.2198510986281103, 213: 0.2863083271467852, 10854: 0.29389014675447095, 215: 0.3740732497685487, 6566: 0.6033951140919452}, {3000: 0.6416573445993667, 7675: 0.2942610922076907, 2015: 0.49319181474512935, 7985: 0.2709876896225374, 4266: 0.4301322676524883}, {4579: 0.4024968917054739, 10308: 0.4678926435645854, 9106: 0.5482720794054211, 1662: 0.476510182304443, 4325: 0.14547716482141992, 11359: 0.26503753297550015}, {6959: 0.3615354768565899, 6128: 0.16449039970011273, 3316: 0.278863396816842, 10747: 0.31326994340585546, 10732: 0.2992451325298156, 4958: 0.46738474227173055, 2945: 0.3322163514634757, 10785: 0.1718132931863116, 1565: 0.46738474227173055}, {8366: 0.2395479783365451, 6243: 0.45856670851245496, 10344: 0.45856670851245496, 7356: 0.17666721901309393, 322: 0.4183276852077021, 6191: 0.45856670851245496, 2856: 0.32489556648556916}, {1057: 0.2148047587103337, 7133: 0.1665920524321682, 1486: 0.38744733140376175, 8950: 0.16366082958316072, 10549: 0.30262043249952697, 10280: 0.38744733140376175, 4475: 0.18929113732175196, 6937: 0.3066526232080542, 9934: 0.2388301858422544, 4325: 0.11665002745704381, 10600: 0.13450286837308514, 6018: 0.15376729405671075, 407: 0.2234198950434238, 5518: 0.3148739370796651, 5149: 0.1865901872480277, 616: 0.21579584003866015, 9773: 0.1865901872480277}, {11655: 0.14705118642498421, 2479: 0.19443715483153093, 6114: 0.23619136979528635, 4600: 0.22436522785920315, 11023: 0.23287364436057925, 10480: 0.10193643723952739, 6758: 0.19217155568722924, 8437: 0.3276226103016309, 10359: 0.2438429010475097, 6076: 0.07469557887183727, 9256: 0.15867650119911744, 8052: 0.2662550824862638, 7896: 0.1331657800963382, 9667: 0.3120202041307949, 3345: 0.27476349898763985, 11477: 0.3011930546446354, 6925: 0.18510605685611298, 9133: 0.2857327556745703, 9211: 0.2593032000175748}, {7388: 0.5305877956105515, 11640: 0.23769121324923018, 7386: 0.44498198432603137, 4967: 0.3027938095301949, 8491: 0.30126900268338247, 7387: 0.5305877956105515}, {8803: 0.31887691845928046, 8892: 0.4288254289054973, 1455: 0.40946537057602955, 8344: 0.5548241257653072, 3157: 0.48880933109519015}, {5096: 0.5549345239640827, 1969: 0.5173056984749741, 9004: 0.5668905791756871, 7356: 0.32105694149158354}, {1095: 0.09063563033709841, 5058: 0.10409610158120376, 6186: 0.06218250791931703, 1326: 0.17853103216035637, 7133: 0.09621255362205404, 10785: 0.08225690565394018, 4412: 0.06388226586831576, 9205: 0.1162835568858727, 11586: 0.16131705891474857, 10480: 0.06962187596431216, 6076: 0.051016559614278915, 2403: 0.1951289214811345, 6570: 0.07354315155876394, 10483: 0.051852215694588547, 2960: 0.13682954110758658, 5215: 0.11670701954900811, 4448: 0.08280172669067298, 9629: 0.1993696887654989, 5054: 0.22376395874937957, 8302: 0.18766155423980327, 6128: 0.10245743315981265, 83: 0.20571275649459142, 8803: 0.06011014911247773, 1763: 0.12036810812472117, 6001: 0.22376395874937957, 9470: 0.22376395874937957, 10288: 0.076286990141894, 917: 0.09310205079780545, 6826: 0.12405702471374118, 5061: 0.13270658024797616, 4904: 0.15696657296556343, 10514: 0.11940346525286184, 7748: 0.08147277476304893, 7896: 0.09095129941389653, 932: 0.22376395874937957, 4934: 0.1590510755730309, 630: 0.15155914973022694, 3658: 0.12405702471374118, 5758: 0.08896470060585285, 2722: 0.08451255266689546, 7942: 0.15323988642072045, 10607: 0.20571275649459142, 7592: 0.12642614632878219, 8871: 0.20170729954572467, 10279: 0.09242116547877414, 7557: 0.16131705891474857, 6405: 0.22376395874937957, 11547: 0.0811438140103766, 11642: 0.0880965408822357, 10336: 0.15323988642072045, 1992: 0.1470837600405301}, {11655: 0.1340504188314944, 10501: 0.3004577168638931, 11547: 0.09985645898248585, 4248: 0.21794398694836928, 1923: 0.18232559527323247, 8646: 0.11888214807015095, 6180: 0.11332361232441626, 6130: 0.17181421921395199, 2130: 0.16531381387047103, 7489: 0.14693920141422417, 10610: 0.18617714295436383, 8546: 0.16235343040969946, 9260: 0.15714501795613461, 5072: 0.22378703470836556, 6923: 0.1085881663125416, 1841: 0.1463617437067141, 5749: 0.12274803812854744, 746: 0.21300376045894504, 747: 0.10605005669449062, 6223: 0.312452751983159, 6335: 0.21300376045894504, 5968: 0.10026128165820912, 3316: 0.16429633045363376, 1455: 0.12358050144397925, 6262: 0.27855355996988124, 6263: 0.2753663583740186, 659: 0.29115364479628003, 11227: 0.14638842124125642}, {8040: 1.0}, {11298: 0.2354062133615151, 9515: 0.21429484991901132, 2152: 0.4239018729200193, 6371: 0.15631706591078998, 1873: 0.4239018729200193, 10937: 0.3759720509212495, 1728: 0.29960959294787265, 1012: 0.36841846654560095, 4145: 0.3846920241331887}, {1158: 0.329357054899425, 8049: 0.18152448387798495, 1934: 0.2531510080452353, 1594: 0.2902642151071044, 8259: 0.18906269632096165, 2711: 0.18290352828786083, 7125: 0.25311915088995246, 3318: 0.2668483885213129, 6834: 0.15982760120492698, 517: 0.2902642151071044, 7823: 0.23589434949254465, 6143: 0.20111185250164376, 7856: 0.23589434949254465, 10495: 0.2902642151071044, 9304: 0.17214555717782037, 6715: 0.16737965451750536, 6177: 0.2160378009833663, 10569: 0.23589434949254465}, {5006: 0.30515241822282907, 9422: 0.5217686199907108, 3869: 0.3737046690434999, 11229: 0.3737046690434999, 1881: 0.23983431704759875, 6371: 0.1478875813229053, 10785: 0.16903892213336175, 4930: 0.2280680092049293, 4616: 0.2610968768340285, 4771: 0.21556554943281572, 8950: 0.19423905554712725, 6758: 0.2073158056188538}, {6844: 0.5413317199410475, 2246: 0.5413317199410475, 5827: 0.6433660994887239}, {7618: 0.17885705598689602, 7398: 0.24809380463003375, 4656: 0.16834881748242647, 1806: 0.23627880717078614, 9523: 0.12343079591664856, 7728: 0.208065987187346, 319: 0.1846512149976615, 2722: 0.09370159898542792, 3878: 0.13135487630490247, 4412: 0.05444008378109237, 10600: 0.08612610190766007, 9966: 0.16463730627631762, 6916: 0.1471357611707099, 4448: 0.0705631348072625, 404: 0.1471357611707099, 6209: 0.21637250981384762, 9734: 0.16990165286524808, 5058: 0.11541446635452376, 7133: 0.10667374055533732, 7896: 0.10084043039706683, 4168: 0.24809380463003375, 11082: 0.24809380463003375, 1072: 0.24809380463003375, 2674: 0.12756268493372297, 7544: 0.1763446923711599, 10453: 0.1266876492766812, 6076: 0.05656358800845749, 10644: 0.22807989590868988, 7848: 0.13882923854420828, 1095: 0.10049043863474462, 10011: 0.24809380463003375, 10861: 0.14356243443493422, 10660: 0.14627378770202074, 527: 0.17189375075021357, 1992: 0.16307617112108863, 10634: 0.12229833834479252}, {9084: 0.15854187063251757, 57: 0.48928233785401376, 11652: 0.48928233785401376, 8803: 0.1314368696857354, 9712: 0.48928233785401376, 11239: 0.48928233785401376}, {2245: 0.27471404052862675, 8680: 0.4742198651919034, 1133: 0.49987873960447143, 9804: 0.4742198651919034, 7380: 0.4742198651919034}, {7151: 0.24710641608786743, 2005: 0.3601700243378845, 5334: 0.4614056398756291, 11547: 0.22649763725148392, 8216: 0.6245948452988889, 10335: 0.3935745262757375}, {7277: 0.3965039958308711, 8239: 0.3657116419634124, 9729: 0.18015883753100717, 1188: 0.43464379411064386, 3727: 0.3842257232312756, 9989: 0.4123334423319075, 10044: 0.4123334423319075}, {6128: 0.27667685482085413, 8049: 0.49164148067894875, 8228: 0.46085595495276704, 7133: 0.33802467958035143, 7086: 0.5958940151246904}, {1047: 0.14136753956476092, 6448: 0.20103119578287768, 11432: 0.16852666482916664, 6371: 0.1241898106212227, 7150: 0.2489181729935304, 9986: 0.29680515020418313, 6180: 0.12214648142291923, 8752: 0.31382156406658857, 5812: 0.07061311380588879, 9897: 0.145870647689729, 9996: 0.1829565554765234, 4656: 0.15480260834106258, 2115: 0.2489181729935304, 1384: 0.13350627293130854, 11464: 0.1580655796847469, 3822: 0.12170922049224682, 9729: 0.11310052145347144, 5631: 0.2728616615988568, 11547: 0.10763083581509483, 7346: 0.08554295723152622, 11640: 0.09001422701371518, 2259: 0.23491213002297018, 3: 0.20326055974597385, 4448: 0.10982992553826121, 4599: 0.13260116039180136, 6186: 0.08248016662833943, 5771: 0.12326448037381522, 917: 0.12349248880741279, 4755: 0.29680515020418313, 9704: 0.145870647689729, 5936: 0.1884952960538442, 10365: 0.08057947452895928}, {7282: 0.3366443378910311, 4785: 0.1385063340296049, 8151: 0.36618472777618644, 523: 0.3366443378910311, 8560: 0.3193643175515327, 9704: 0.17996858672449875, 6130: 0.15467933696020467, 8607: 0.3366443378910311, 10947: 0.36618472777618644, 4946: 0.2537139789474397, 9817: 0.2568722957557843, 4418: 0.1498921171421413, 4044: 0.24802316823556517}, {6117: 0.28164860290508154, 1248: 0.36462150388055486, 4826: 0.3483991125452544, 3900: 0.2344392782280564, 11315: 0.21883136738056624, 692: 0.5735842493356397, 9773: 0.3167285263706647, 10784: 0.3663038202620538}, {1113: 0.6931355403929909, 4188: 0.7208072714978091}, {2245: 0.2939155097787908, 10466: 0.4471445249143848, 3494: 0.4110730489650744, 8923: 0.3389300970664537, 2801: 0.4471445249143848, 8072: 0.24678230674278745, 2273: 0.4110730489650744}, {10431: 0.3630583319562875, 10480: 0.14696682156622037, 3254: 0.3234784862155417, 2649: 0.316637662965249, 11019: 0.17897033255908598, 8970: 0.3630583319562875, 2617: 0.3337701507091494, 11545: 0.17583898068870443, 6076: 0.10769232385518428, 10067: 0.37385029495376104, 6371: 0.11676256604064612, 4475: 0.1773756559921336, 2497: 0.3044819694620113, 4437: 0.2153172027065195}, {4689: 0.1635778854442842, 10020: 0.1899566854866317, 11377: 0.26349017407924874, 7067: 0.1170112039094652, 6970: 0.24223422897067548, 10615: 0.26349017407924874, 5953: 0.25514554180996546, 4795: 0.22980029816477285, 1843: 0.26349017407924874, 11259: 0.06458656231846731, 7346: 0.07594116435204264, 213: 0.11493907599609324, 10854: 0.11798281331509089, 215: 0.1501724875127704, 7135: 0.26349017407924874, 1871: 0.19972233875352902, 6927: 0.20854435305619962, 3278: 0.26349017407924874, 4325: 0.07932984318060836, 3767: 0.26349017407924874, 9105: 0.21413539794826894, 9993: 0.1406019093556519, 10365: 0.07153480913738751, 1702: 0.12584233973287437, 1676: 0.26349017407924874, 9420: 0.15818404924856927}, {5192: 1.0}, {4046: 0.3045892085980018, 8803: 0.16845557378258003, 4599: 0.2801583744009314, 4818: 0.26908806636134974, 4697: 0.5469075014515099, 6186: 0.1742632518044106, 8821: 0.5764993256770871, 3822: 0.2571459952685349}, {11655: 0.1906472432549659, 9729: 0.18376291684371635, 3902: 0.214487647821562, 7133: 0.2073510819769766, 4448: 0.13715973841175175, 4382: 0.3062624731436293, 4785: 0.18240389510672492, 6076: 0.10994759453447116, 6592: 0.2126166460526246, 8129: 0.3341249216309982, 3822: 0.19775011712595258, 11359: 0.2645144636735599, 5985: 0.31136825104599647, 4325: 0.11159624020070585, 4821: 0.24221436503248567, 3596: 0.30875732189267957, 1874: 0.39191216259492995}, {7346: 0.3952302634514001, 5099: 0.7710347920409224, 5968: 0.499297895363628}, {3036: 0.430534521945562, 5337: 0.430534521945562, 9316: 0.34989039537979993, 236: 0.430534521945562, 10365: 0.11688559151040341, 8373: 0.306023271886506, 6923: 0.1697772906914628, 3268: 0.430534521945562}, {1671: 0.7011803854235312, 1638: 0.3052272427647206, 8646: 0.30271610118249914, 3329: 0.38526846345322385, 11359: 0.2956154801450939, 8950: 0.29618416301922507}, {5023: 0.4027555071551455, 1747: 0.48285593347354283, 2411: 0.5272363716298274, 3764: 0.3504198938843574, 11259: 0.15902267957144967, 1593: 0.42258448001340676}, {2436: 0.4940985014798452, 6226: 0.2548901202717548, 213: 0.18995176832773292, 10854: 0.19498194001702307, 215: 0.24817956217252615, 11678: 0.3300671341926642, 11259: 0.10673769226245725, 2659: 0.25981070789273564, 2955: 0.36519534734262854, 2228: 0.2949389210426999, 2883: 0.3095184467825953, 4745: 0.22629149651455593}, {7346: 0.09949582238954348, 8931: 0.3452168765815568, 6720: 0.14996192396401725, 3775: 0.18570060415041065, 1057: 0.19139176312859846, 1907: 0.3173679785920489, 6058: 0.1303836400940545, 2743: 0.20473605933266978, 6511: 0.17229044323154977, 11685: 0.3452168765815568, 2212: 0.3173679785920489, 9195: 0.2488755183324486, 10784: 0.19227481992850864, 8202: 0.26703513573683174, 2609: 0.28055373792806626, 4239: 0.3452168765815568}, {7346: 0.18465269728834888, 11623: 0.5206751720682338, 1717: 0.8335470871977495}, {6186: 0.13830459794273872, 6923: 0.19625925216440948, 1439: 0.49768954943186894, 8803: 0.13369531534601725, 7559: 0.3335820853423641, 4934: 0.3537569614919737, 4771: 0.23331002132653975, 2171: 0.166708006901554, 8646: 0.21486431043316806, 6076: 0.11346960748237545, 9188: 0.49768954943186894, 11315: 0.16559874529766974, 10480: 0.1548510326369798}, {2375: 0.45146264821026266, 5680: 0.4692042787693536, 11464: 0.24829280835719547, 8969: 0.38836326051505865, 11585: 0.32367739364767883, 2157: 0.5087105523744939}, {6851: 0.3904445013901369, 7346: 0.2211172827407714, 10892: 0.499714799051864, 5609: 0.42931383476188406, 9987: 0.3246989138137139, 11167: 0.5091223869231645}, {1300: 0.39642662683151153, 865: 0.24730920342291288, 4049: 0.4084888941950475, 1444: 0.5544460797772047, 5832: 0.5544460797772047}, {6018: 0.26551761036145416, 449: 0.659768476088482, 7748: 0.316921507555541, 9908: 0.6275083382685432}, {1881: 0.14517797040207567, 7773: 0.21531300823733024, 1518: 0.16926653742838985, 4601: 0.2783517007451914, 5988: 0.24276162877587254, 8668: 0.2783517007451914, 10943: 0.23344203007080583, 4509: 0.2783517007451914, 9741: 0.1952589361991251, 5871: 0.2783517007451914, 5243: 0.2783517007451914, 6923: 0.10976540835407114, 3123: 0.2783517007451914, 1638: 0.12116785338484194, 2224: 0.20717155680655372, 1384: 0.12520570517397206, 9857: 0.2783517007451914, 5175: 0.2558968654079986, 4246: 0.20375835237250117, 5030: 0.13294017286528648}, {659: 0.24615100944637375, 9201: 0.25401698252394667, 5674: 0.23972404147550455, 2143: 0.1990912472995346, 2656: 0.1616234506279163, 6056: 0.21529009736657215, 2148: 0.2217170653374413, 705: 0.19728312122850883, 11259: 0.0965923284190954, 5615: 0.19392336781442168, 8307: 0.15466638650236705, 10033: 0.2254311004270625, 6570: 0.09954734479277903, 8049: 0.18941714815093594, 11315: 0.1007804054188949, 6076: 0.06905555367729942, 2458: 0.2784509266328791, 6142: 0.2217170653374413, 4325: 0.09119053255663344, 9925: 0.26415798558443704, 4520: 0.3028848707418116, 10061: 0.15914869926090436, 9084: 0.09814360805319466, 10480: 0.09423954161389984, 2760: 0.19235641073422136, 1703: 0.23972404147550455}, {6434: 0.42706392757204037, 11240: 0.4865862445963063, 11019: 0.34619460518978235, 7283: 0.6124939768007636, 4325: 0.16251764509292757, 10600: 0.24380046572993297}, {6345: 0.4556980576414278, 2479: 0.23834667392352354, 9996: 0.3220828174610231, 6980: 0.5225056777316301, 6688: 0.4803547624514232, 9328: 0.35782666322709833}, {8307: 0.3161763297011917, 4674: 0.5031935152749348, 9631: 0.5192735088238714, 2171: 0.1594122250085659, 6220: 0.4789469199223009, 2038: 0.3498305706630588}, {9193: 0.872197324973293, 2171: 0.22313190831893756, 4821: 0.43529757384963796}, {8344: 0.481910219739451, 7848: 0.4434594639895873, 10305: 0.5258976593085346, 7223: 0.5427134564782289}, {11315: 0.39378009752443854, 8646: 0.5109295300807134, 10312: 0.7641258077601785}, {4052: 0.49441966945309346, 4745: 0.34521341196914923, 5149: 0.3199157458881658, 306: 0.4013374562178618, 7609: 0.6107037353937722}, {11511: 0.22123636531552854, 2148: 0.4391693778870079, 10430: 0.19727872263695428, 371: 0.2110680669287252, 5812: 0.07427129266918574, 642: 0.354226073733505, 269: 0.2849124331549684, 4049: 0.2750973363354919, 5720: 0.2283657517250812, 11227: 0.16595979829658092, 7940: 0.2814093550052126, 9515: 0.179071686532615, 1384: 0.23769054737531542, 3814: 0.37339237824060745}, {3808: 0.14088711286050842, 1987: 0.17800707307156957, 5494: 0.1552470736725919, 3339: 0.16364711225948608, 4989: 0.17800707307156957, 7353: 0.12652715204842496, 2209: 0.1082466563556913, 7745: 0.16364711225948608, 6613: 0.17800707307156957, 8892: 0.08366420399489025, 809: 0.17800707307156957, 11432: 0.10107283623655458, 5030: 0.08501579477339316, 10787: 0.16364711225948608, 3604: 0.12457929432751468, 2328: 0.20198109958709806, 5847: 0.1942270620003023, 6273: 0.26293803112445147, 5612: 0.1492871514474026, 5613: 0.1942270620003023, 7995: 0.1552470736725919, 5812: 0.042349783015587145, 3582: 0.1552470736725919, 4624: 0.06589376234479526, 6076: 0.040584321558639794, 10861: 0.07917258859917475, 7231: 0.17800707307156957, 10909: 0.1552470736725919, 1923: 0.09059134522620031, 6821: 0.17800707307156957, 4862: 0.07563084832998752, 4967: 0.10158439418426092, 3315: 0.18821256318431692, 11281: 0.2205152244595116, 11640: 0.05398548197188737, 7356: 0.06304661826889196, 8803: 0.06221315658292861, 11411: 0.13248707427361428, 11690: 0.1446642766205114, 9088: 0.1446642766205114, 3305: 0.169529823431237, 7280: 0.16046072035005207, 2190: 0.1446642766205114, 8552: 0.1492871514474026, 9358: 0.1446642766205114, 8801: 0.16364711225948608}, {7193: 0.6854547362081705, 6925: 0.3581688863460124, 10224: 0.633929691261929}, {6527: 0.13482191550946093, 10785: 0.17685550888608378, 4624: 0.13688507472609446, 5812: 0.06762007874820788, 6715: 0.21323486762178814, 10480: 0.11505477190866872, 11296: 0.22199710734966818, 10564: 0.1089193192617378, 6170: 0.3697847964982651, 3878: 0.1957849623689595, 3838: 0.2860391250397125, 1403: 0.24539255580443706, 10160: 0.13164563143961622, 4412: 0.08114316007350517, 3900: 0.1318160742831608, 4367: 0.1571125089363787, 6076: 0.08430825151845435, 4279: 0.3697847964982651, 11461: 0.3697847964982651, 8546: 0.21802165877976204, 2757: 0.3101231763728179}, {2297: 0.693936062308521, 5572: 0.5618037334042791, 237: 0.4503657475438802}, {4599: 0.11807348781705766, 3724: 0.17106904066227444, 2623: 0.19346305487150414, 4624: 0.0978325888986131, 4695: 0.15082253495771827, 9319: 0.22164690549882293, 1283: 0.14652364862146927, 10480: 0.08223033974659276, 6076: 0.07839436394428856, 9713: 0.1345011351592255, 5919: 0.14458035195354674, 4325: 0.061159138910081144, 3900: 0.09420974369468758, 9693: 0.12874276398468545, 10564: 0.07784529471728487, 4309: 0.22817924678157125, 4537: 0.11185295819872253, 5812: 0.07138702252646054, 6570: 0.08686175508703946, 9800: 0.15768612562814413, 11642: 0.10405075110241482, 4412: 0.05799350613856426, 10600: 0.09174773940019591, 7688: 0.12454188718428921, 8667: 0.18311362872618483, 10925: 0.2642874254126088, 345: 0.24296716545571587, 3227: 0.1452145730477349, 2508: 0.23049561287530665, 2174: 0.18785509296152078, 6017: 0.2642874254126088, 9084: 0.08563690034948446, 6527: 0.09635803655697044, 4710: 0.14458035195354674, 6130: 0.11163710725458702, 4630: 0.24296716545571587, 9729: 0.10070932262192334, 5590: 0.22164690549882293, 10785: 0.09715356279532213, 4828: 0.23049561287530665, 7970: 0.12836992750084944}, {10157: 0.4626342869531147, 1063: 0.5484877670211217, 11259: 0.2509477520401825, 6371: 0.32925530977357154, 11528: 0.4863060121067727, 10365: 0.2779448061832826}, {7235: 0.28914935332970415, 7911: 0.33355083640030514, 10157: 0.2148707233911253, 6527: 0.17336300893715356, 9693: 0.23162814167632945, 3867: 0.33355083640030514, 1057: 0.26361870284114447, 4818: 0.20403830893546615, 1047: 0.22647656067567618, 9403: 0.589042070198503, 11315: 0.23370051109789058}, {5577: 0.29729139868985344, 7346: 0.16606036871991375, 3408: 0.5296930291057937, 8153: 0.4217690541137135, 2171: 0.14834188694837538, 11167: 0.3823538813795047, 1808: 0.40954312915946117, 10610: 0.29942036911888464}, {6923: 0.23528439009367333, 5735: 0.5966525440338325, 9084: 0.1933329759330048, 806: 0.3789226630722781, 6862: 0.5966525440338325, 9729: 0.22736031975976037}, {4656: 0.34972005814768103, 6715: 0.3866543228175276, 3126: 0.34972005814768103, 11568: 0.3953341110449301, 6622: 0.6705230325269961}, {860: 0.24119869479031128, 3900: 0.11552022470659971, 9407: 0.32406990586595635, 8871: 0.22453439217330726, 9144: 0.25067734856436974, 1248: 0.17966766652542607, 6834: 0.1371543787136208, 10610: 0.1684096113796871, 4984: 0.2826343003281338, 7067: 0.11061510014478979, 4127: 0.13636585563390274, 4865: 0.17092496417066338, 9659: 0.29792694947489384, 10480: 0.10083104945080207, 6128: 0.11405247866580445, 9604: 0.21301652212960617, 1535: 0.29792694947489384, 2856: 0.21108192815600824, 10638: 0.15152867224784497, 3372: 0.1957892790092482, 9693: 0.15786470105531386, 6205: 0.15879601537388324, 3139: 0.29792694947489384}, {11292: 0.6126175527424561, 9385: 0.6414742282387439, 7336: 0.46174727782343417}, {1598: 1.0}, {11640: 0.3120466733177488, 5812: 0.18815096669340792, 9223: 0.3852338430023866, 4367: 0.4371611359117174, 9386: 0.4397550693649493, 10483: 0.2384278679836757, 6186: 0.2859288188648818, 7133: 0.4424064377207603}, {5149: 0.5327151605119609, 5617: 0.7340594981418617, 10480: 0.34417119072434643, 4412: 0.24272907206146369}, {7272: 0.4297712443514219, 7264: 0.5924829065468682, 1976: 0.543004393688353, 892: 0.4115907086093534}, {1911: 0.6881283981948164, 5097: 0.36433405002419184, 4152: 0.2983039710048367, 6128: 0.24217845604864952, 8155: 0.4960890483566874}, {11575: 0.3051490407636753, 6720: 0.1997508616532995, 9811: 0.40103829023119814, 2214: 0.422737489018472, 11640: 0.13945670520638692, 6076: 0.10483847806622766, 4872: 0.24635481663627698, 5812: 0.08408650416773046, 1624: 0.422737489018472, 6733: 0.4598325143585965, 7133: 0.19771575678298578}, {3608: 0.48658415651647363, 641: 0.3622634866613516, 6355: 0.23317682610463236, 11227: 0.2053255242834312, 1455: 0.22551398323375957, 7114: 0.21349913342759455, 9777: 0.29626816816205814, 388: 0.3142506010822017, 9177: 0.5024980769611959}, {11259: 0.05632351122131903, 9934: 0.18427899201894044, 1566: 0.11128861190931004, 11464: 0.09405687432402073, 6355: 0.10662596102174006, 7193: 0.21883703726112197, 11568: 0.13547606200049553, 3126: 0.11984469580730793, 6210: 0.1293285639234496, 6834: 0.09724850870816953, 10160: 0.0818030274002454, 10782: 0.12681717600396522, 6570: 0.07552036425340891, 371: 0.11940984832130985, 11555: 0.14014388653332083, 4412: 0.05042139320726398, 3469: 0.15265732357835743, 8142: 0.10974237023348965, 9713: 0.11693955192988374, 5962: 0.13627440579757288, 3790: 0.13046955150376624, 6186: 0.06385428892729703, 11020: 0.19270684804997218, 9629: 0.15735975132091995, 2413: 0.16118663009018153, 1477: 0.20040019483503602, 1991: 0.20040019483503602, 2209: 0.13972985455217282, 1593: 0.11504212292019611, 10577: 0.211243358120584, 9117: 0.2297798681911958, 6583: 0.2297798681911958, 10352: 0.27483394529966826, 2092: 0.20040019483503602, 4583: 0.1318069567340218, 5812: 0.04201831153951833, 1283: 0.12739230636709678, 1082: 0.14164084812271646, 6128: 0.08086824182171236, 2091: 0.2297798681911958, 7133: 0.0987992347524341, 10483: 0.053246266084649545, 9729: 0.08755987857372398, 9592: 0.16332717469381244, 4680: 0.11467827334622042, 5577: 0.11856080776752498, 10785: 0.08446838822755862, 11547: 0.08332537103849813}, {11640: 0.18884150347037157, 8803: 0.12856657801164093, 10480: 0.14891073270727354, 6058: 0.1807596530010247, 11568: 0.282176605249551, 6130: 0.20216338020529526, 6371: 0.1539209158858897, 6018: 0.14599379725783215, 10533: 0.2774666963192243, 899: 0.4013798702746712, 7640: 0.2759812552712257, 3236: 0.3090153423646861, 1343: 0.4174039740665936, 1461: 0.3787951644535697}, {7356: 0.36564247456785387, 4325: 0.23890044792012685, 10600: 0.3583859490008767, 8491: 0.5861777041792597, 11547: 0.3743668475308728, 7133: 0.44388830906786525}, {4367: 0.5392028699094811, 5158: 0.8421759110075396}, {2883: 0.5487895635696722, 11248: 0.5287386462952843, 1066: 0.6475071110272372}, {3487: 0.16757527908608102, 8892: 0.10182067833655727, 9151: 0.16420855539811546, 8072: 0.1195637874558608, 10711: 0.1991611607169211, 3074: 0.21663746337632395, 8048: 0.1760587452367018, 8628: 0.2591146440639694, 4870: 0.1326226737672754, 8853: 0.19528327064956919, 8597: 0.1991611607169211, 2023: 0.1423993964761794, 2645: 0.158582442577299, 2324: 0.1991611607169211, 8641: 0.21663746337632395, 7433: 0.21663746337632395, 3566: 0.21663746337632395, 4537: 0.09168631877766663, 9752: 0.14673225273871265, 1236: 0.15196745888921495, 10546: 0.21663746337632395, 1021: 0.16420855539811546, 7914: 0.21663746337632395, 367: 0.18893817900991705, 337: 0.15398557369111143, 11315: 0.09378197872966249, 5812: 0.03961504764944482, 685: 0.14520372565357043, 9096: 0.1991611607169211, 9264: 0.1816848580575183, 5322: 0.1991611607169211, 10365: 0.05881479128694989, 5334: 0.12300726303012803, 9924: 0.1365092710317086}, {9210: 0.30134257173088436, 6232: 0.36971327856308867, 7356: 0.23273159974130478, 7346: 0.18938415677078219, 4418: 0.2689734315469908, 5097: 0.2674076260301482, 5557: 0.6575765589293224, 4930: 0.3259047615059495}, {10533: 0.42567438015650033, 5149: 0.35360076164506726, 9060: 0.5811270288436454, 3052: 0.5967069630813474}, {6018: 0.2920416502239167, 1082: 0.5901431194607651, 2479: 0.43671578251561904, 8124: 0.612961743006347}, {9304: 0.3688503432536588, 6500: 0.4780359534045397, 1566: 0.23152575600255842, 5149: 0.2302166277255626, 697: 0.43947244331920626, 9826: 0.3273725385184963, 2967: 0.4780359534045397}, {3435: 0.3932030427280009, 4285: 0.33002910936999946, 9685: 0.450848580286017, 650: 0.3250280670493894, 6341: 0.31626253382989, 2479: 0.20565950598054097, 968: 0.450848580286017, 4973: 0.2863253436067579}, {11711: 0.5345756088831394, 10677: 0.5206179154979804, 3975: 0.46142608114641015, 417: 0.41448964424833695, 10785: 0.24180614321913355}, {2673: 0.4217098594565404, 827: 0.693213567564342, 747: 0.2669728378429429, 8366: 0.2558826379409071, 1696: 0.25042758696575057, 9450: 0.273960201569968, 7067: 0.18186765143729425, 2171: 0.1371798655909723, 4325: 0.12330043436959312}, {7172: 0.2676983018091089, 5319: 0.2821828006898086, 7480: 0.22468857695752018, 7067: 0.10476957124038487, 9693: 0.14952223540825324, 3900: 0.10941548121604666, 11315: 0.10213109146249162, 8309: 0.30694421507593217, 551: 0.2821828006898086, 1011: 0.2374301365219402, 6143: 0.21266872213581664, 11640: 0.0930891739492537, 8803: 0.08245502376945177, 11359: 0.12940673099779149, 1908: 0.28013269658842505, 2628: 0.3993436308093241, 10767: 0.16444506088933894, 4046: 0.14908922198847344, 9615: 0.18544266369069695, 10480: 0.09550256524516798, 4760: 0.30694421507593217, 9084: 0.09945895503076124}, {11692: 0.6846375287316214, 7468: 0.5797902251510422, 3469: 0.4417179519462192}, {10365: 0.2641782202554968, 7675: 0.5805785602942714, 7518: 0.7701547917589485}, {7356: 0.2351468872414144, 7346: 0.19134958470182253, 3142: 0.663918387520997, 747: 0.25569057546776824, 3416: 0.539558746787356, 10861: 0.2952924086195913, 4325: 0.1536383232155719}, {9523: 0.7312055688500785, 6742: 0.48474271587150525, 4862: 0.4799613687497675}, {6186: 0.08042935560786056, 10684: 0.20572305532730123, 10664: 0.18530581858679226, 10596: 0.2427289986351314, 11640: 0.08777608690918022, 11618: 0.14536896132758245, 7429: 0.159026753943524, 9777: 0.1706424509620119, 1955: 0.32840515456731934, 2530: 0.3157977080417875, 3886: 0.26765186578045974, 404: 0.1716480259708507, 3902: 0.1287283129439286, 6018: 0.08828775641161049, 329: 0.26607714932702, 8491: 0.16433636847545569, 8853: 0.20053060356120384, 7005: 0.20053060356120384, 8871: 0.20053060356120384, 7346: 0.08341599208843582, 10861: 0.1287283129439286, 3272: 0.2894253000189086, 3153: 0.23521257296374865, 7896: 0.11764006709167847, 747: 0.11146448555685738, 1380: 0.18530581858679226}, {4052: 0.4532414479740596, 2107: 0.5598406827528136, 2128: 0.5598406827528136, 11590: 0.2639872315974424, 2674: 0.3131130007866538}, {2357: 0.526064957997939, 3636: 0.30421640199895345, 9515: 0.24728311766500782, 11469: 0.6708420878300152, 9934: 0.34573110018506836}, {8480: 0.47990480228567567, 3900: 0.18608155689013745, 10584: 0.522016233762851, 750: 0.522016233762851, 11359: 0.2200804283681265, 7312: 0.3763346162046326}, {274: 0.44183981810312745, 2638: 0.55093580691457, 10300: 0.4100504092965852, 4821: 0.27671722549518724, 10753: 0.506491412314104}, {213: 0.21418638599717568, 215: 0.2648614132182993, 7114: 0.20861728948636685, 11669: 0.29664620038524714, 10570: 0.49100802005507016, 2690: 0.29870004399508354, 938: 0.3773994617276168, 4624: 0.13970383317372276, 2693: 0.3773994617276168, 8683: 0.28606418969495373, 4821: 0.18955553558499597}, {1997: 0.8359125959130508, 6919: 0.5488625802456428}, {1859: 0.4345781844037248, 5247: 0.4634294899054606, 11528: 0.2904193316868451, 9331: 0.3717908340245127, 2961: 0.6113943874818041}, {6758: 0.31211412848874015, 9148: 0.515254446726411, 9929: 0.30010625808291475, 1251: 0.515254446726411, 4818: 0.2970655011707457, 4173: 0.4396575973740012}, {3791: 0.5678156840913412, 10736: 0.5220095774499511, 10164: 0.4952147227607036, 4789: 0.3177403771666642, 9912: 0.242682469555243}, {7356: 0.2311050246703174, 8646: 0.28170244287590485, 4930: 0.32362699363441455, 1428: 0.45772210896968224, 6180: 0.2685309690776479, 8803: 0.17528409840661782, 2413: 0.45772210896968224, 3157: 0.3495797213763971, 7399: 0.34415306777090043}, {4325: 0.19058972649384384, 10811: 0.4012000268999559, 8954: 0.6907164464714906, 9398: 0.5706355098872423}, {10365: 0.10869714130957733, 2381: 0.30347809045631924, 1236: 0.28085500248988504, 4348: 0.2683547716600552, 6018: 0.1221318997426798, 4785: 0.15143788345993156, 3716: 0.3253787336660477, 2383: 0.4003733151866099, 1131: 0.3680749069431797, 9513: 0.3813062945137862, 11080: 0.4003733151866099}, {6186: 0.06829121637400291, 6076: 0.08276065700024762, 5773: 0.15368393228052388, 4367: 0.10441153095854912, 6205: 0.1204169571871641, 8892: 0.11550191249238223, 6876: 0.12114106525684129, 5812: 0.04493796193208937, 4412: 0.07015795577970713, 10600: 0.08531111601940639, 9576: 0.12679902811144417, 10638: 0.11490604216821891, 4818: 0.10545167253145384, 4325: 0.056868479041716664, 3878: 0.13011190387063884, 1393: 0.19971504922244376, 9084: 0.10359970051603883, 6180: 0.10113378928734645, 9466: 0.22592164315554375, 5601: 0.11403488626288404, 7334: 0.19316314368385204, 6834: 0.1040058398864992, 11590: 0.13860012258633428, 9830: 0.24574616616436368, 6128: 0.08648738703080135, 11065: 0.22592164315554375, 180: 0.24574616616436368, 7356: 0.08703847808889607, 7138: 0.20609712014672385, 6225: 0.19450051759188045, 10674: 0.13113346750132954, 6255: 0.24574616616436368, 7490: 0.16153302943430214, 4924: 0.22592164315554375, 4: 0.14170850642548224, 6592: 0.10834760434776095, 5569: 0.18627259713790392, 8546: 0.18850577031795157, 8200: 0.22592164315554375, 9294: 0.13113346750132954, 6058: 0.09281492840370573, 4019: 0.14488964201147997, 2582: 0.15044264790046902}, {}, {11464: 0.3394553396002922, 11019: 0.4087979556533594, 9435: 0.6285883052354666, 9025: 0.5679181687458926}, {7346: 0.10500794231476551, 2171: 0.09380369577410487, 8803: 0.09787392153675982, 4862: 0.1548000275662427, 1257: 0.2215575490400873, 4376: 0.16822787016690596, 4367: 0.1548000275662427, 10430: 0.13602160560053175, 5386: 0.22304553748356187, 3850: 0.31775726194739756, 9200: 0.3643420697628144, 9093: 0.33495032844765976, 4182: 0.2883655206322429, 8142: 0.17400898793395123, 9575: 0.33495032844765976, 4624: 0.13487031353809797, 4475: 0.17800283850114712, 11195: 0.3055585871325051, 5431: 0.24951175203007836}, {7399: 0.30668494137649055, 9848: 0.4256446690708162, 6143: 0.40287454342784446, 8072: 0.32091628756251983, 4169: 0.4497819749262402, 4176: 0.5071212636942378}, {9001: 0.6559841699993757, 6259: 0.754774647633471}, {6081: 0.7009320448153094, 10160: 0.27143273889352937, 7938: 0.5674676112521453, 6592: 0.3361533087559461}, {371: 0.19683694321116985, 7133: 0.12517936282909747, 1626: 0.26764690068373304, 10019: 0.16530588892478443, 1777: 0.26764690068373304, 8265: 0.2911327963541665, 2991: 0.25390853242068007, 6394: 0.2911327963541665, 2479: 0.13280340604587754, 2995: 0.16788051828743306, 6758: 0.13125596886310975, 5770: 0.2911327963541665, 4325: 0.06737146538737807, 6128: 0.10246066186359734, 4138: 0.2911327963541665, 7151: 0.11517991615555369, 6592: 0.12835822232498661, 11315: 0.09687007863895652, 1464: 0.2911327963541665, 6027: 0.2911327963541665, 4851: 0.2911327963541665}, {6896: 0.14106476890776473, 1384: 0.166959232602183, 3227: 0.20394539454708205, 2528: 0.19359183749169756, 7366: 0.15220616685517602, 5867: 0.16451369643019362, 11545: 0.13817568957776935, 7490: 0.18752858553186805, 6916: 0.16919799492355034, 4412: 0.0626030881000028, 10600: 0.09904025804066963, 6451: 0.23838477093284102, 571: 0.3412332067013921, 10494: 0.17348827230482072, 11642: 0.11232116786614416, 6186: 0.10314740291082114, 4367: 0.12121450815628451, 9068: 0.31129014563323365, 7151: 0.14684725299247908, 6058: 0.10775166107354898, 10586: 0.16919799492355034, 3604: 0.15346671212073917, 10365: 0.07745436351086472, 6448: 0.25140402349691676, 10483: 0.06611044389414614, 11257: 0.2712336852985756, 11578: 0.23926438796237484, 407: 0.16451369643019362, 4325: 0.06602033945249075, 8493: 0.132056167274696, 3195: 0.21624949886070044, 5812: 0.05216984085126131}, {3535: 0.3817780535955626, 8225: 0.40243512819263594, 2699: 0.3557532174130726, 6355: 0.20313079982355062, 7748: 0.1593848972454919, 2528: 0.22831352402990332, 11500: 0.33861109604988404, 9996: 0.26983689986144355, 6851: 0.2227789975996717, 7151: 0.1731850591389811, 1068: 0.2566156830003345, 10122: 0.27583754678560407, 3902: 0.1946983995154781}, {4046: 0.2239439229844606, 7970: 0.2239439229844606, 8274: 0.4021042372066631, 10634: 0.22727785564849948, 11232: 0.27854969689756237, 3008: 0.3649106144731493, 3781: 0.3431537399080739, 959: 0.4610547345052524, 10861: 0.20506430551784716, 4325: 0.10669334913961456, 2023: 0.30305892117091837}, {2171: 0.3243383441994053, 8950: 0.5321324471691657, 9623: 0.7820739716623634}, {8192: 0.6895023860364609, 496: 0.6895023860364609, 6371: 0.2217496771137553}, {7768: 0.6757681968093113, 9412: 0.3713760900355955, 7757: 0.5649881677510273, 371: 0.2936077557427632}, {3822: 0.4264366527997659, 1520: 0.5206279957050551, 1555: 0.5253996205986508, 1109: 0.5206279957050551}, {11655: 0.18496172692513932, 7312: 0.438827171993961, 9897: 0.2991574557987955, 7748: 0.22162855921309518, 6720: 0.26441880439199106, 8844: 0.5308716284908707, 11259: 0.14920421683720808, 8055: 0.5104915108982016}, {412: 0.4748416364503495, 9387: 0.6223043549153237, 11548: 0.6223043549153237}, {6856: 0.37227043115955893, 10610: 0.24442898414190334, 10785: 0.224954182847034, 6052: 0.2540574669886277, 9523: 0.23400877455126906, 2174: 0.33432665692492386, 11023: 0.33432665692492386, 10460: 0.4703536646919203, 8056: 0.4324098904572852}, {11590: 0.48888186671455464, 5905: 0.8723499987950321}, {3009: 0.3757797825557081, 6018: 0.11462976424147725, 6181: 0.3757797825557081, 8368: 0.3757797825557081, 1956: 0.23865058040656972, 11329: 0.25187067648402267, 5149: 0.18097122965604695, 3857: 0.3454653525818855, 6128: 0.17206270468754453, 4616: 0.21336864748060022, 8344: 0.2285128578647522, 4934: 0.2671036878688166, 2734: 0.17616039794734875, 6521: 0.24700628141554812}, {4360: 0.5338344689973389, 5738: 0.4867332783500789, 10781: 0.5420483097154403, 495: 0.4292960579734551}, {10160: 0.34818151126586605, 4309: 0.6490238225077205, 3636: 0.530480012869131, 6742: 0.4196768625698241}, {898: 0.33525235291177485, 2732: 0.33525235291177485, 4624: 0.12410202857175226, 377: 0.2270721426111141, 1748: 0.33525235291177485, 5133: 0.21646189728590925, 9084: 0.10863162442713077, 8172: 0.30820730033660965, 388: 0.20965901810781118, 3280: 0.30820730033660965, 5402: 0.33525235291177485, 5472: 0.24169184290554035, 166: 0.33525235291177485, 8950: 0.1416132561771851}, {10603: 1.0}, {9868: 0.8836177895395639, 9693: 0.46820892986915047}, {11464: 0.5866963041871338, 5914: 0.8098070428522823}, {11031: 0.5633588388401, 9345: 0.6254428212369552, 1908: 0.5398593298850104}, {4187: 1.0}, {9987: 0.23285075645463243, 11564: 0.550182381582073, 2809: 0.550182381582073, 1274: 0.326293933931603, 2622: 0.293584736564078, 7489: 0.293584736564078, 6758: 0.24804736000308966}, {9331: 0.8207809035027297, 9987: 0.5712431255124588}, {}, {860: 0.4166180614942876, 1969: 0.3194416784818914, 5572: 0.4166180614942876, 2698: 0.3500608608436074, 6018: 0.22215349051605557, 4046: 0.2718871229618699, 9552: 0.469447483711305, 8185: 0.29089081883717516}, {9764: 0.3825385773100566, 5415: 0.2999813879539987, 5470: 0.4540217236152354, 6058: 0.15715754439800828, 4376: 0.19212893570159673, 1163: 0.173130371998965, 1420: 0.2162381525272319, 1455: 0.1867425235619676, 8447: 0.33816463854400247, 7786: 0.2999813879539987, 5472: 0.2999813879539987, 2357: 0.2999813879539987}, {4012: 0.8472402148370293, 4704: 0.5312099569500788}, {5520: 0.21950874032379986, 4382: 0.19177831546400484, 6592: 0.13313829082956677, 11259: 0.05689326409968027, 6570: 0.07628430712513781, 6097: 0.30197460052764175, 6018: 0.07080225573273409, 2475: 0.13245636778323766, 8779: 0.17953928671437894, 8875: 0.2133802368185573, 4359: 0.23210425703792392, 6834: 0.09823224740896, 9481: 0.23210425703792392, 9219: 0.15557052015244283, 2173: 0.2133802368185573, 10480: 0.07221687480235452, 4704: 0.08792106805743631, 2970: 0.17146189092417619, 8012: 0.13530247677830906, 7119: 0.23210425703792392, 6451: 0.12105701121364387, 4624: 0.08591918561754583, 4330: 0.17953928671437894, 8950: 0.09804268136002184, 1612: 0.16990440822197092, 1562: 0.23210425703792392, 11480: 0.14625532646938325, 6218: 0.23210425703792392, 1719: 0.23210425703792392, 9777: 0.1368464999330762, 3203: 0.16990440822197092, 4914: 0.2024273871274831, 6180: 0.09551963064306582, 2722: 0.08766256798798512, 3222: 0.1181224797137096}, {2702: 0.19709198554743984, 1643: 0.20456614149451538, 7067: 0.0982342636243955, 5945: 0.24612128641621953, 6128: 0.10128690605176933, 11188: 0.2877976743129937, 1275: 0.2877976743129937, 6186: 0.07997704930744738, 8142: 0.13745154950003569, 4994: 0.2645808253775486, 1841: 0.15296919237306913, 4117: 0.1928996669984396, 4485: 0.19940289024395919, 8539: 0.19940289024395919, 9747: 0.2413639764421036, 8960: 0.22261973917940422, 3174: 0.185821904231816, 8825: 0.20748057302058948, 11264: 0.16871188536143866, 8012: 0.1677683065469271, 9004: 0.17998196667706257, 237: 0.17171342963576855, 5771: 0.11952363613831823, 1742: 0.14274048507376327, 2441: 0.18917418294465332, 6834: 0.17991791173021085, 5386: 0.17618604130851417}, {160: 0.2506175560837686, 150: 0.22651060348074428, 129: 0.23115468924390756, 97: 0.274724508686793, 51: 0.23115468924390756, 3232: 0.25876221767018065, 8640: 0.274724508686793, 848: 0.2506175560837686, 1846: 0.2506175560837686, 7399: 0.20505974952752634, 8654: 0.23115468924390756, 10534: 0.29883146128981736, 3227: 0.16419503499526408, 8675: 0.29883146128981736, 1998: 0.24285685075873706, 1422: 0.274724508686793}, {10892: 0.4179713177426416, 985: 0.5381697747551702, 5435: 0.6417030902589475, 11359: 0.3519807026479805}, {9256: 0.24894437328277216, 6834: 0.21753770343337522, 3260: 0.5140005279827726, 924: 0.323885981174713, 1617: 0.4177222471002178, 2713: 0.29799159321645935, 7434: 0.47253572935505056, 7748: 0.187148321286771}, {316: 0.14363417795978417, 2722: 0.09873137240603062, 6431: 0.2614111400635348, 5284: 0.2614111400635348, 6180: 0.10758051516811135, 7653: 0.17026933537264266, 9444: 0.2614111400635348, 6896: 0.12925571720789106, 3634: 0.24032291215000043, 7366: 0.1071953975974975, 1921: 0.22152551266157827, 1253: 0.34010373442337705, 9445: 0.2614111400635348, 4367: 0.11106719494210937, 4680: 0.1304647722706884, 3573: 0.24032291215000043, 5586: 0.22798708961392167, 2792: 0.19456303916430853, 5957: 0.21923468423646603, 2892: 0.2614111400635348, 7067: 0.08922772190100238, 9773: 0.12589260428366386, 2051: 0.21244579119971144, 7232: 0.1811207717194671, 9729: 0.09961328580246297}, {10776: 0.46472461258854936, 4465: 0.7799264819161638, 2734: 0.41922036837479454}, {5842: 0.4296963054576444, 10785: 0.20550914880173646, 203: 0.37475529954589165, 156: 0.4296963054576444, 167: 0.39503239013677427, 7336: 0.23020937240151612, 1686: 0.39503239013677427, 8484: 0.2880088396513394}, {8398: 0.4165707764610431, 4325: 0.10485824261758057, 6210: 0.19602564098637962, 6587: 0.3434629783111492, 10221: 0.39518811624705336, 6128: 0.15947174191143268, 11524: 0.4165707764610431, 3227: 0.24897251994726557, 8950: 0.1914034613017873, 674: 0.45312467553599006}, {5914: 0.5437070135685251, 3415: 0.8392750939926643}, {9168: 0.646505810742447, 11688: 0.7629090618653389}, {8803: 0.17669971064708548, 8950: 0.2778498995475945, 1566: 0.31857883472283177, 1858: 0.5206096093449772, 11266: 0.5516496435852667, 9693: 0.3204236021545133, 1923: 0.3347554214908476}, {1163: 0.40909645224416213, 8995: 0.5700291297240088, 6511: 0.49071097574248845, 2739: 0.5166329667309123}, {7675: 0.5731541555361743, 7518: 0.7603060972127562, 8892: 0.30566149991898206}, {6283: 0.3231121070757255, 10813: 0.3231121070757255, 11529: 0.3231121070757255, 6272: 0.28179896577212604, 2754: 0.3231121070757255, 4673: 0.2970464170137827, 4536: 0.2970464170137827, 1170: 0.18534445098615726, 2074: 0.1564919423560172, 6279: 0.3231121070757255, 9623: 0.2005919022278139, 6130: 0.13648511992787174, 7289: 0.3231121070757255}, {}, {6226: 0.267709059543194, 11547: 0.24498052306963777, 2475: 0.3855280778926246, 1244: 0.5120690129502766, 2022: 0.5346878155559113, 10019: 0.38358663842356405, 4325: 0.15633317180474157}, {6018: 0.1920548059272415, 3675: 0.2982980566373528, 10128: 0.42204624909649013, 212: 0.48392034532605877, 3478: 0.3394618088573394, 11240: 0.3352880308578446, 1585: 0.48392034532605877}, {11640: 0.13994153230783704, 3822: 0.24617631075150473, 8493: 0.21358595872531644, 8803: 0.12395514841571662, 2171: 0.11880029786335701, 2717: 0.34975917816813873, 5978: 0.3377757307057382, 6559: 0.46143114143614766, 6144: 0.42420715368014467, 10160: 0.16427228635547622, 7037: 0.3749997184617412, 6018: 0.14075728767728485, 11547: 0.16732937211406196}, {5573: 0.7139655638370532, 10634: 0.35195075597669356, 11655: 0.21694806678622403, 1625: 0.5650817421754066}, {1818: 0.25728062131197577, 4087: 0.33736100658920043, 2702: 0.27961481283826534, 4390: 0.255340793218385, 1076: 0.40265029192352786, 11464: 0.16713101723596158, 9523: 0.20313562768035415, 10072: 0.4883564032828539, 4419: 0.30388883245814574, 4157: 0.3231562057681096, 11259: 0.10008205984276582}, {11640: 0.19396346701656708, 6570: 0.21019980780508468, 4537: 0.3521587618436852, 5247: 0.4847774759899365, 6226: 0.2534410770869886, 4309: 0.4244167809336106, 5590: 0.536371120866242, 2171: 0.16466103576377045}, {4710: 0.23440513415075642, 11259: 0.07110431223614727, 8349: 0.26667928486555253, 1818: 0.1827876210663245, 4376: 0.1339389285524343, 324: 0.1986551734560548, 163: 0.25299058451351725, 7748: 0.10561863872741101, 6128: 0.1020902389036931, 5029: 0.1393090304374846, 3174: 0.24367733180967557, 2702: 0.1986551734560548, 8539: 0.2614867415693295, 10312: 0.18729570618801508, 175: 0.37740313590873664, 1872: 0.3469577488323034, 5922: 0.37740313590873664, 9523: 0.14431976239859234, 10211: 0.18140945056635852, 9223: 0.10860824473079217}, {2479: 0.39286300736590857, 9729: 0.32818311259097527, 3596: 0.5514112459379921, 1818: 0.5426897771466042, 10910: 0.3733469508208066}, {6117: 0.36046667087899154, 9773: 0.3115712559765338, 1451: 0.5004464904344512, 3708: 0.33994352326165805, 10529: 0.512053417012915, 3822: 0.2652975911841562, 11359: 0.27275873785489513}, {576: 0.5170301269894829, 4325: 0.15022049364500828, 4418: 0.26571917121502314, 2535: 0.4920469470278452, 4826: 0.26431572402901143, 3270: 0.46141403069618747, 2528: 0.33857209771536795}, {4360: 0.4218810461337575, 924: 0.4186559608846106, 5809: 0.2834269424713314, 11298: 0.2178468555788379, 3001: 0.41350773038650707, 11257: 0.2516967272060007, 8820: 0.3197120606250104, 4159: 0.265193658242966, 4419: 0.33477174532455073}, {9095: 0.35204609232357237, 6065: 0.4448007572051578, 2463: 0.3730359522180769, 2363: 0.30127114723099596, 10699: 0.31616368983003196, 2141: 0.4448007572051578, 7508: 0.38792849481711283}, {11655: 0.11509578837059284, 8340: 0.4530428796885845, 7346: 0.1420303147542309, 8227: 0.34821862770148804, 1853: 0.2871065704364384, 7922: 0.17944051041868525, 6570: 0.12448958318882203, 1342: 0.34821862770148804, 4472: 0.33034449678049904, 6371: 0.12181706610729469, 2083: 0.3787746563340128, 6549: 0.34821862770148804}, {}, {11259: 0.06595434709588985, 10365: 0.09503990385069239, 9359: 0.20813353954453775, 1163: 0.16536779610051847, 6916: 0.15957615683692064, 7356: 0.09529941128840595, 6186: 0.07477282300323101, 4412: 0.09459045638699927, 11568: 0.1586413031185971, 10811: 0.17052942138265106, 10480: 0.10892023615986168, 2074: 0.1695476362688363, 7366: 0.11033612532253452, 10061: 0.14138104691326064, 10453: 0.1373991477627324, 4: 0.1551582418883151, 620: 0.2690702488838024, 7151: 0.10645138264906714, 8491: 0.15277872239732423, 3292: 0.2690702488838024, 6893: 0.21867025980044116, 4325: 0.09197434204730974, 4159: 0.1586413031185971, 2739: 0.18394098285252866, 4659: 0.23466690413347396, 4991: 0.1369352092932385, 1763: 0.14473947007286292, 10332: 0.22413287095347162, 2622: 0.18680125253036972, 10477: 0.21867025980044116, 9011: 0.23466690413347396, 940: 0.1586413031185971, 5812: 0.04920308133438356, 5609: 0.15056731393653186, 352: 0.21296081030811537, 10279: 0.11113400985793827, 8803: 0.10676760390157039}, {4862: 0.3206983774864123, 7356: 0.26733731845266195, 6197: 0.5059169709139697, 484: 0.7548054897663081}, {2704: 0.6969375300934298, 6834: 0.22671357049600782, 10781: 0.3921282068468773, 6186: 0.1488622707586576, 4727: 0.5356813696964362}, {11655: 0.2244611570595639, 11467: 0.5713984626635323, 10319: 0.6791000538777309, 5968: 0.20672725567211606, 6434: 0.34526503504231654}, {5771: 0.20649887733040576, 1696: 0.30404713247791454, 11315: 0.1654434960240127, 3343: 0.4571116238497719, 4730: 0.3534253133793988, 4246: 0.36397597593642395, 7114: 0.21125786710148559, 10803: 0.40408731621368005, 6579: 0.40408731621368005}, {5077: 0.5276291120813074, 4785: 0.10486699725429496, 6058: 0.10471297738257868, 3674: 0.24179960610781823, 2757: 0.23251693937283532, 11464: 0.11348749027914928, 4826: 0.11288808415368413, 6117: 0.11873150513136739, 7389: 0.31458854048600715, 8228: 0.16252786268406608, 371: 0.1440779751399679, 8162: 0.24179960610781823, 3915: 0.14975345719090416, 2280: 0.22531674367326648, 9344: 0.25488279005165876, 11545: 0.13427902376097098, 11039: 0.2772486407304822, 10776: 0.18744976737162763, 6226: 0.14293991717307752, 9084: 0.08983671539120297, 6521: 0.18224012827033723, 4418: 0.11348749027914928}, {6252: 0.33369068789316564, 1321: 0.5295604000455353, 8922: 0.35494388572872565, 8012: 0.30870107530240576, 9960: 0.4303675740382224, 371: 0.2751969854634655, 10747: 0.35494388572872565}, {9084: 0.21133289166902047, 3801: 0.42481026848037884, 9729: 0.24852828956749468, 4475: 0.318639977624895, 10600: 0.22641308819458986, 10666: 0.4943617658738138, 4331: 0.4943617658738138, 7366: 0.267445136457439}, {6788: 0.5903808607985372, 5575: 0.6139503641314433, 861: 0.5239421624431901}, {11227: 0.23834416817341073, 3329: 0.3205012264257147, 6128: 0.20528726975043243, 4862: 0.24783218520996286, 5096: 0.35709207436818496, 9604: 0.3834163075741251, 7978: 0.45120342036447203, 9916: 0.5087239184182888}, {7786: 0.4378054151649641, 7133: 0.20069902637775208, 6593: 0.3196578289324831, 11175: 0.3914612990360431, 10483: 0.10816352766514321, 5812: 0.0853552584405076, 4412: 0.10242513062641277, 3874: 0.25425284062384584, 8717: 0.4291160393074148, 8484: 0.312858427922622, 10912: 0.3694346880028959}, {10430: 0.35058594869563875, 5606: 0.6360451616815019, 5788: 0.6874125725346686}, {4152: 0.24480397873570325, 6896: 0.2792251860367822, 627: 0.5647144729306538, 11258: 0.5647144729306538, 7150: 0.47360261359414135}, {6742: 0.21819588671718596, 2832: 0.5084869812528425, 7067: 0.17356236211846823, 3402: 0.4132414518400316, 8707: 0.5084869812528425, 11432: 0.28872010812696736, 5030: 0.24285228725576002, 6094: 0.30338689735590485}, {8782: 0.5058349199350514, 6831: 0.32606686643931354, 2966: 0.38879574000666933, 2228: 0.2633379928719578, 10906: 0.38879574000666933, 9951: 0.3390842838483006, 5542: 0.38879574000666933}, {8307: 0.26798249451650075, 7748: 0.24859806685090663, 2582: 0.32127156171931787, 5786: 0.3388423562436219, 3764: 0.2834624252629033, 1598: 0.3449551663697419, 11298: 0.2541714792513768, 9819: 0.45769295788317843, 2425: 0.3681333022551483, 4785: 0.19849861184738093}, {9704: 0.1885630980420545, 6076: 0.06723477375970009, 5611: 0.2499036365251823, 1653: 0.3836721062374415, 9084: 0.0955558667193251, 1163: 0.15963544855151227, 895: 0.15995345921766463, 4785: 0.11154300075699883, 4325: 0.06824294787072575, 5801: 0.23340326498120456, 4504: 0.19973995150531967, 10548: 0.294898739856981, 8242: 0.23340326498120456, 6254: 0.22811261664296245, 10480: 0.09175473835511776, 4412: 0.06471065300761285, 5030: 0.14084300311220377, 4507: 0.294898739856981, 6201: 0.24731934568115033, 2713: 0.17096742229504727, 2439: 0.2571929620691199, 3484: 0.23340326498120456, 6451: 0.1538082950883705, 9205: 0.15325021323089416, 10564: 0.08686179177871466}, {11355: 0.28723967771749365, 5039: 0.35176118710902665, 10430: 0.15658902571692426, 2132: 0.3179252085058896, 9574: 0.4194331443153007, 5142: 0.28112983907763656, 10331: 0.3179252085058896, 7429: 0.23046047264157707, 7656: 0.4194331443153007, 11655: 0.12745041835944826, 6469: 0.258546872531774}, {5968: 0.35704468259372774, 11547: 0.27332425282964923, 4483: 0.7537249460267855, 11259: 0.1847526321148544, 2171: 0.19405441040744242, 4325: 0.17442059001512505, 7922: 0.35706926735635836}, {4085: 1.0}, {11655: 0.19273448255586365, 5609: 0.3549326285312777, 8010: 0.51547183823246, 3801: 0.4131362019624447, 6570: 0.20846492942241415, 6205: 0.4043608586351844, 9704: 0.4055689432250328, 8803: 0.1703878298211194}, {9295: 0.42437030586562524, 1076: 0.3688255875293107, 2386: 0.4865851204702163, 10811: 0.2370310022331018, 6321: 0.35618888683114697, 3724: 0.24208419388142363, 9223: 0.18218114371553765, 6916: 0.2885766219817156, 4152: 0.21093486920305649, 11590: 0.21093486920305649}, {6570: 0.1793143673690424, 3915: 0.2946934071346312, 10365: 0.14812077873373913, 9912: 0.23318145595950215, 7997: 0.5455857119622721, 6003: 0.457560114909048, 1001: 0.5455857119622721}, {6720: 0.3046167856764686, 7356: 0.24836442203917197, 6018: 0.213908890320933, 10795: 0.5550075058606746, 118: 0.7012370375481208}, {6720: 0.6313261310892453, 2622: 0.7755174506120962}, {678: 0.3763529108572919, 1741: 0.3763529108572919, 2551: 0.25773711564726615, 10150: 0.3763529108572919, 9987: 0.1592818362068007, 10549: 0.2259402180514966, 3761: 0.3763529108572919, 4065: 0.3058576306195644, 893: 0.3763529108572919, 388: 0.23536235038183692}, {11532: 0.6075815196294713, 6221: 0.55856747394311, 10811: 0.2959721752217035, 999: 0.4808820495215941}, {11242: 0.19840809486643407, 5023: 0.16549439974106767, 8892: 0.12529264016247366, 1699: 0.17867645792368897, 1745: 0.176903110454158, 11640: 0.08084680477106435, 2171: 0.06863312363177, 2278: 0.21098770432620853, 5919: 0.14583303493338465, 8803: 0.07161117588943627, 10365: 0.072372926613091, 3900: 0.12363167923453357, 4117: 0.2324634312777126, 4624: 0.15809165916879242, 4618: 0.26657728261053515, 11574: 0.20620515877195988, 7063: 0.1951392707112805, 4957: 0.20206232937370694, 5432: 0.176903110454158, 4046: 0.1294821590117743, 10153: 0.26657728261053515, 5236: 0.23249268873848458, 7651: 0.22356731378598302, 9084: 0.08637888144210443, 5812: 0.04874720922360102, 5607: 0.20620515877195988, 10288: 0.09088317280505376, 1696: 0.12529264016247366, 2734: 0.1249677666771043, 1283: 0.14779316884528804, 7381: 0.176903110454158, 6091: 0.26657728261053515, 776: 0.20206232937370694}, {3956: 0.21081044704204835, 6923: 0.1684086082144156, 11054: 0.2310883459030778, 8293: 0.17632910241830588, 8617: 0.2310883459030778, 3375: 0.25136624476410724, 6884: 0.20428242169339855, 6146: 0.19443898558760994, 6076: 0.05730968062371688, 10699: 0.17867073775390047, 2856: 0.16372662397133964, 5812: 0.04596566821175401, 10480: 0.10175361585274219, 7856: 0.20428242169339855, 5782: 0.25136624476410724, 10365: 0.06824328992766937, 7151: 0.09944720539504381, 11347: 0.1210550980140599, 11547: 0.09115326671740909, 5914: 0.1420213774384326, 7336: 0.13466921803863405, 7513: 0.25136624476410724, 3106: 0.3270350243355177, 9084: 0.08145005771833813, 4785: 0.09507719579813943, 7133: 0.10808080281699112, 8682: 0.15186481354422118, 6241: 0.21081044704204835, 2393: 0.25136624476410724, 8718: 0.25136624476410724}, {10876: 0.4999028329135396, 2970: 0.28384600843891833, 8474: 0.4999028329135396, 4862: 0.2123964777546445, 6186: 0.13891965462298284, 4537: 0.21157121110096316, 2030: 0.45957530566072236, 10610: 0.25978481893778915, 3900: 0.17819885939524507}, {9704: 0.25775434854185586, 6018: 0.15998288000530883, 11298: 0.2540085128473027, 10305: 0.3480341456892966, 10430: 0.1957979308975426, 9515: 0.23122888457991234, 4991: 0.2669063902328608, 9337: 0.38391144846507463, 8050: 0.40568216175329547, 1745: 0.3480341456892966, 7782: 0.35522338267683184}, {5577: 0.2548731868187751, 1969: 0.28189327248089957, 10436: 0.49396363246379327, 2722: 0.18656323269148037, 7322: 0.4014384953034087, 109: 0.4142668172487872, 90: 0.49396363246379327}, {9623: 0.29432473480915655, 5142: 0.3177684633275187, 1229: 0.4358506347594176, 1969: 0.2705554780377403, 7527: 0.4358506347594176, 6495: 0.3976049367531936, 2074: 0.22961769105412083, 9345: 0.3852925720564125}, {2480: 0.48819550464385897, 2375: 0.5244279835389715, 1109: 0.458946388746462, 11259: 0.17271392438877187, 2734: 0.330312097044029, 2739: 0.3702329948180781}, {6226: 0.2132166055700978, 6039: 0.5380519801419336, 11547: 0.19511448603504364, 8043: 0.3824465147741799, 4334: 0.5380519801419336, 2056: 0.43726858235667665}, {407: 0.5665742600695535, 9041: 0.8240106842915557}, {6514: 0.5769927559379532, 8618: 0.5197181228517205, 3900: 0.25308455498061405, 4510: 0.5769927559379532}, {3360: 0.7361769959877907, 3300: 0.6767890591450134}, {7544: 0.6465592185451705, 446: 0.7628637997141158}, {11655: 0.15917605742822208, 9515: 0.23095734921885563, 2283: 0.397064849989654, 213: 0.2285085747201215, 10854: 0.2345597811576105, 215: 0.29855556768948444, 4892: 0.4815820723354175, 271: 0.4146037582938438, 282: 0.35874091293349053, 10160: 0.18649046203967917}, {4785: 0.31825655034761935, 1109: 0.5480501408119999, 5785: 0.7735333291570444}, {8646: 0.24140018466066912, 6923: 0.22049739027547596, 5835: 0.5140471105444182, 1923: 0.28456487017026927, 3877: 0.4544183662578602, 9080: 0.30723135590819145, 7336: 0.2995664802632268, 6018: 0.17056731927046798, 2683: 0.3580011318241206}, {3336: 1.0}, {2397: 0.8964214161308701, 7346: 0.4432027128774434}, {6200: 0.5311692698038954, 518: 0.5311692698038954, 11340: 0.44546964262222505, 4367: 0.22568082149149635, 4123: 0.43167508376664887}, {9361: 0.5505763193739389, 4325: 0.13858981123656916, 3630: 0.4867102380659183, 7114: 0.25445333883207205, 7748: 0.21805638708361189, 870: 0.4383974126141962, 1082: 0.3691670948910625}, {8682: 0.40138322357594264, 9401: 0.3347722158095423, 7775: 0.6107733782743607, 1202: 0.3831053191310055, 11458: 0.4549783139871714}, {11640: 0.18360538745777258, 2001: 0.6054045722616513, 4711: 0.49200525011592827, 4418: 0.24781310136804865, 9004: 0.3786059279702052, 5968: 0.22042866345441015, 2656: 0.3230520420356489}, {11019: 0.43084669522936114, 1696: 0.41079065290127464, 10710: 0.8035061696707015}, {}, {6309: 0.3468248995997774, 2622: 0.18507044244141102, 11528: 0.16474579684806512, 10814: 0.3188462812169833, 4656: 0.18089106290351697, 3972: 0.3468248995997774, 2371: 0.274501220258267, 9025: 0.3090147880641424, 6896: 0.17148887049124426, 6745: 0.3468248995997774, 11642: 0.13654600194480068, 11680: 0.18656559907455303, 7627: 0.3468248995997774, 3209: 0.19407754575496447, 4367: 0.14735741072573902, 7346: 0.09995927473927639}, {208: 0.6634591367161198, 7259: 0.3526394317058469, 495: 0.38464015972894017, 7356: 0.23498422960267473, 11019: 0.32705351147744594, 7819: 0.3540307403405579}, {11640: 0.1732595364121597, 4599: 0.2552309378127237, 2385: 0.35215508744632273, 7845: 0.3212136816061405, 3768: 0.4060727978473309, 4865: 0.3013174051798943, 4862: 0.24272758603758895, 10586: 0.3388127501803861, 1430: 0.4982457325816721}, {3836: 0.6864738965242831, 8228: 0.4024226589408406, 9912: 0.29339658125210255, 6018: 0.20940546716304848, 4656: 0.35803943990495485, 5029: 0.32967430725809016}, {2656: 0.48018663646463605, 9084: 0.29158670266273284, 8364: 0.82728349976961}, {10160: 0.12086018702561908, 11157: 0.26260455262278226, 6076: 0.07740105717824064, 7264: 0.2847154492520325, 4412: 0.09692058716882113, 10600: 0.15333173257075844, 8452: 0.2758988573992637, 7011: 0.2758988573992637, 11012: 0.33948912071959225, 9836: 0.33948912071959225, 3632: 0.24851202166548378, 319: 0.2526749054166898, 3188: 0.2847154492520325, 10480: 0.10562858105511766, 9796: 0.33948912071959225, 4967: 0.19373835019792404, 5812: 0.06208011063355436, 10564: 0.09999579289279244, 9401: 0.17106700655343166}, {10430: 0.13635173229838304, 5611: 0.2700867454382938, 6128: 0.09879632718074352, 8124: 0.1797327994494524, 10892: 0.18284669899327433, 6291: 0.18815639016649974, 10633: 0.2580749556748342, 6875: 0.2089349170209357, 9336: 0.20549266077593314, 6371: 0.09028217170228105, 10413: 0.2089349170209357, 6165: 0.24482791723921435, 7259: 0.14920777983424086, 2722: 0.10602440828237882, 7133: 0.12070253169752762, 3328: 0.1516289774561194, 11547: 0.13244249827027374, 1881: 0.1464136663680561, 4461: 0.2580749556748342, 2123: 0.17555632765969095, 924: 0.17689003189123786, 7288: 0.228138622558592, 7051: 0.2089349170209357, 11259: 0.06881015238166935, 2171: 0.07227455109906128, 4543: 0.23788903999839298, 7970: 0.17739806555939902, 5749: 0.1251349005848797, 4656: 0.1464136663680561, 1170: 0.16102790080315837, 6143: 0.19449970339955852}, {7634: 1.0}, {7399: 0.2585784159200201, 7114: 0.2082990017128892, 8923: 0.37161025869543224, 745: 0.4507093450561285, 6826: 0.27180453614607175, 11315: 0.16312630404971964, 8484: 0.3286016000983779, 11511: 0.2670469541553062, 9000: 0.37922928352399654, 11375: 0.35343994805571927}, {7356: 0.23858581717159238, 5919: 0.3685130426809444, 6461: 0.5331557484559992, 1221: 0.5649438145816, 1276: 0.4515067878481008}, {5867: 0.3148681985872028, 6527: 0.153018437256909, 8493: 0.19426659038548028, 10664: 0.26871065324566273, 4624: 0.15536005507140518, 288: 0.3858365870789948, 4152: 0.18193735398283267, 2171: 0.1080545225932882, 4127: 0.176603480902374, 9623: 0.2605508449801367, 2603: 0.30256764096651534, 2123: 0.2624665928052394, 4046: 0.2038539430527836, 575: 0.29078800595481546, 4680: 0.20946018844654138, 11234: 0.20269090457688946, 7569: 0.28130439557419346}, {7346: 0.5796726077684724, 11640: 0.6099716843793361, 8803: 0.5402908587588117}, {6601: 0.8402378083612536, 3900: 0.2995170445385979, 3604: 0.4519844747678184}, {1387: 0.7968165009036912, 9006: 0.5832837267841355, 2171: 0.15768182506683504}, {10365: 0.4904841477071431, 4325: 0.41807754217938803, 9987: 0.7646152428334394}, {3222: 1.0}, {5786: 0.7586872930915773, 758: 0.6514549802567902}, {5968: 0.31172206850104767, 6876: 0.42203630861650776, 4213: 0.7466745383774352, 1702: 0.40889098786549966}, {1638: 0.2819367360135768, 11204: 0.6476764899240567, 3420: 0.6476764899240567, 9515: 0.2855556087442817}, {2970: 0.6665158079950096, 11352: 0.7454908971226675}, {9897: 0.36864204862316774, 10583: 0.7500813929743123, 2623: 0.549072803825348}, {6186: 0.13643277314511756, 9852: 0.411742580258742, 11662: 0.4909537817567467, 7133: 0.21109707442263498, 1871: 0.3721369795097397, 1791: 0.4281803897510442, 10483: 0.11376738921676485, 4376: 0.22668836763509823, 617: 0.2801756224157392, 8189: 0.2481998457099838}, {10430: 0.10228964976049235, 1638: 0.12935680429539625, 1805: 0.18548849111894716, 11655: 0.05636317995938173, 4325: 0.05584562294853564, 4796: 0.18548849111894716, 3488: 0.15556150656042184, 1809: 0.2492188658511542, 9291: 0.2492188658511542, 9729: 0.09195968025016296, 3442: 0.13184493241795997, 10373: 0.18548849111894716, 9879: 0.14059801428115917, 9414: 0.1705249988396845, 9290: 0.1197640832751263, 1810: 0.18548849111894716, 112: 0.18548849111894716, 50: 0.18548849111894716, 5029: 0.08907955586308909, 7133: 0.0797551200729509, 2722: 0.07005643787501349, 8546: 0.10936228017286198, 5232: 0.18548849111894716, 4821: 0.09316486599623541, 168: 0.18548849111894716, 1244: 0.14059801428115917, 5852: 0.18548849111894716, 7220: 0.18548849111894716, 9215: 0.14348065741758817, 2645: 0.1357808457188344, 3651: 0.14680842469722263, 51: 0.14348065741758817, 4543: 0.12081735343957171, 7657: 0.18548849111894716, 4878: 0.1617719169764853, 7424: 0.14680842469722263, 1255: 0.18548849111894716, 3997: 0.18548849111894716, 10767: 0.09937527641229889, 418: 0.18548849111894716, 2268: 0.18548849111894716}, {1515: 0.879977654570062, 4187: 0.385988237590511, 4418: 0.27686171259158193}, {4826: 0.43675146029458106, 8399: 0.8995822152146801}, {5812: 0.10568041722396464, 6876: 0.21897030638381498, 5096: 0.2719348111568444, 4583: 0.2548044132405659, 2318: 0.3436030248703463, 8723: 0.330610649902614, 9532: 0.4230477985099492, 8228: 0.2603988054044604, 10480: 0.13820895182817278, 9817: 0.31160012885535093, 11315: 0.14780159112859426, 1455: 0.19935157022417846, 5196: 0.38740636551739904}, {}, {358: 0.29878256486386845, 5096: 0.24131107256391696, 3900: 0.14051165674999916, 6720: 0.1712308518550245, 11381: 0.2885456779078777, 10671: 0.3941785907333015, 11577: 0.33058124015367946, 4771: 0.1847855063772875, 6936: 0.3941785907333015, 9477: 0.304908423143539, 1360: 0.25237457271513775, 6186: 0.10953959465541815, 9557: 0.31198020753653155}, {8892: 0.2641027751903282, 6186: 0.15615239069471584, 6076: 0.12811251934588488, 4978: 0.561914890202987, 3900: 0.2003041109564887, 3980: 0.39417364220067136, 10861: 0.24992409381342154, 3320: 0.561914890202987}, {1480: 0.1802157662751604, 5812: 0.07395478281569483, 3822: 0.16584091816321114, 1844: 0.1457226994164219, 8892: 0.14610152863387257, 11655: 0.09445628377032313, 3987: 0.2711056066568971, 9692: 0.2857744664503125, 10480: 0.09671813647343863, 8339: 0.21537560209362452, 1384: 0.13982427465681882, 1731: 0.2857744664503125, 3227: 0.17079928086451696, 3242: 0.31085104789840756, 4019: 0.18327487159563643, 2579: 0.2711056066568971, 10483: 0.0720326708313646, 2960: 0.14610152863387257, 6340: 0.2857744664503125, 6058: 0.11740414186405097, 4779: 0.24045218354171957, 8373: 0.22095244376070705, 6076: 0.07087178428958747, 7688: 0.14648436670053758, 8153: 0.22754844746224945, 2970: 0.17650195869219737}, {3227: 0.485079562054563, 11315: 0.2257826082570253, 4097: 0.6238253976597908, 3728: 0.4024336487729737, 8803: 0.1822844548508427, 11259: 0.16632966403222174, 2960: 0.31892927780017627}, {9420: 0.2190673511346877, 9260: 0.20824238472071527, 6609: 0.24989702010941856, 4325: 0.08444311679067844, 11480: 0.229936532850823, 3874: 0.19876574723178153, 9929: 0.15818632861214227, 1853: 0.2765932717047077, 3369: 0.36490464594729577, 9710: 0.3354675211997664, 8334: 0.36490464594729577, 9549: 0.36490464594729577, 6846: 0.36490464594729577}, {11655: 0.3244614664740381, 8803: 0.19418967220397287, 1638: 0.31467482178420364, 4271: 0.5591712702849501, 1384: 0.32516118641509256, 4325: 0.16728358304087085, 6308: 0.4628294914185135, 8646: 0.3120859538224092}, {5086: 0.31410018758213804, 7346: 0.12736043959993984, 9450: 0.22721105312221093, 8906: 0.3234768931061137, 3329: 0.24280376919723182, 2416: 0.44189768075494557, 7348: 0.5285427482254442, 4864: 0.44189768075494557}, {7630: 0.21832760904588522, 9746: 0.21832760904588522, 8163: 0.23822160168592565, 1692: 0.21832760904588522, 4789: 0.12217256195581978, 3002: 0.21832760904588522, 369: 0.21832760904588522, 10511: 0.16888265514125977, 10108: 0.19041222251140363, 961: 0.21832760904588522, 10543: 0.3050518035453574, 8123: 0.16888265514125977, 8692: 0.21832760904588522, 3852: 0.21832760904588522, 8809: 0.21832760904588522, 2183: 0.1495169204177823, 650: 0.1573978578499247, 1063: 0.11696880149538799, 3724: 0.108621618324161, 2804: 0.21832760904588522, 8646: 0.0942571754213382, 3749: 0.19041222251140363, 9571: 0.17743230695226392, 3477: 0.21832760904588522, 6753: 0.16548966520472785, 6226: 0.08651779646033315, 10449: 0.18310231315178033}, {6826: 0.43698976458655575, 5738: 0.5181012346775828, 4537: 0.33358845709502566, 7067: 0.2690393185751615, 9435: 0.5974509541591878}, {9808: 1.0}, {3209: 0.8054244073694984, 1063: 0.5926985102170345}, {5097: 0.48408174553241995, 6130: 0.261459455430601, 11655: 0.1880835430359003, 7688: 0.2916830684912399, 6964: 0.5690407457683208, 6773: 0.5030328168800219}, {11331: 0.3018558341552719, 1111: 0.4545214168534485, 11640: 0.1643648332591014, 4862: 0.29958371484675617, 5311: 0.5419624279650814, 8307: 0.27674994181159407, 865: 0.2222394880895361, 10561: 0.3177075813253839, 1696: 0.25472501934013614}, {1455: 0.22516096607955513, 3177: 0.4207646089319339, 606: 0.4207646089319339, 8736: 0.4077350742411104, 383: 0.3566157377561319, 8195: 0.3362778246536222, 9704: 0.24657583370343944, 1204: 0.2877931073332555, 9987: 0.21233667173672935}, {11454: 0.25948493859498306, 9084: 0.10913740053582419, 747: 0.16876271340191074, 1081: 0.2506833077423128, 7613: 0.2362687076698816, 10116: 0.23336371774668216, 8509: 0.18506439347608586, 11464: 0.13786935139453918, 6018: 0.10274321575778142, 4127: 0.14172814568145253, 6715: 0.1942219613985259, 4083: 0.4382041393214699, 595: 0.1797280904692268, 8430: 0.22812936388795843, 10514: 0.2338316367638738, 661: 0.28803884609904723, 269: 0.2362687076698816, 1568: 0.2937482781541854, 7334: 0.20348826566076614}, {9412: 0.42115308181579136, 9508: 0.5207022288852304, 11590: 0.2777505642668536, 6923: 0.2526603040394407, 5938: 0.6407157445569419}, {7346: 0.15167484174233323, 10785: 0.1934563344482631, 4081: 0.4589727344040307, 3372: 0.4136549606810924, 9223: 0.19703588241739667, 11528: 0.24997973154387598, 6058: 0.1987612896914293, 1557: 0.42768573371711316, 5756: 0.48380662394617435}, {10933: 0.6430265093499374, 7346: 0.18532828406184457, 4280: 0.5225801605395779, 4160: 0.31143514969248054, 5617: 0.42671824280944226}, {4716: 0.2363711039311452, 4876: 0.19888392679859182, 3769: 0.25711256244327607, 10872: 0.25711256244327607, 4178: 0.25711256244327607, 6099: 0.20895238719501874, 7263: 0.2363711039311452, 1696: 0.12084417491238925, 1301: 0.25711256244327607, 1626: 0.2363711039311452, 1300: 0.1690043501606466, 8879: 0.2156296454190143, 184: 0.25711256244327607, 6111: 0.2363711039311452, 11642: 0.10122598605787875, 6349: 0.170622236027163, 7925: 0.18821092868288783, 257: 0.25711256244327607, 5690: 0.25711256244327607, 6361: 0.25711256244327607}, {7346: 0.1278694069660018, 10514: 0.2367448921937837, 3126: 0.23139856705896988, 3372: 0.2680427271595957, 6021: 0.3726925912017175, 2931: 0.3869367568651128, 4535: 0.40787295625016085, 10365: 0.12045000494620188, 10515: 0.40787295625016085, 5589: 0.40787295625016085}, {747: 0.3496649610373102, 4624: 0.33609274457214783, 2228: 0.6149561010755196, 6262: 0.62177574399355}, {8491: 1.0}, {11200: 0.22833635305854053, 10856: 0.2526376155872839, 9920: 0.3012401406447708, 9180: 0.27693887811602735, 4785: 0.1139415829727652, 4475: 0.14717378133181677, 11296: 0.1808469154871273, 11314: 0.3012401406447708, 11582: 0.3012401406447708, 5620: 0.2526376155872839, 679: 0.3012401406447708, 1570: 0.19990569845955036, 7976: 0.24481435643804794, 6604: 0.21412102575904546, 1664: 0.27693887811602735, 7624: 0.3012401406447708}, {5096: 0.5536780000293655, 8950: 0.49704125107351155, 3790: 0.6681247391129598}, {5257: 0.5870203608189495, 1163: 0.2656753056232993, 2739: 0.33551163931724276, 6069: 0.35564153217838035, 6494: 0.44791862142580163, 7675: 0.38097726575011887}, {8803: 0.3068183797653977, 2320: 0.8011996360964625, 1384: 0.513752493869404}, {4325: 0.30000898489059497, 10664: 0.8300456229705093, 11547: 0.4701264433877486}, {940: 0.41895999026210606, 1403: 0.471557107516896, 4367: 0.30191429412310483, 9080: 0.39044122320081587, 6058: 0.2683817901704361, 3878: 0.37622897828892304, 10767: 0.3807004094298529}, {3915: 0.19979754905700522, 7639: 0.18203210005101983, 9841: 0.18357140902659097, 6012: 0.36626694502009494, 11248: 0.19470499801603747, 5734: 0.2843121471960812, 11547: 0.10310048196282011, 1125: 0.2613764782831188, 6995: 0.24795997204007972, 2654: 0.24795997204007972, 7605: 0.2843121471960812, 6424: 0.2613764782831188, 10653: 0.2613764782831188, 11642: 0.1119345440559928, 9084: 0.0921254992724946, 2199: 0.20496776896398233, 5816: 0.22502430312711724, 7336: 0.1523199528151143, 1780: 0.17525562172807677, 415: 0.23105717317203897}, {10835: 0.4411060492276311, 9678: 0.5259662065749761, 7685: 0.5259662065749761, 3231: 0.48353612790130357, 2171: 0.13541552876706678}, {7346: 0.15311801244005988, 4418: 0.2829302163474264, 7067: 0.1813381454070475, 8124: 0.3401465105816497, 5812: 0.09714938970086355, 5749: 0.2368193224403602, 4325: 0.12294144626470052, 1696: 0.24969846930029138, 6570: 0.17460857013031925, 10480: 0.16529854859708307, 6896: 0.262687330152194, 4376: 0.24530256044603085, 5720: 0.2987102097103408, 407: 0.3063533440938231, 7399: 0.2802078194197981, 8797: 0.39541187352019697}, {6186: 0.26774720556202664, 6636: 0.9634891976113308}, {10480: 0.3825398217649425, 4704: 0.465726463463336, 9515: 0.5420678233828526, 3796: 0.5855976612376081}, {11259: 0.19492393563795954, 9141: 0.6935433149109752, 3261: 0.6935433149109752}, {8803: 1.0}, {3640: 0.4448622042488562, 4203: 0.4207403456960314, 9256: 0.20377594179744885, 7160: 0.4207403456960314, 5440: 0.4207403456960314, 513: 0.36694444955589783, 2011: 0.3079893508325938}, {6186: 0.16012448522909772, 10138: 0.502534419799221, 5968: 0.2097983268228195, 9848: 0.42179477005938154, 2948: 0.5297253253518658, 2575: 0.46827793012369867}, {5786: 0.7384472225285335, 4019: 0.6743112779272603}, {4510: 0.3518853760729368, 3114: 0.4329893137042154, 8028: 0.39805974899897717, 8775: 0.39805974899897717, 5616: 0.3222652132555838, 495: 0.251025375290558, 6018: 0.13208124878734062, 4545: 0.4329893137042154}, {6742: 0.1446983291825928, 7086: 0.25559891968823295, 9983: 0.30396828527113107, 8228: 0.25718314531667, 6834: 0.21080646074242293, 1593: 0.16882692356334264, 8521: 0.24684159059805016, 7298: 0.33720716602579043, 6018: 0.10286337833074538, 6186: 0.12191642011450711, 2357: 0.2431011167852279, 8249: 0.33720716602579043, 4709: 0.2940918292942427, 6226: 0.1336268055271636, 4131: 0.3100044172466046, 6923: 0.1329745145427475, 11561: 0.2093422852214913, 9290: 0.21772405861549893}, {5271: 0.38571137328811084, 11536: 0.2823475254583222, 7353: 0.27416304719088247, 9273: 0.38571137328811084, 10480: 0.12001016401022092, 5952: 0.2345522891666956, 3004: 0.2823475254583222, 7173: 0.35459575462409715, 1802: 0.38571137328811084, 5771: 0.16018762467538736, 3680: 0.3134631441223359}, {5968: 0.22448686636383786, 4947: 0.2672748660489312, 5978: 0.4513257451249944, 9080: 0.3387679132189201, 6834: 0.3394898939362944, 1455: 0.27669903131104834, 527: 0.5557763280976191, 8803: 0.16562513121921044, 7346: 0.17769753118975815}, {10782: 0.21455986974843172, 5812: 0.07109008207757432, 530: 0.5057893777405897, 10610: 0.20202753530562093, 2613: 0.31594132512743006, 9819: 0.33905375483224165, 11315: 0.12935431346204604, 3961: 0.35739912235671173, 6437: 0.2946758965025639, 2470: 0.2292096948756209, 10512: 0.24890576698532813, 6527: 0.14174045959207615, 11259: 0.09529281136998892, 2171: 0.10009053789812848, 1593: 0.19463785338169462, 10480: 0.12095894192700306}, {9322: 0.4017479629904504, 4631: 0.5026202727711525, 7022: 0.3743618290180888, 1923: 0.2344320414434708, 6758: 0.20768037063727077, 4347: 0.46064622870260746, 10586: 0.27319317220213263, 8725: 0.24679076329457875}, {4624: 0.29675500308232006, 11547: 0.2907075317969685, 8299: 0.9096293745818275}, {495: 0.3939300304111979, 3894: 0.4425791926250646, 4495: 0.5377899136447489, 1163: 0.2827142850606295, 3254: 0.46532922272886423, 4624: 0.25152762407886825}, {3378: 0.47667783695695504, 1806: 0.41606513891854463, 11545: 0.35815073512624856, 9304: 0.337087081468617, 5123: 0.44985629079485007, 9080: 0.31230108274798185, 6834: 0.24055299148621279}, {7756: 0.4344832598772372, 9511: 0.39254776973267813, 3783: 0.573206134546879, 4381: 0.573206134546879}, {6158: 0.30613143284851696, 7805: 0.32234546849966156, 10308: 0.3721859350743121, 1704: 0.23529928892399027, 213: 0.21813581851139272, 10854: 0.22391234077468108, 215: 0.2697452543282549, 10445: 0.3352143182517922, 3487: 0.29731223847428956, 8893: 0.3533519432748112, 8118: 0.26321951334795596, 3915: 0.207607877722158}, {6186: 0.07384881333429488, 6056: 0.1888912747914879, 3179: 0.22286947813460106, 11464: 0.10877873156915703, 4826: 0.10820419566337129, 6896: 0.1313987079205265, 10533: 0.15406566793217596, 6061: 0.26574519705874367, 6018: 0.10546703138266507, 4448: 0.07558356482299924, 10226: 0.22286947813460106, 6742: 0.11403341885017385, 2475: 0.1516544504932996, 2507: 0.19158238819975149, 9585: 0.24430733759667236, 5376: 0.20556151677951864, 8702: 0.26574519705874367, 4752: 0.14666670188689054, 6834: 0.11246992312145329, 9912: 0.11357858280696276, 8757: 0.23176699371563042, 3658: 0.14733185211474986, 4386: 0.26574519705874367, 1384: 0.1195351589563117, 7339: 0.26574519705874367, 2087: 0.1701445287376802, 9935: 0.24430733759667236, 8892: 0.12490155584124393, 6481: 0.26574519705874367, 6527: 0.09688953370766035}, {11655: 0.16120017957032265, 7970: 0.2576758976980623, 3728: 0.3146221706932866, 10562: 0.44491000702659783, 4680: 0.26476231600775435, 8595: 0.419875962298137, 6019: 0.4877059891580989, 11329: 0.3555749845722759}, {9515: 0.32537649567685073, 4226: 0.6784607097685765, 6186: 0.2050839684277877, 371: 0.38351449799992404, 7468: 0.49464977214100475}, {9052: 0.31485052436094313, 2668: 0.40962997634412, 7440: 0.2894513022206562, 3915: 0.17006378966256303, 2856: 0.205076912604552, 1345: 0.23865285794008237, 2667: 0.31485052436094313, 9053: 0.31485052436094313, 11642: 0.1239575946286483, 176: 0.31485052436094313, 11533: 0.18457072559315688, 10242: 0.2745937097211018, 3920: 0.31485052436094313}, {10785: 0.10362461905129905, 2038: 0.15926761585902544, 10480: 0.08770741273432592, 8209: 0.2606840734420313, 9148: 0.2729633097791096, 3675: 0.17376298088319309, 2171: 0.0725757184907392, 11315: 0.0937949024606034, 3342: 0.24584811292529765, 659: 0.22908927410678065, 10059: 0.1953101818800777, 6403: 0.28189067894634995, 6656: 0.25915035164860445, 7943: 0.28189067894634995, 5812: 0.051547467849526366, 3192: 0.15289787009789219, 7136: 0.28189067894634995, 7911: 0.1977414686102444, 6376: 0.22908927410678065, 10401: 0.21366969705311342, 9729: 0.10741721550240114, 867: 0.20036745832980665, 6779: 0.20980554690424538, 1095: 0.11417986844374198, 10365: 0.0765303525511004, 11167: 0.18706521960649988}, {6742: 0.46461735370780843, 6825: 0.5689239525445238, 6570: 0.3558613367739474, 2656: 0.5777706810181084}, {2734: 0.11458639214590646, 1283: 0.1355156329675776, 4247: 0.24599250359124025, 2541: 0.2444320632001608, 7346: 0.07044837838548523, 229: 0.1819258547048416, 11642: 0.09623363551929426, 7366: 0.1304059872524251, 1520: 0.1592100663144646, 371: 0.12702416369788122, 8725: 0.130954237095863, 11266: 0.2049950364775229, 5464: 0.15402341886854437, 7059: 0.19346044559118225, 8878: 0.1819258547048416, 8000: 0.2444320632001608, 7911: 0.17146489317524036, 5013: 0.16220734134352266, 10692: 0.18527652311620396, 9126: 0.12237792493166502, 6371: 0.07861137566537507, 685: 0.16383337255107128, 1696: 0.11488427760535079, 5720: 0.1374341891325134, 8171: 0.2131789589525012, 8195: 0.16383337255107128, 9382: 0.2131789589525012, 7896: 0.09935207569050011, 1866: 0.2444320632001608, 9383: 0.2444320632001608, 2209: 0.14863990003870425, 6453: 0.22471354983884187, 4084: 0.1986470930371025}, {792: 0.5559140721602488, 1196: 0.5273788930625629, 10910: 0.26213567794686565, 4826: 0.3203336537412115, 4499: 0.4914288190109107}, {5762: 0.43659478824215875, 1638: 0.3104472837615109, 7133: 0.3066450329853762, 3894: 0.46452241933553606, 6592: 0.31443211108629, 9468: 0.5516590143220369}, {4046: 0.20142633330661958, 5812: 0.09866051579906909, 11359: 0.17483439099371276, 4127: 0.1745003852986059, 2139: 0.34778802428700667, 11352: 0.26336528913726504, 10747: 0.2779544613903947, 6720: 0.1801434335352961, 10640: 0.4270225120088936, 9729: 0.15802384546328563, 10480: 0.12902831795713335, 11547: 0.15038161051207757, 10638: 0.1939034633568585, 10413: 0.30864958811556126, 4947: 0.1797707395572993, 6117: 0.17759306354900833, 611: 0.38124182356746883}, {3305: 0.5421961120684982, 6337: 0.6019478929167583, 11659: 0.4868659977453104, 6592: 0.32656333307697466}, {11655: 0.1693978794573003, 9515: 0.39376833293477437, 1025: 0.44122840208977215, 6048: 0.5574802039545642, 2506: 0.5574802039545642}, {5805: 0.9549085364577129, 4325: 0.2969001296735801}, {4996: 0.6375789420161609, 6706: 0.4900570656641719, 4710: 0.26808927031027807, 4325: 0.11340482092944837, 6371: 0.15760641047672608, 6710: 0.41099054165259225, 6742: 0.27359029382159594}, {4160: 0.28917456519685714, 9459: 0.597064626247005, 9890: 0.597064626247005, 9704: 0.29343899081050745, 1204: 0.3424898446441411}, {4930: 0.22643763573578118, 6825: 0.23989115548582704, 11315: 0.1519103078978656, 2051: 0.3710331931980741, 3192: 0.24763352713389278, 4357: 0.45655039569197203, 3902: 0.20306090105968969, 563: 0.39817582353452496, 7133: 0.19630453300963346, 10170: 0.3029709969509406, 4963: 0.3613455691083876}, {10430: 0.2726144460053638, 1177: 0.5000713508063575, 895: 0.5152978367800484, 2722: 0.27579181477723486, 4935: 0.5779420578601071}, {1300: 1.0}, {9256: 0.19190911270111563, 4856: 0.34557555275523333, 3753: 0.396238661482152, 11590: 0.17176963848003207, 10365: 0.10757462634433756, 3355: 0.396238661482152, 4466: 0.5155183840300259, 8232: 0.3642737987779132, 878: 0.30034407336943575}, {10365: 0.0611723811355991, 4331: 0.1312735701334555, 575: 0.11999392151728021, 749: 0.15921580051567272, 8366: 0.08317180995647476, 9205: 0.09000015318389243, 10564: 0.05101183483597084, 4969: 0.15104322203561849, 6737: 0.13396503670838883, 5214: 0.225321372191052, 6349: 0.11492841317334707, 4159: 0.10210938632118814, 1095: 0.09126646092201648, 10289: 0.16493921692969207, 6480: 0.17318691570678135, 9644: 0.15104322203561849, 7029: 0.15104322203561849, 4877: 0.17318691570678135, 7490: 0.11383863109298145, 11573: 0.17318691570678135, 4325: 0.04007743697447296, 865: 0.07101778555940885, 9704: 0.11073855853658975, 4158: 0.17318691570678135, 5968: 0.06305760165375542, 3228: 0.12485479806703635, 6049: 0.12485479806703635, 2734: 0.10562756282752943, 4624: 0.06410946075284576, 4680: 0.08643392746176223, 11444: 0.1978826031112609, 4870: 0.10602280632617159, 2197: 0.17318691570678135, 3886: 0.1231009916534013, 1221: 0.1452446853245641, 10160: 0.0802157757696536, 1841: 0.092051691135063, 2698: 0.10830706594849594, 6018: 0.05282981213667047, 9349: 0.15921580051567272, 6069: 0.09645960348531296, 1384: 0.07790140979890316, 4695: 0.0988336452543128, 7860: 0.15104322203561849, 776: 0.1312735701334555, 6570: 0.05692029968098698, 7861: 0.17318691570678135, 360: 0.1231009916534013, 11547: 0.06280299542287571, 2478: 0.15921580051567272, 1682: 0.1452446853245641, 647: 0.12889952836445567, 3010: 0.12677587563653003, 6128: 0.060951037573400146, 3900: 0.06173541898384593, 4412: 0.03800300541533468, 10600: 0.06012207347732331, 1566: 0.08387911265500878, 4: 0.09986751590187283, 2497: 0.1452446853245641, 3947: 0.15104322203561849, 2171: 0.04458879196572133, 8892: 0.08139870621383874, 1224: 0.17318691570678135, 9987: 0.07329697511288445, 10561: 0.10152511182924612, 8142: 0.08271369799583682, 3604: 0.09316147923057169, 9223: 0.06484248911996701}, {10160: 0.2668622514330354, 2722: 0.28311366243161207, 4325: 0.17346609705269092, 2275: 0.6091914307377544, 11347: 0.3609988943050961, 11259: 0.18374159845690427, 3822: 0.3073843791498499, 8995: 0.4345804569412964}, {7354: 0.9073300034353357, 8803: 0.4204191537811208}, {10365: 0.26146642871577486, 3822: 0.3949253257099837, 6527: 0.35113520840746754, 8552: 0.8076963285691758}, {7346: 0.16060511791111018, 11640: 0.16899983366353905, 2479: 0.25419361367502474, 2742: 0.35979587468582297, 10160: 0.19838277180312114, 10861: 0.24784726958518177, 6322: 0.4859960246549591, 1891: 0.4223852773220014, 2508: 0.4859960246549591}, {6923: 0.24475364890114754, 3180: 0.6206654305468089, 9480: 0.6206654305468089, 2969: 0.41187922744156763}, {11464: 0.4880884009236467, 9987: 0.7454303009369914, 4412: 0.26165123346641816, 10480: 0.3710013630086634}, {4046: 0.09303048426795821, 6371: 0.06159787195873232, 2960: 0.09002039414917408, 4905: 0.09847958695496502, 9136: 0.20109492331882683, 10785: 0.09160264770567433, 7688: 0.09025627965960467, 2233: 0.15159067826685751, 4610: 0.1915307397202058, 4: 0.11044540585900711, 5812: 0.045567207684519294, 2174: 0.13613975337250964, 687: 0.17607981482585788, 213: 0.10869988197716245, 10854: 0.1115783972643027, 215: 0.13441752716026137, 2953: 0.1915307397202058, 11677: 0.14517796503716213, 6076: 0.04366761946613044, 7133: 0.13193464887278133, 6186: 0.07861994404474, 8142: 0.09147466884252624, 6373: 0.1915307397202058, 3332: 0.17607981482585788, 6845: 0.17480056623358753, 10747: 0.12837565835160897, 1403: 0.1877443887643134, 6355: 0.08887701672314821, 4448: 0.07087413027464985, 4049: 0.12972704014281425, 4278: 0.1311656665570245, 7968: 0.17607981482585788, 5500: 0.10768993053845663, 6812: 0.1915307397202058, 9084: 0.0620615939092643, 6861: 0.17607981482585788, 1063: 0.10261240514885674, 5872: 0.14255246660220502, 558: 0.17607981482585788, 1205: 0.14255246660220502, 4771: 0.08978697868946599, 10288: 0.0652978421304656, 9101: 0.12710154170785712, 8623: 0.18546503500535663, 1898: 0.1972240194882296, 9693: 0.09330068118494095, 4872: 0.10261240514885674, 9126: 0.09589222535176062, 10480: 0.05959283826886663, 4367: 0.08137672327483922, 9105: 0.1556548031160249, 6920: 0.11292473345726109, 3636: 0.1038865217926086, 2292: 0.1670416031612054, 3809: 0.12262839670916069, 7649: 0.17607981482585788}, {9084: 0.27298658481891275, 5251: 0.774511646850923, 6448: 0.5706224964029878}, {5029: 0.36818375025018424, 6896: 0.3790785520245777, 1638: 0.3337314052686027, 6742: 0.3289805811962879, 11421: 0.6686359232538985, 11640: 0.23251087267967155}, {10430: 0.43098957927818377, 9199: 0.7660912928827821, 10279: 0.47681454835481024}, {4979: 0.6882420485721503, 3844: 0.7254811386777811}, {7880: 0.9404560309004857, 4325: 0.33991536291098196}, {8185: 0.19673102848813712, 3900: 0.1349472422121582, 8778: 0.2512216271443936, 8372: 0.2302088008903454, 9115: 0.2869504501053046, 9987: 0.16021960839959878, 8820: 0.35008900766532536, 4624: 0.14013665203388198, 9144: 0.2928337177298184, 3160: 0.3076583681536153, 3126: 0.19744745175102987, 3236: 0.24442989311017563, 2740: 0.3076583681536153, 11545: 0.18335106683180102, 11404: 0.3785686895228665, 6570: 0.12442188931849157, 10365: 0.10277741492653986}, {10767: 0.4257711494824556, 1681: 0.48346111516961066, 1780: 0.3765338994185685, 6210: 0.2642546930975995, 6923: 0.24087912089137506, 2779: 0.5615631819043019}, {3147: 1.0}, {1057: 0.518850764307488, 6672: 0.4728229124450214, 10785: 0.2644272194924449, 10760: 0.6612946570448285}, {2171: 0.1371963197283819, 4898: 0.4122002233834503, 9548: 0.4039187855495507, 962: 0.33842381542135774, 11721: 0.6932967155631081, 8646: 0.23005813552416446}, {3523: 0.5844207236663249, 7212: 0.3757250628757423, 6058: 0.20229456224112827, 9502: 0.4352884729888623, 9699: 0.5356154873106671}, {7346: 0.2314447624756555, 8646: 0.4510530273736059, 6018: 0.24496188681098843, 4826: 0.3269739901062646, 6609: 0.5499411464552237, 6052: 0.4337524708387889, 5968: 0.2923862200932888}, {8867: 0.5604582605997883, 4325: 0.17110626756913586, 4774: 0.6448626438011086, 6349: 0.49067438689406173}, {9694: 0.2887764287296875, 11359: 0.12174724850859639, 2854: 0.2346852428186299, 8296: 0.2887764287296875, 4427: 0.2887764287296875, 10742: 0.3053325404467224, 946: 0.2518534501597065, 7407: 0.2887764287296875, 11545: 0.139862243626642, 10684: 0.20526183861914196, 4376: 0.13333690394641012, 3696: 0.2887764287296875, 7043: 0.2887764287296875, 10782: 0.1593778057072299, 2069: 0.2887764287296875, 11555: 0.13537442130829508, 7410: 0.21493047158972559, 11259: 0.07078471474478891, 6117: 0.1236682709758265}, {251: 0.8138665409077599, 2995: 0.5810518510346884}, {11026: 0.44884189196222213, 10723: 0.3981617188078819, 7907: 0.36303118281751257, 3900: 0.15132175043522575, 2534: 0.3449896570087575, 9680: 0.37022709371725376, 10253: 0.2564674410163057, 3005: 0.32176903867249596, 407: 0.2447886086566202}, {4818: 1.0}, {6180: 0.2846653333614468, 2743: 0.3153116846670038, 9906: 0.47370365748795934, 2074: 0.25749956127126195, 8444: 0.4458850548632315, 11464: 0.21762877133123293, 2900: 0.5316645992171954}, {8803: 0.4827762975875983, 10160: 0.6398021156569487, 11315: 0.5979801830232365}, {2479: 0.09404304531326106, 11655: 0.09253441183136743, 10634: 0.16281410047664527, 4187: 0.11765173551150476, 11651: 0.16754545171621035, 8892: 0.09689715511157204, 2683: 0.1319961203160312, 3187: 0.21229018811460096, 9331: 0.12536775048797516, 7281: 0.14284096353031422, 10564: 0.06072457293905501, 10932: 0.15626827598285234, 917: 0.1116002127140154, 4108: 0.19189043849250154, 8012: 0.15635744042734573, 1923: 0.136503994788125, 11511: 0.11229767250996091, 10638: 0.09639726607959914, 9948: 0.3045262605755908, 2758: 0.1534422127168367, 4139: 0.18953075345265605, 1593: 0.10321754222166576, 6128: 0.10717432939250093, 3317: 0.16317086371729544, 7741: 0.15091421298130847, 6834: 0.08725284094889867, 4785: 0.07797906244629477, 6197: 0.13818242185468327, 1865: 0.1728995147177542, 6568: 0.233927928573757, 2722: 0.10130409345238911, 8565: 0.16317086371729544, 10331: 0.2033097144243882, 9828: 0.2179816583198609, 2995: 0.11888245684801776, 4745: 0.10713633187504198, 7346: 0.09519194579919546, 747: 0.07939782872838584, 2171: 0.053078572035160636, 2951: 0.18953075345265605, 5812: 0.03769946811935923, 11547: 0.07476079009136809, 5903: 0.13818242185468327, 7194: 0.1798021024521973, 7086: 0.15626827598285234, 9223: 0.07718860677677052, 5720: 0.11591648761332365, 11227: 0.0842397634419123, 7133: 0.08864417593894529}, {4599: 0.26379896488256815, 7970: 0.28680319877772104, 6742: 0.25337509014379794, 9435: 0.5822999102900756, 3796: 0.2812389461373462, 9401: 0.29753467733555466, 4785: 0.22334004646821137, 9420: 0.35448303819143867, 3915: 0.31893679578416667}, {59: 0.23079673215498597, 11642: 0.10834607989975603, 7087: 0.22364980495553474, 2479: 0.125534346210488, 10160: 0.1274644843186018, 3236: 0.1776863676664164, 8725: 0.14743678921654466, 4599: 0.12294769052150459, 1065: 0.35804020140334314, 9443: 0.2751974993632773, 2528: 0.18674055025685243, 10480: 0.08562489809995859, 10483: 0.0637707706609638, 3864: 0.2751974993632773, 1057: 0.1985011454134839, 8872: 0.2751974993632773, 2101: 0.2751974993632773, 1696: 0.1293441846344825, 8015: 0.18639596494669464, 7858: 0.20482394833488599, 11266: 0.23079673215498597, 129: 0.21287314318061204, 6302: 0.2751974993632773}, {10480: 0.12900585035736772, 4704: 0.1570593047694135, 7542: 0.3477274642312246, 2100: 0.38117543822786704, 6316: 0.41462341222450955, 1209: 0.23909086921858394, 2171: 0.10674915594141661, 3675: 0.2555820586951856, 2204: 0.38117543822786704, 2622: 0.2212486428613004, 6826: 0.22987145554947103, 6295: 0.2725388432152264, 1082: 0.2555820586951856}, {5649: 0.3691976197085409, 10160: 0.19192635186319126, 4448: 0.1533342434912996, 2209: 0.32783433673375767, 10638: 0.25207705861472024, 2171: 0.13879947905415202, 1593: 0.2699119539260942, 3222: 0.2743637039324472, 1163: 0.22430880332397363, 10480: 0.1677385143388602, 5479: 0.426688775006494, 3438: 0.45212901650110005}, {5812: 0.072275713501891, 10288: 0.13474917363311742, 6570: 0.12990260699086634, 6923: 0.1558609737214824, 4543: 0.25744123880175457, 7133: 0.16994460090232502, 10279: 0.16324769991987553, 5096: 0.24196357045635628, 4412: 0.08672985744291475, 10600: 0.1372096444708194, 4537: 0.1672771964496312, 4579: 0.2530569859796939, 8546: 0.23303242526304718, 11538: 0.3952444378034978, 6117: 0.22021638243359434, 829: 0.2941724633967277, 3572: 0.3952444378034978, 2960: 0.18576683893309284, 9729: 0.1506117801069401, 3822: 0.16207566612596488, 917: 0.1644503785667554, 7151: 0.15636926439498444}, {6018: 0.3803452957896848, 9987: 0.5276974979142182, 449: 0.726421968846568, 4325: 0.22177450223776088}, {2171: 0.7242517898437696, 11259: 0.6895356008996898}, {729: 0.3669630996899734, 5386: 0.2674297248113219, 6925: 0.24681538258012292, 8851: 0.43684307945877543, 6349: 0.3771595439208888, 10077: 0.31050738480352286, 2143: 0.37358321583060267, 747: 0.16823853725906002, 11655: 0.17269958288549928, 10057: 0.3251334933803417}, {9304: 1.0}, {1414: 0.7071790250085873, 1384: 0.4112285144093349, 8803: 0.24558998352322683, 371: 0.4750957573934145, 4325: 0.21156208739855584}, {5634: 0.41413714174555744, 6015: 0.3908346141187617, 7259: 0.2624676019940578, 3722: 0.49380900993563054, 3728: 0.2928608582474773, 3915: 0.26672666901103337, 5168: 0.453973075840594}, {461: 0.7653935496864589, 1518: 0.35774594471325427, 8487: 0.513078305647355, 2171: 0.15146354483380786}, {1869: 0.9077452308225981, 4367: 0.41952186584113593}, {7025: 0.6255020882847306, 1384: 0.3060468518307761, 11555: 0.3189573019859288, 9623: 0.6239273018033469, 6076: 0.15512389761143913}, {5706: 0.6772728520180019, 9148: 0.5779812561108065, 8228: 0.45523527049644974}, {10430: 0.17578491220755865, 4418: 0.19273544322681746, 9713: 0.2396251899069924, 7122: 0.2792449588680274, 8892: 0.22130205397253275, 4785: 0.17809528738327737, 4117: 0.31559289582990396, 1093: 0.4708504833773713, 8344: 0.28632564756889883, 5406: 0.4708504833773713, 4032: 0.3014640884833853}, {4785: 0.34158640290049075, 4773: 0.9030902798382278, 7346: 0.26028191603570017}, {11665: 0.43026485705615225, 10835: 0.39250930854182214, 1909: 0.46802040557048236, 11635: 0.3374076672610144, 4325: 0.10830528524900468, 2823: 0.3620271806137931, 10794: 0.43026485705615225}, {2171: 0.11497338901794985, 6117: 0.19124198921293437, 9418: 0.446567080078832, 1742: 0.22148615961052834, 10160: 0.15898059031585768, 11480: 0.2813942963430318, 2475: 0.25484518963020825, 4680: 0.22287218667081649, 11464: 0.1827954035262562, 9858: 0.446567080078832, 10472: 0.38946897549895987, 8207: 0.31325913498751484}, {11640: 0.17030931273415192, 2171: 0.1445803597263297, 9080: 0.3085548481841933, 4994: 0.5162615244290151, 8307: 0.2867589827737024, 7949: 0.43438524221378244, 9412: 0.3691248295497511, 9377: 0.42565808930292526}, {1881: 0.2759081341912062, 5552: 0.4863274123791557, 7133: 0.22745697952199925, 3010: 0.3872391015887032, 11259: 0.12966877497154938, 11227: 0.2161554546047381, 6742: 0.22699922998509586, 6620: 0.4863274123791557, 5968: 0.19261053327485364, 6186: 0.1470062366912229, 4046: 0.25694753672453174, 5812: 0.09673514426849082}, {10732: 0.3527890762134353, 3900: 0.15097136353566015, 11555: 0.19854096453393846, 865: 0.17367099952771947, 3333: 0.4235212984887947, 9436: 0.4235212984887947, 9099: 0.38652679405913387, 316: 0.2327067375250857, 9245: 0.38935552549764746, 4865: 0.22337884962281127, 6570: 0.13919619235022265}, {9034: 0.6073498115476397, 6860: 0.48069865971515324, 11227: 0.2481689467024413, 2038: 0.3431513125555707, 6487: 0.4698024643880571}, {3615: 0.13313377861535683, 1844: 0.1568127249606715, 3701: 0.25711008994743567, 8074: 0.2089503778264244, 4599: 0.11486692953951946, 4325: 0.07740897051268264, 5557: 0.2089503778264244, 4905: 0.17199434270775002, 2722: 0.09710692525872704, 395: 0.13174174569284847, 6018: 0.07843016139488654, 8075: 0.25711008994743567, 4771: 0.12052967685855855, 4704: 0.09739327492312444, 8072: 0.14190092363587056, 1082: 0.1584877364920434, 10304: 0.18820911877234178, 7831: 0.18820911877234178, 6617: 0.22423597212897156, 8264: 0.25711008994743567, 3209: 0.14387460445722997, 4418: 0.10524408255601135, 5097: 0.1361286071296732, 333: 0.25711008994743567, 9395: 0.19136185431050748, 10253: 0.15533500095387767, 9912: 0.10988796774056181, 346: 0.21562757183927045, 11547: 0.09323616473130422, 9729: 0.1274675399968342, 9416: 0.25711008994743567, 10692: 0.19488631278518784, 8195: 0.17233096428312583, 7346: 0.07410234429233084}, {4367: 0.5764589084912698, 6511: 0.6771358568920638, 4325: 0.31397242292099975, 11259: 0.3325710086240552}, {7151: 0.19164178401382637, 2479: 0.16983818937373418, 1638: 0.16207304967208663, 747: 0.14338948113086478, 2171: 0.09585789718916969, 11640: 0.14690761313875395, 5431: 0.2549757946764341, 5334: 0.2114046145631407, 2342: 0.32471580885488155, 11259: 0.09126305749768719, 9912: 0.15912861789977478, 3528: 0.34228538527026686, 10480: 0.11584381563770386, 3990: 0.34228538527026686, 9457: 0.23284074617973538, 4865: 0.25548850994752187, 404: 0.22081043354874483, 6769: 0.23114123787918717, 9098: 0.3723207745366302}, {10480: 0.18400925528043932, 7227: 0.5914037626504378, 7346: 0.1704499626780973, 4448: 0.1682077611396073, 9802: 0.5914037626504378, 512: 0.45746773911652333}, {3775: 0.5398957496075156, 10672: 0.8417318929182491}, {5507: 0.5310687342455278, 4826: 0.23521137676949053, 4947: 0.2504201092569316, 7356: 0.20459931923168156, 11464: 0.23646028750326456, 11331: 0.5154526968794412, 10061: 0.486277230505724}, {8478: 0.6953174007459624, 9693: 0.4945927454356888, 4624: 0.3758447250845261, 10160: 0.3614588095598221}, {4537: 0.18432583469268413, 7896: 0.17702475677507137, 6205: 0.21341067164016478, 9576: 0.22472138795637944, 6720: 0.18919263231509675, 11528: 0.20688038180191096, 3995: 0.26662413070163743, 5578: 0.3241540748534254, 3875: 0.37984061325190704, 2166: 0.36525856375238563, 4862: 0.18504482648741968, 895: 0.2362304922817966, 9256: 0.21093759230315579, 11264: 0.2553134143854228, 758: 0.2414607022407597, 9401: 0.21946012864168568}, {7235: 0.3086483252612085, 10289: 0.3715422739380007, 10564: 0.14950047094957766, 9084: 0.164464107681864, 11545: 0.2458246578481033, 4: 0.2926818983850079, 6720: 0.2204832889122954, 1455: 0.22778533775261828, 6527: 0.18505385453753148, 8133: 0.3659121686439356, 5517: 0.3847236748352595, 6834: 0.21481156938955556, 10480: 0.15792187007713562, 6076: 0.11571981345803423, 11454: 0.2647241333708908}, {4325: 0.18787157234431343, 879: 0.5736671553569589, 3316: 0.37231101148457435, 7067: 0.2129926478669702, 3222: 0.31756918040490545, 11232: 0.3769980491412051, 3004: 0.45678353347309103}, {10483: 0.2200937835016117, 2171: 0.24453510948255752, 11655: 0.375488311726504, 9515: 0.5448167679245511, 7067: 0.32419499657594864, 8892: 0.44640907856489803, 4826: 0.3867312537270348}, {4865: 0.2596517492739489, 7137: 0.3235929453814577, 4325: 0.11392245101152673, 4367: 0.2091636302060797, 8948: 0.4922939042486677, 8311: 0.7271777895346333}, {7499: 0.8015488024906837, 6226: 0.3642005499030376, 5577: 0.47421248051488063}, {11227: 0.25607988500306256, 7429: 0.3443504317115218, 1225: 0.6267104404655206, 6371: 0.20155526743847943, 7896: 0.2547332878590868, 9515: 0.27631183795865843, 8491: 0.3558476673083809, 3915: 0.3385122281215478}, {6117: 0.3896907604111658, 3540: 0.8365552863248413, 9987: 0.385118636490277}, {11315: 0.3066440147388736, 1520: 0.37468779498246635, 9090: 0.5752507545759519, 2618: 0.4552932452883467, 1841: 0.30575522734691224, 5968: 0.2094496156654648, 2528: 0.300029555407852}, {5368: 0.30718354959543165, 8053: 0.3794309829885237, 11051: 0.31925528370756157, 2722: 0.15588085775377092, 1707: 0.30718354959543165, 3568: 0.35995471570729626, 2341: 0.2795462864966121, 2871: 0.3461360841578865, 6592: 0.1819676833432848, 3316: 0.24625138766597496, 11020: 0.3461360841578865}, {7346: 0.29593582655642653, 5215: 0.4116273751089342, 10638: 0.48011008921488035, 7922: 0.3738840955210854, 8082: 0.6104833134261354}, {8569: 0.2928356911582361, 2171: 0.09808937012713281, 5601: 0.13588608626597148, 6018: 0.0893280793624983, 8373: 0.20814715607107787, 1112: 0.18452388663098687, 4567: 0.2455891522780541, 8566: 0.21795169874428355, 11315: 0.09743669138630022, 6205: 0.1434910803918308, 7099: 0.22196588283796312, 6128: 0.10305997506669243, 4396: 0.2928356911582361, 2074: 0.14182825433334542, 9375: 0.23798415815219479, 4325: 0.06776553475979996, 6742: 0.12565816953126596, 4905: 0.1505676736437334, 5029: 0.14063230096851984, 11124: 0.2143608887121038, 9991: 0.21111260592159714, 2737: 0.2553936949512598, 8413: 0.2317704255111688, 6019: 0.2692124217181451, 7067: 0.09995389487611109, 241: 0.2928356911582361}, {2678: 0.538995907410797, 6840: 0.49551471323835816, 1047: 0.2567223823903382, 617: 0.3075921185452421, 4046: 0.2618015800394473, 2722: 0.20357130016329042, 395: 0.27617843305222467, 2760: 0.34230603164842477}, {1047: 0.4226161696621449, 9223: 0.3322098022393402, 2475: 0.50635788647444, 11642: 0.34933054552535586, 6076: 0.20229673517722135, 7366: 0.3638479154080181, 1384: 0.3991150292762325}, {3902: 0.22278989004363875, 10483: 0.1510157991795629, 4448: 0.14246882442661815, 5149: 0.24123150161366463, 6742: 0.21494364769613095, 8819: 0.3964531782795033, 9320: 0.4368617861949503, 1067: 0.4604993065318309, 2960: 0.23542919510913213, 1384: 0.22531397684186247, 11640: 0.1519139364428334, 10160: 0.1783262570959229, 4187: 0.28585620870677264}, {4325: 0.13409793100325754, 8067: 0.4118923147626626, 7346: 0.1670129097337498, 5577: 0.29899669570018583, 10910: 0.25120412635256134, 11259: 0.20981237877657416, 6834: 0.3929044650952636, 962: 0.4787994125517993, 4159: 0.3416550177698513, 10365: 0.15732227341023336, 6210: 0.2506873300912411}, {3552: 0.8259828024746824, 6052: 0.5636953166525955}, {10483: 0.28379355440183696, 10157: 0.553423500325396, 9929: 0.5309026571620848, 8892: 0.5756092566887678}, {2723: 0.4237144447095039, 6018: 0.12925199586957153, 8793: 0.24305223917586669, 9084: 0.13729594444978005, 3498: 0.4237144447095039, 6527: 0.20098887572760926, 6254: 0.32775525164665, 2734: 0.2584255510691123, 4785: 0.16026647197578628, 3874: 0.23079979701806275, 11618: 0.21281805261284986, 6923: 0.1670878565661527, 3354: 0.4237144447095039}, {8787: 1.0}, {5968: 0.20866499885675854, 11315: 0.24809217496222585, 10480: 0.17831292368217502, 8596: 0.6059525068277692, 2722: 0.216450363411675, 6592: 0.25267355951460174, 11359: 0.24161542141965067, 11298: 0.2775658205709914, 2031: 0.4998197350803688}, {4600: 0.7988449759317209, 6720: 0.5067220562259196, 6186: 0.32415962451055513}, {4785: 0.1970403290019262, 5863: 0.39486435905062733, 7922: 0.24678871690913756, 5805: 0.3877233596817328, 2289: 0.5209376144565219, 10365: 0.14142907967196217, 10612: 0.4789131959878904, 4745: 0.270715977064442}, {5968: 0.338590642337183, 7819: 0.49622583881343485, 4983: 0.644312785414568, 9713: 0.4732624307579437}, {6117: 0.3130986433793567, 5617: 0.4851733869898805, 4278: 0.5006870085535942, 9729: 0.2785978835400304, 4785: 0.2765375082145086, 10365: 0.19848954515891667, 11259: 0.17921005916643154, 9210: 0.4362160296574668}, {8376: 0.42567940236260343, 3363: 0.40382919458244965, 9450: 0.2380780088925295, 3416: 0.3763011883606764, 6230: 0.3208157772858286, 7848: 0.25910546415547664, 10447: 0.3883262254440552, 9615: 0.2797446267434262, 5215: 0.24150069825372555}, {8327: 0.5087815060570701, 4325: 0.1177378710138969, 6128: 0.1790594893718136, 7784: 0.3724370324099024, 3928: 0.4351041372188533, 464: 0.29997281921120944, 2623: 0.48455175066136136, 10480: 0.205955887071847}, {7228: 0.316269247356523, 11568: 0.2836827749049563, 4072: 0.4423372005872854, 8969: 0.30805973851394264, 4980: 0.481152091839923, 3543: 0.481152091839923, 9294: 0.25674924326708326}, {10811: 0.21499633016509648, 8803: 0.11856114554270109, 2674: 0.22693028092177894, 4019: 0.2602167848062983, 9244: 0.38492035865596735, 516: 0.38492035865596735, 7067: 0.15064698081961697, 9197: 0.44135161320019234, 1202: 0.2545035753406027, 8258: 0.3181814649090899, 8521: 0.3230771620286936, 8646: 0.19054189531823285}, {1420: 0.5254268616045911, 5030: 0.482888053099899, 11518: 0.7005324698240183}, {6110: 0.47900419488930257, 9450: 0.23918047804116135, 3935: 0.41446416906759503, 279: 0.346221670521975, 7748: 0.16937151330720193, 1053: 0.3003499910169954, 6130: 0.19649435191139183, 2960: 0.21863536974316783, 9493: 0.46517674679936766}, {6611: 0.2825566798311454, 3708: 0.21011028613829885, 6186: 0.0854107605218837, 6018: 0.0937558726275496, 4273: 0.3998727426241489, 1511: 0.2825566798311454, 6371: 0.09884659523601141, 7940: 0.21295049834268312, 5133: 0.19844680817183055, 11515: 0.2680529896602928, 10985: 0.2825566798311454, 8178: 0.2156013776558817, 7896: 0.12492612333156246, 8509: 0.1688761012069503, 2131: 0.30735090194448106, 4559: 0.30735090194448106, 2855: 0.30735090194448106}, {5968: 0.23724656050085333, 7489: 0.3476997257735783, 9906: 0.4462304873080871, 4256: 0.6515947165386375, 5592: 0.4462304873080871}, {11640: 0.2867601896584708, 6130: 0.5196351479385256, 8142: 0.45158706642425, 8803: 0.2540018054871918, 1520: 0.6158734536424925}, {11545: 0.15875597965380542, 8339: 0.22710961000897187, 8558: 0.20817109509971957, 1592: 0.23994565149878816, 308: 0.30134391730145604, 1998: 0.26638844528635647, 125: 0.3277867110890244, 11213: 0.3277867110890244, 9912: 0.1400949124217551, 8072: 0.18090786350938667, 2395: 0.3277867110890244, 3693: 0.3277867110890244, 2793: 0.26638844528635647, 10254: 0.3277867110890244}, {9929: 0.21778938780014326, 1696: 0.2361291395610199, 2171: 0.12934742541572597, 6117: 0.21515116799952003, 11259: 0.12314730313348589, 10365: 0.13639553661333448, 10508: 0.4618683972877829, 2281: 0.4381606012520515, 6076: 0.14902375456136724, 7133: 0.21601741524255602, 11545: 0.2433245394138927, 5500: 0.3675116247117868, 4448: 0.14289239238376197, 3597: 0.33339531518403986}, {6180: 0.39270504873831674, 9993: 0.39137802806513666, 11470: 0.8322295259381762}, {8185: 0.2410692984804952, 9331: 0.28209212629627456, 10654: 0.3516219288698497, 6128: 0.16325998734086153, 717: 0.3049216914274816, 10935: 0.4045757955762786, 6445: 0.3344289705868098, 4718: 0.3109266381986749, 10112: 0.42646640010600584, 8509: 0.2548868531039663}, {9884: 0.9590825843665722, 2171: 0.2831264670862419}, {4991: 0.5330761936303353, 4624: 0.29802972019785207, 8803: 0.28138277416936563, 5170: 0.7401565996896435}, {1704: 0.8019459727028804, 6570: 0.330922033052764, 9084: 0.4244683689598876, 2171: 0.259229374605559}, {8803: 0.0905885381274402, 4117: 0.22602671887536685, 8142: 0.1610563834621832, 10068: 0.3100178282729097, 2974: 0.3077654981532474, 1534: 0.21590770793325428, 7679: 0.2941045519299193, 8757: 0.2941045519299193, 7618: 0.24311163352399232, 1804: 0.27405619474328785, 7499: 0.2941045519299193, 7366: 0.13828263009431577, 1283: 0.18695918541524142, 1455: 0.15134032234861322, 9304: 0.19999443159026392, 6715: 0.19445752428234164, 7748: 0.12278291889045993, 8012: 0.1965794988147149, 11437: 0.27405619474328785}, {10430: 0.22538039292643838, 9919: 0.37212949920547966, 8725: 0.3234289844809579, 8158: 0.5549944532935143, 1115: 0.603694968018036, 11655: 0.18344085887869988}, {4376: 0.15228395056463162, 11038: 0.30320516575715895, 7716: 0.37423032972932474, 9219: 0.2210597666074321, 2039: 0.3298112832631627, 6816: 0.26103546722036225, 1141: 0.28764158472636603, 6732: 0.28764158472636603, 8568: 0.3298112832631627, 6925: 0.18634265228303953, 1209: 0.19018430717749243, 5967: 0.24547188618956936, 4599: 0.14734703505285776, 9938: 0.30320516575715895}, {7356: 0.4190495148226279, 3902: 0.5262333769947148, 4390: 0.7399161689432517}, {1841: 0.4363244875112537, 7133: 0.35296732075728326, 7151: 0.32477195515029633, 6250: 0.5060220435607452, 8425: 0.5687704991065622}, {11655: 0.4196673561618356, 4418: 0.5653341700744954, 2674: 0.7101243456735704}, {2924: 0.22098799075123948, 5771: 0.11864755701740945, 4019: 0.16843908436408064, 8065: 0.28568818750638464, 10365: 0.10090962484526052, 5863: 0.2165481622319271, 9993: 0.15244706860177615, 1384: 0.12850573889381478, 9876: 0.23959483732341294, 7093: 0.28568818750638464, 1593: 0.14303331202457722, 5006: 0.18958528084228005, 3154: 0.28568818750638464, 3780: 0.23217546511772288, 6674: 0.2626415124148988, 4572: 0.24916007172007523, 1420: 0.14846375970997455, 10776: 0.14846375970997455, 7591: 0.3680396377651986, 11655: 0.08681020923484746, 4448: 0.10571619912727784, 7151: 0.14705006454055283, 6128: 0.10054449771748207, 2702: 0.1956473493314135}, {11655: 0.2244611570595639, 11467: 0.5713984626635323, 10319: 0.6791000538777309, 5968: 0.20672725567211606, 6434: 0.34526503504231654}, {8185: 0.2380052662843237, 4325: 0.10598472026611833, 6585: 0.3352586075898017, 3028: 0.4579925314741615, 10699: 0.32554038256432377, 6700: 0.38409934632469706, 5013: 0.3039280113786827, 9713: 0.2330815220646633, 9875: 0.4579925314741615}, {1844: 0.2882728079696613, 6349: 0.40807608029799625, 2970: 0.3491612181774544, 4826: 0.32575777724479776, 9223: 0.23023609276611914, 4771: 0.2882728079696613, 3469: 0.3140126652232034, 2464: 0.5363088578531713}, {10211: 0.5938616148037152, 6058: 0.31589720747148897, 9080: 0.35323285910831226, 11276: 0.47059676291322233, 6018: 0.19610622645675516, 2960: 0.30215502324260163, 1163: 0.26748319691217415}, {6527: 0.1956265248049864, 4785: 0.155990588244641, 4704: 0.20324793764338475, 3924: 0.3596790580607212, 7862: 0.3190107941750643, 6570: 0.17634713090166387, 4674: 0.3351606540399947, 2171: 0.10617924113479357, 2990: 0.4124098113794687, 8159: 0.4124098113794687, 6754: 0.4124098113794687}, {11023: 0.51789889269195, 2688: 0.3681169255892513, 10861: 0.24908569397550095, 18: 0.5600298804558953, 450: 0.46967384012362057}, {10861: 0.308421631046792, 10160: 0.24686791245877943, 10322: 0.693437372529856, 4862: 0.2946245665590178, 1355: 0.52561707207765}, {7399: 0.28105153945313144, 4117: 0.35716045844668276, 3157: 0.28548319935313565, 935: 0.4468938169772651, 11655: 0.16191895531200165, 7346: 0.15357905858369486, 8878: 0.39660247883520083, 11259: 0.1306161621652901, 6180: 0.21929498620757484, 4487: 0.4898806221827816}, {11640: 0.1590228158463054, 9966: 0.3479621839855161, 1311: 0.5243481203917164, 10946: 0.517093696489019, 4325: 0.12134016396054755, 9450: 0.26960469317991637, 4152: 0.2273051467024159, 999: 0.415005378949866}, {4127: 0.25531074372985046, 1593: 0.23348541329565006, 1274: 0.2765777017287608, 6363: 0.40672512926456883, 1360: 0.2985846014355284, 10279: 0.1926176794809138, 10754: 0.2835907579911993, 2295: 0.46635307250769475, 1923: 0.23733636793105256, 5968: 0.16979981516608106, 10638: 0.2180574641356028, 3995: 0.2854953636902394}, {10160: 0.4138688254740497, 2683: 0.7443168970399979, 6758: 0.524123031435826}, {5617: 0.3578722501279098, 1384: 0.24257493898241003, 3820: 0.39476371963432516, 5086: 0.383320605058144, 7748: 0.19635323562208765, 7067: 0.18407355730935604, 6586: 0.43826796973388, 10823: 0.4957774601874878}, {10629: 0.38566494346486263, 10628: 0.38566494346486263, 6018: 0.11764518368953514, 10480: 0.11999571784376367, 4930: 0.19128021530240194, 1068: 0.22608334096198074, 9457: 0.31379006288295935, 8280: 0.3881270065319044, 10483: 0.13200898543113757, 8517: 0.5181722015692469, 11257: 0.21581179955003352, 5812: 0.07052397528095644}, {4784: 0.5024151968244921, 3922: 0.546501905341261, 407: 0.31513805473894196, 9018: 0.47662612556195827, 3241: 0.34990024341993564}, {6428: 0.9038013005739516, 1638: 0.427952344403946}, {4475: 0.15200918785844067, 4448: 0.11513344205356647, 8646: 0.1343253542412234, 3174: 0.20089163724013426, 11259: 0.09922412700106341, 7346: 0.08967369377006101, 10782: 0.2536497750171671, 5354: 0.3111374099068192, 3497: 0.2713553543947246, 9260: 0.17755870453943418, 8723: 0.23157329888262995, 3018: 0.24625567191391265, 6514: 0.25285775198186994, 2763: 0.23583836246438347, 8237: 0.2713553543947246, 10480: 0.09680723509478031, 11534: 0.1917912433705353, 10483: 0.07209902872344821, 8491: 0.176664555716468, 4108: 0.19605630695228887, 2171: 0.08010559681412066, 6308: 0.19920709223412997, 6834: 0.13168102738915213, 7151: 0.12309427599605682, 10260: 0.20089163724013426, 3205: 0.2713553543947246}, {4680: 0.29981288039275544, 3892: 0.4754612497822295, 6186: 0.16693954077008355, 3008: 0.4754612497822295, 11315: 0.19988473921554423, 3908: 0.5038094270118813, 2656: 0.32055895335603113, 6018: 0.18325047365585015}, {10365: 0.5295427693068395, 10480: 0.6068811220627925, 11655: 0.592688585311586}, {2656: 0.3931223080711274, 11301: 0.7367173451825786, 11359: 0.3105977523826754, 3942: 0.4541271192285187}, {10995: 0.4363108842538446, 9886: 0.4363108842538446, 1946: 0.4227999385096197, 10260: 0.3359081191163783, 6926: 0.4537295228369022, 6058: 0.19649068097093378, 1136: 0.28843110870533845}, {5386: 0.3469867224546361, 718: 0.566798430785258, 758: 0.3142388404666685, 6840: 0.6779334070848861}, {10494: 0.47053502036303174, 10514: 0.4128967980921382, 11642: 0.3046375544984148, 6180: 0.31843753216734666, 8803: 0.20786070193878375, 7057: 0.5427899035235391, 3900: 0.2758252643541562}, {11655: 0.14341225437118724, 6288: 0.3354704081477073, 6769: 0.2930001947012033, 3899: 0.4338891902101171, 10785: 0.17349620311133443, 9221: 0.41161757239860286, 7296: 0.3835586523254864, 9084: 0.1529298219166765, 10032: 0.47196277233556494}, {7346: 0.13056333472842652, 11640: 0.13738778775327343, 8803: 0.12169313384394224, 6030: 0.589380436657026, 2380: 0.4530106443520047, 8479: 0.37992124416018214, 2551: 0.31023449922795515, 3094: 0.39508866509988444}, {10483: 0.08679789046786418, 8055: 0.3141356027267226, 7028: 0.2646767330704991, 5758: 0.11446488159605832, 10312: 0.1858892626660831, 4440: 0.32667669378688574, 8004: 0.37456913503141376, 7369: 0.3141356027267226, 7353: 0.3022785285964154, 3469: 0.1470154701594443, 2722: 0.10873660304051438, 664: 0.23397460303002954, 2209: 0.17507410289403738, 9615: 0.17393816447196958, 8213: 0.2646767330704991, 747: 0.11087782609364016, 3476: 0.2510908240975383, 7133: 0.12379020536689472, 1163: 0.11978813111657802, 6834: 0.12184722781278393}, {6018: 0.2562435529522389, 7653: 0.4205464770383645, 9395: 0.48054924571916685, 10811: 0.31451979076322983, 6193: 0.4726320738759258, 11464: 0.2642896865256806, 10064: 0.3666053876007912}, {7346: 0.15471463764518378, 3902: 0.23875702719786707, 5083: 0.536807502934222, 1618: 0.4152359694289869, 6593: 0.36762095754466007, 10483: 0.12439294774809631, 7133: 0.23081295553331804, 10660: 0.3164966849460241, 1651: 0.3929525562310156}, {5338: 1.0}, {2002: 0.46912259576427673, 11315: 0.20593143865804092, 7338: 0.39960761830535463, 7062: 0.4898443952152922, 1508: 0.37635818379733543, 4325: 0.14322175626007752, 2363: 0.4191950603318855}, {9521: 0.37121846510474127, 2171: 0.12459548398171856, 213: 0.21110326492232334, 10854: 0.21669355595292258, 215: 0.27581483617051356, 3916: 0.4839401703992416, 6758: 0.2181823440470719, 1984: 0.3314159132725368, 10610: 0.25148949209022325, 4087: 0.307341182014527, 6875: 0.3601869082690408}, {4412: 0.07475761282969653, 10600: 0.11826913799093067, 6226: 0.13500484369678364, 25: 0.3406846445453116, 7334: 0.20582719875457484, 1956: 0.21636232690163784, 500: 0.313201365029883, 5156: 0.28571808551445443, 10754: 0.2695361991067806, 5386: 0.20856276549253108, 6511: 0.17002850205971243, 7366: 0.13970263822161827, 6018: 0.10392416595517916, 10116: 0.23604604500795967, 11031: 0.24938716618336706, 5812: 0.062298728099729396, 9735: 0.1694962794234875, 11458: 0.23331047827000342, 5914: 0.19248607757916747, 10480: 0.10600055611299478, 10723: 0.24560811840579125, 3368: 0.2971246771165194}, {10365: 0.19854229032184376, 10480: 0.22753887865404754, 4412: 0.16047334106426706, 10600: 0.2538743948583694, 10670: 0.6378024637858543, 9380: 0.4092277270582518, 1956: 0.46443946193734975, 4325: 0.16923293677165682}, {7356: 0.2186651648403044, 10430: 0.23049082390492595, 1136: 0.34228375590069693, 10480: 0.19209260622609176, 988: 0.5177739976561552, 7067: 0.21073214045183963, 6128: 0.21728066892876946, 5294: 0.40581645364168634, 404: 0.3661486064560051, 11227: 0.25226883458944105, 11259: 0.15133271008371868}, {10365: 1.0}, {10160: 0.15303603313254052, 10483: 0.09961241777575155, 2171: 0.17730692679033289, 4942: 0.3325161081160443, 8646: 0.1855846540717814, 5812: 0.11611252446158611, 8803: 0.1154765945050975, 10767: 0.23030198460411552, 7748: 0.2036319778163689, 6058: 0.21122955858470555, 11298: 0.208197278764484, 3750: 0.349349671891818, 9663: 0.39519130990345025, 7403: 0.3325161081160443, 10480: 0.13374941267191398, 4: 0.24788227235135363, 6226: 0.1703464511361629, 4412: 0.09432768256486991, 10600: 0.14922966749946434, 595: 0.22938398495248505}, {9430: 0.30880760882901276, 758: 0.1963057647963843, 4757: 0.5214968514869375, 7961: 0.27389128816950736, 4247: 0.27389128816950736, 1109: 0.23062913958142076, 8950: 0.1495663527727401, 3900: 0.1262179584063892, 2397: 0.20640702286118645, 11227: 0.14468062142785423, 8803: 0.09511729642700853, 9913: 0.30880760882901276, 6127: 0.3255164315380374, 2550: 0.259193060667688, 11640: 0.10738448851138324}, {10480: 0.20635605445650435, 11359: 0.21491747790107563, 8803: 0.17816373132492627, 1844: 0.23897319698228825, 7748: 0.185608000586332, 2898: 0.3382880480738105, 7067: 0.17400031542520664, 5292: 0.5097700564089707, 8087: 0.37941157146472215, 2613: 0.41428419208685535, 3469: 0.26031109569382427}, {4745: 0.2319918962748756, 6226: 0.16681476904749137, 5771: 0.10912529967401349, 7780: 0.1507249989539611, 4752: 0.14501904295623688, 10624: 0.22036575451298593, 6128: 0.12031292000744333, 4238: 0.1523348887545591, 7922: 0.16195181714274864, 6076: 0.07794123324785379, 4160: 0.12726168883079045, 4656: 0.13704581380059838, 10674: 0.1402121751495709, 5924: 0.1711477848064698, 1204: 0.1507249989539611, 11259: 0.08379613489123096, 4905: 0.17577373236392257, 10654: 0.199168728724423, 3764: 0.1419274383785268, 4367: 0.11164021012596578, 895: 0.14252125983264424, 9897: 0.12913840300555535, 11298: 0.12726168883079045, 3708: 0.1380652696678785, 8871: 0.1820552056210448, 4771: 0.12317818614560576, 10099: 0.19556682408672876, 1428: 0.18432148995565256, 8142: 0.1254935175540581, 7869: 0.18676926351129447, 1403: 0.17436979829816587, 9810: 0.24156278030154882, 9557: 0.20796628929985733, 4325: 0.060805630292709495, 5765: 0.22036575451298593, 1384: 0.11819229674118285, 4599: 0.11739100607891834, 8618: 0.1923448105950327, 4490: 0.18676926351129447}, {1808: 0.40573540713319645, 8812: 0.37879895103811523, 10160: 0.20321409887355382, 11259: 0.13991819058521185, 11331: 0.317926573672234, 1226: 0.5708163907588797, 2000: 0.45178358297673044}, {9773: 0.1538050705064243, 11559: 0.259548526144079, 9758: 0.3193703002445078, 11099: 0.4155103403423163, 7848: 0.23251279588779128, 10776: 0.16596736437674164, 2707: 0.29360646447709104, 8128: 0.2678426287096743, 4390: 0.19972675204365015, 1742: 0.15839972190142282, 3702: 0.29360646447709104, 6570: 0.10496551153957853, 316: 0.17548024360874684, 1082: 0.19686615954622577, 11193: 0.3193703002445078, 6902: 0.24704206909176665}, {6919: 0.25012359718337507, 8892: 0.2203080143050525, 7346: 0.13509544279359567, 7193: 0.4464128860641887, 11655: 0.14243161252763184, 1818: 0.2953632496516994, 6570: 0.15405648049765694, 7086: 0.35529581380397646, 9147: 0.46873552759848164, 11562: 0.2377524389039314, 4935: 0.37098972551470283}, {9450: 0.24195129714979868, 11575: 0.31227164772754806, 8892: 0.32669222135078746, 5030: 0.22474114146029786, 4325: 0.10889428414746426, 6180: 0.19365546209680448, 5965: 0.4326047754291965, 6434: 0.2861524399493385, 7346: 0.1356229071752043, 10861: 0.20929449617653592, 11056: 0.4326047754291965, 6018: 0.14354372456332465, 7366: 0.19296221275716074, 10061: 0.24725537148017782}, {8789: 0.5668046305013708, 213: 0.24725020858098615, 10854: 0.2537977180373305, 215: 0.3230422599005413, 8790: 0.5668046305013708, 7238: 0.35715930994230427}, {11640: 0.10195808662545446, 6076: 0.07664838067512082, 2694: 0.2600508888412102, 1163: 0.13987853130706435, 4418: 0.13761333480487925, 5788: 0.24609539886639625, 5058: 0.15639623057919771, 7346: 0.0968935304224807, 11366: 0.35546210055396704, 7133: 0.144551817909819, 8140: 0.33618780296572726, 6925: 0.18994537194124733, 10652: 0.309067286950348, 5099: 0.18902470278214104, 5415: 0.24236623234836507, 11493: 0.33618780296572726, 11497: 0.33618780296572726, 4104: 0.26608228606576434}, {9765: 0.2747580492975683, 1036: 0.37325834437299404, 9349: 0.33324166260248783, 1455: 0.1626774205832191, 9365: 0.3160965188754019, 9432: 0.2302063451555057, 3900: 0.12921339211941393, 213: 0.1581217028893495, 10854: 0.16230897274389516, 215: 0.2065923120302093, 10843: 0.33324166260248783, 5968: 0.1319807452951527, 1707: 0.2697891352599733, 4812: 0.3624834692549476, 8049: 0.22668872443172658}, {6462: 0.7194796141134914, 6031: 0.6945135598928946}, {4152: 0.39560943538136345, 8654: 0.9184188448835523}, {1691: 0.24893321252945674, 10365: 0.12459916220127884, 11677: 0.3478758996026105, 6742: 0.1969376924471661, 6350: 0.4589465650979789, 3014: 0.4219230099328561, 251: 0.28491957589304795, 10767: 0.245880170192554, 736: 0.4589465650979789, 6371: 0.14760101586880112}, {6923: 0.1511673997637994, 1896: 0.33432795208211236, 11131: 0.3833421061556244, 2478: 0.35241761800451, 1691: 0.2079252558797457, 4493: 0.3034034639309979, 8172: 0.35241761800451, 7197: 0.32149312985339557, 1104: 0.3833421061556244, 2283: 0.29056864170228114}, {251: 0.35497496252144617, 865: 0.23447125333385163, 2995: 0.329721089391926, 3608: 0.4255727453579004, 1455: 0.2566118032093107, 1163: 0.23790669688690724, 4390: 0.3575849809708901, 11184: 0.49868203154051566, 11655: 0.17374650440912906}, {8049: 0.27764418018122905, 8803: 0.11926267056816958, 3775: 0.23881860614470715, 407: 0.2560094702638964, 1921: 0.28917397918712523, 8307: 0.22670715139172382, 7180: 0.4439630859577604, 10300: 0.33043277061456916, 3277: 0.3114324778462507, 8062: 0.3723334320753909, 3608: 0.33043277061456916}, {11640: 0.2511813374162318, 7489: 0.4419513234175739, 7346: 0.23870442614232965, 11585: 0.4419513234175739, 7232: 0.5738413156592809, 11234: 0.3999904156041613}, {3539: 0.3154121987826659, 6018: 0.13143799736060327, 4599: 0.19250093220667916, 4022: 0.28065274522142614, 725: 0.43088060413068086, 4238: 0.24980285178865722, 1699: 0.28880262931694656, 4744: 0.43088060413068086, 4734: 0.27820605064737824, 10157: 0.19471042134853372, 4325: 0.0997107095827449, 8725: 0.2308438592490707, 1384: 0.19381491020726577, 4818: 0.1848943610235353}, {6923: 0.2688554897477602, 10652: 0.4817603657747875, 7067: 0.17886925368750298, 8934: 0.4147572547263075, 4496: 0.4817603657747875, 2739: 0.2753502618779278, 9528: 0.3355154667297235, 9713: 0.266691657491649}, {11640: 0.31653315998242626, 4599: 0.46628922681031504, 5155: 0.8260636268424959}, {831: 0.5928518394789605, 8905: 0.5928518394789605, 9032: 0.545026047866362}, {10430: 0.32212930104251764, 11259: 0.21149952652451962, 5400: 0.7236304384584348, 9199: 0.572590300499595}, {6021: 1.0}, {10160: 0.1556975223578238, 7520: 0.3067900998267092, 1747: 0.3255071641336851, 4325: 0.10120667640010166, 8778: 0.29022621203598253, 11432: 0.24832560062700013, 6371: 0.14065381747415276, 4540: 0.4020641841651784, 1063: 0.2343072259713846, 7848: 0.24473119084505582, 5450: 0.4020641841651784, 11370: 0.4020641841651784}, {3192: 0.47678234119950097, 6598: 0.8790213871802681}, {10160: 0.28825348511970195, 3942: 0.49910705513279, 8803: 0.21750779953254784, 865: 0.3320237991154546, 3157: 0.4337890125172391, 7067: 0.27637126104264464, 6158: 0.4956799859642168}, {1707: 0.3460891166056647, 9628: 0.3524633060929605, 8646: 0.20075087849679132, 3829: 0.3149515248177383, 747: 0.17908190919877015, 6210: 0.20116242466883746, 8366: 0.22331236273373414, 10674: 0.24812954887150881, 4785: 0.17588187994642265, 9800: 0.27743974354251616, 7057: 0.32618856458957357, 2722: 0.17562355935657228, 8546: 0.2741588565718239, 7336: 0.24912256867396126, 9773: 0.22393801202357258}, {6933: 0.5254437266732138, 1136: 0.2913113700253566, 10260: 0.33926248393130315, 7552: 0.5254437266732138, 7277: 0.44066786699769667, 6592: 0.23166411868466444}, {10494: 0.22761664084328714, 10634: 0.1845148888115205, 513: 0.3264471956897335, 8524: 0.3041941229684331, 7819: 0.19973472351573626, 10365: 0.1016201372391442, 3232: 0.2194245302905973, 9084: 0.12128615212200997, 1300: 0.2460375340291563, 2841: 0.344110453202466, 9504: 0.37430599136253073, 7922: 0.177323527378865, 1527: 0.25352383872227197, 4238: 0.2170037434676044, 4915: 0.37430599136253073}, {11315: 0.23560048566269498, 865: 0.29035526748920315, 10638: 0.33108063212646116, 3544: 0.6509516733461572, 5968: 0.25781016193595, 1566: 0.34293863149715476, 10674: 0.3778369744013601}, {6971: 0.38476637642127287, 7929: 0.5680742319743088, 5867: 0.32757764732752886, 11651: 0.46166731701877173, 8892: 0.2669976962602507, 4412: 0.09581216332965807, 1844: 0.20468812720364046, 6592: 0.19250869166854528, 3222: 0.22221185763643103}, {8015: 0.33203881662297924, 6076: 0.06578579899582232, 10483: 0.08699125858956071, 3822: 0.11832136493508791, 4524: 0.251650203602838, 3351: 0.23449585106115947, 9360: 0.23449585106115947, 3708: 0.1516130677765623, 7399: 0.1521871265604263, 10480: 0.11680300512469467, 5812: 0.05276400386073189, 11590: 0.1250836876383506, 6720: 0.12534300646433555, 1326: 0.15585421580677017, 7748: 0.10505905581868585, 6130: 0.12188301729389829, 9929: 0.16273762958575805, 4412: 0.06331607550078285, 10785: 0.10607019176286955, 3900: 0.10285619273354586, 4607: 0.23449585106115947, 5607: 0.22319656790278075, 11234: 0.13935195406668752, 4947: 0.1250836876383506, 3190: 0.2147570219971588, 10413: 0.2147570219971588, 6366: 0.28854338520851724, 4826: 0.11748699602488882, 10365: 0.07833649228429632, 9912: 0.12332244997474975, 8307: 0.14734299085345917, 1111: 0.2419893732506408, 10771: 0.251650203602838}, {6462: 1.0}, {1274: 0.2864349496836316, 2171: 0.12434671458628015, 2722: 0.18241257310333442, 420: 0.4829739274844001, 11227: 0.1973477699171777, 7429: 0.265373400052384, 11315: 0.1607023424426018, 9256: 0.23391735055484772, 9929: 0.20936941541483947, 8763: 0.4440120674174939, 10406: 0.39250728996598067, 10638: 0.22582904687332817}, {7037: 0.3156775187478853, 4330: 0.3004664621132992, 9300: 0.2843420923083171, 4367: 0.16503700221885304, 6128: 0.13670539024058292, 7366: 0.15928381985670986, 2797: 0.3074352234728049, 11259: 0.09521324326805093, 6226: 0.15392756698745086, 2381: 0.38306206184302194, 6527: 0.141622108384095, 10288: 0.13242804186371018, 1768: 0.3387706499123731, 7346: 0.11195215066343017, 4127: 0.16345061263024485, 7640: 0.2239900792054974, 10480: 0.1208579429819285, 792: 0.3571006993236268}, {11655: 0.18604248539798934, 10586: 0.4724145271346259, 1095: 0.24799472097779837, 1320: 0.5628655911445518, 10480: 0.19049746374029822, 10279: 0.25288025316430185, 7024: 0.5134743875986583}, {2980: 0.3532289255955845, 8803: 0.08362583282459803, 4448: 0.0885410635695989, 5149: 0.1950501207189113, 2171: 0.08014813402759707, 11359: 0.1312442245248411, 9372: 0.2610766067656348, 6186: 0.08650891885403116, 8491: 0.1767583671131631, 4152: 0.1349498297866544, 8493: 0.18747177623364664, 10654: 0.2359635959957353, 4332: 0.20462437891057259, 9386: 0.1330496721609537, 5746: 0.19931287403896492, 6665: 0.3113026283054339, 5812: 0.08408636053982368, 7067: 0.10625723272059996, 9084: 0.10087120912811004, 9496: 0.1946814177977566, 8892: 0.19035860830620124, 4488: 0.21568848314297126, 1696: 0.1463137736567339, 5738: 0.2662224548067671, 10483: 0.07213731433516857, 5276: 0.2861896175355344, 6487: 0.2408014939128708}, {1140: 0.28293768569203426, 6052: 0.1930919721951625, 2281: 0.3117761933192147, 8276: 0.3574841464850568, 8546: 0.2107693105346617, 5174: 0.29980713123069586, 21: 0.3574841464850568, 900: 0.29980713123069586, 11240: 0.322246541493622, 6186: 0.09934245395941792, 9101: 0.2372297325261922, 2171: 0.092038051336701, 2074: 0.173139251732893, 8344: 0.2173872245043726, 699: 0.22356226433670923}, {4152: 0.17812124596067588, 7356: 0.1455293896819688, 11072: 0.49145591559455487, 11701: 0.41089056660729956, 6834: 0.17389902413716213, 4642: 0.41089056660729956, 10902: 0.41089056660729956, 3356: 0.41089056660729956}, {3785: 0.40715284722606615, 6639: 0.48548107405614876, 307: 0.355380705391368, 10137: 0.4234074229193179, 2053: 0.48548107405614876, 6876: 0.23931886869747537}, {8524: 0.26232865869759453, 7114: 0.13714608376062468, 7429: 0.17735988571268504, 719: 0.29675140581789544, 8597: 0.29675140581789544, 6496: 0.27071160285627216, 8220: 0.24024698633883754, 4291: 0.27071160285627216, 10699: 0.2294394916359316, 9380: 0.1806286851477475, 5193: 0.2815190975591781, 10421: 0.3227912087795187, 1823: 0.2815190975591781, 492: 0.27071160285627216, 11112: 0.2815190975591781}, {6117: 0.1670278687748407, 3671: 0.30869273042578266, 4367: 0.24477679944536224, 4772: 0.2956341573067454, 7922: 0.18477021965634385, 7133: 0.2181831996007385, 2047: 0.2902877053356764, 4680: 0.19465320613961284, 9695: 0.2956341573067454, 10483: 0.09037942502891338, 6527: 0.14220138743994504, 4448: 0.11093136042864188, 10850: 0.3900249519341759, 6186: 0.10838532620680756, 2011: 0.28550514107697444, 3672: 0.3900249519341759}, {8108: 0.3565789604333799, 1314: 0.45052791399376474, 10483: 0.10439961244608834, 7133: 0.193715025983725, 4187: 0.2571055431474595, 4624: 0.16677415555541175, 6919: 0.2404077690824639, 11655: 0.1368989835432521, 6076: 0.1336380238354061, 3524: 0.5388651540504289, 9453: 0.37783908115059983}, {11464: 0.3677304747399312, 7067: 0.3066385622883856, 9515: 0.3960806427359903, 995: 0.7834967865378272}, {6128: 0.1623586124434787, 8892: 0.16665733033788174, 7906: 0.29298028025117984, 7859: 0.274282681075841, 9523: 0.17641260241809661, 5215: 0.18493914176861417, 8803: 0.09525321982690918, 7338: 0.22894524215683726, 5173: 0.29737685803465813, 1691: 0.1923280890256263, 6076: 0.08084311284369759, 3995: 0.2170732027463849, 3666: 0.23766552794463003, 4826: 0.14437788431984083, 8415: 0.21857401960710293, 5812: 0.06484083773869385, 2111: 0.4613274596623105, 3636: 0.1923280890256263, 9515: 0.15633440260388612, 8189: 0.17925979439760603, 11533: 0.2078645335623341}, {6758: 0.2520132683354208, 4704: 0.21174117841179144, 2403: 0.37466202346057015, 5968: 0.20352503928201846, 11640: 0.16952556491121687, 9223: 0.27228730023979636, 8754: 0.5138857362860144, 4821: 0.2807570507798274, 9134: 0.5138857362860144}, {490: 0.8594173906051966, 7067: 0.2254719609469091, 4947: 0.28635606228760413, 4325: 0.19887903290027306, 10600: 0.2983479167279373}, {213: 0.21279015762764786, 10854: 0.21842511978715473, 215: 0.2631348468102294, 11367: 0.32699951849548253, 440: 0.4254361821303016, 1655: 0.44845549103973825, 5609: 0.20980989773658035, 10464: 0.37493928746091265, 11464: 0.15347566223906542, 4026: 0.37493928746091265}, {3902: 0.261471652715727, 6018: 0.17932920159967106, 5338: 0.4930286972905957, 10279: 0.24281098492796088, 7429: 0.3230135409118762, 11259: 0.18747876372495093, 11711: 0.4777614091714123, 3838: 0.4547402707591771, 2171: 0.15135530754530857}, {3900: 0.15219544788492392, 526: 0.28918406907747873, 2171: 0.10992411286050197, 11315: 0.14206296070130894, 5481: 0.39251244210685904, 9195: 0.30780275286325665, 11333: 0.39251244210685904, 11692: 0.33792190993616067, 9800: 0.2547412780676853, 6058: 0.16125508695344248, 11227: 0.1744579951735834, 2538: 0.31777416877525577, 11655: 0.12973610650227305, 5569: 0.32362686008727215}, {4278: 0.32577020979544746, 5038: 0.47569620083318337, 9075: 0.4148736444131575, 9223: 0.17810424997208693, 5228: 0.43732144029951037, 10157: 0.21496211899580986, 6449: 0.47569620083318337}, {4865: 0.5438313660415601, 10369: 0.7285866933343725, 6825: 0.41642391334487}, {3513: 0.3200346706019762, 3321: 0.25083402344265116, 7509: 0.38160306793282883, 5283: 0.38160306793282883, 9782: 0.3200346706019762, 7429: 0.2096744727716727, 7907: 0.25083402344265116, 10181: 0.2557737997902305, 3694: 0.3508188692674025, 4458: 0.38160306793282883}, {4785: 0.21786254779906528, 9942: 0.5295223042968779, 8142: 0.21144052166364138, 2074: 0.2144196640482821, 9729: 0.16870153581559455, 2450: 0.40700237970042796, 4448: 0.1259179976001722, 6371: 0.14238134278185663, 3593: 0.3712881031560578, 6774: 0.3295050781183745, 10480: 0.17921268869421458, 10483: 0.13347205688831043, 11640: 0.13426585613678652, 9929: 0.1919178701740162}, {4412: 0.11669926738420384, 3388: 0.3928453330806571, 4486: 0.3928453330806571, 10480: 0.11202235566578322, 6923: 0.1419778956218068, 5695: 0.37074084569132243, 10288: 0.12274659687032724, 7232: 0.2494556158667314, 6925: 0.20342104451130924, 5946: 0.24131999878592997, 1699: 0.24131999878592997, 10260: 0.23246562527235345, 3874: 0.19611520647876146, 6058: 0.20086133589172964, 7993: 0.31400407203097047, 5099: 0.20243505848763985}, {7133: 0.30280599477999154, 6514: 0.5723305185073139, 4152: 0.23465245822217393, 9735: 0.26930422364794243, 1841: 0.28770820548686643, 3900: 0.19295448450546257, 4624: 0.20037456868520492, 2727: 0.5412969182575265}, {7356: 0.31477479447962964, 3521: 0.7751068503061345, 2385: 0.5478377491272491}, {8950: 0.15324876671058626, 865: 0.14877054781218632, 3254: 0.24845434634877536, 4424: 0.30426366060749765, 362: 0.3335308426824675, 8803: 0.09745914171239346, 5968: 0.13209527539976856, 9729: 0.13824775531734457, 6288: 0.257876274516892, 5867: 0.20920597470524005, 4325: 0.08395562050597884, 1508: 0.22061861045629538, 2960: 0.17051686446215353, 3725: 0.36279802475743744, 6930: 0.36279802475743744, 3495: 0.24316950555789, 3764: 0.1959622176192121, 8726: 0.29484173243938105}, {6923: 0.28983646650216127, 11432: 0.41732903558729184, 9420: 0.441244873205667, 4680: 0.3668179484545287, 7906: 0.46677812199412555, 10549: 0.441244873205667}, {9576: 0.5190210085789447, 995: 0.5475996380020578, 3157: 0.33638635436111775, 2857: 0.4526548182890739, 6180: 0.25839643491170455, 7067: 0.21431506624116126}, {2479: 0.4344276285481936, 3328: 0.5144069023709953, 3822: 0.39052730467059166, 9729: 0.3629046478272063, 1763: 0.512295632656774}, {6923: 0.16909880093892268, 2785: 0.4288139546066375, 11302: 0.4288139546066375, 6130: 0.2356613133092062, 3980: 0.3008056224541621, 6076: 0.09776647142324868, 6371: 0.1379101188068493, 9929: 0.18589104274291718, 336: 0.39422121917443326, 10480: 0.13342109304040134, 237: 0.2558502774456161, 10194: 0.3739857661492257, 10564: 0.12630623126742804}, {9934: 0.3380584697421847, 8079: 0.6753155406118668, 8995: 0.36098118278356217, 6834: 0.17840168148529, 1879: 0.34257211554739275, 5256: 0.38752440311864217}, {6186: 0.06744135426643888, 6758: 0.1094148113296623, 7851: 0.35847950643532034, 417: 0.15292439507774444, 2183: 0.16619956019236373, 4777: 0.24268793458215787, 1638: 0.13744502664746647, 1792: 0.3157442824771254, 7532: 0.2035323081227071, 3305: 0.2311303814431178, 10255: 0.18772606424856383, 8389: 0.197229660007601, 8969: 0.15538201524174491, 11694: 0.22311012135243247, 5684: 0.21165783476692057, 11211: 0.2035323081227071, 1603: 0.197229660007601, 11695: 0.24268793458215787, 11680: 0.13054777772799583, 5867: 0.1399449898808222, 9729: 0.09247862421802804, 9515: 0.10699925344306752, 4285: 0.17765184677787563, 9122: 0.22311012135243247, 751: 0.17250220830746985, 10279: 0.10023732993945217, 10494: 0.14757928998599482}, {4577: 0.4447189565390831, 4160: 0.3030246219330844, 6851: 0.318411407102948, 10160: 0.2227391556571962, 3736: 0.4388905280222935, 4376: 0.28888686367671274, 2645: 0.4579949712033731, 4152: 0.2712243781829111}, {7356: 0.33834729744563397, 9223: 0.3576702139311011, 10157: 0.43168844707728127, 4376: 0.4410895816224303, 1841: 0.507755771066772, 5812: 0.17468869293541736, 10480: 0.29723076477843624}, {9729: 0.5174714211148654, 1112: 0.8557004898499017}, {4325: 0.09480769527537966, 4771: 0.14762037008842513, 10480: 0.09797764629429338, 4412: 0.06909940113734749, 10600: 0.142225542646169, 3473: 0.31489909888374223, 1229: 0.2894959582011647, 8838: 0.19693056678523643, 9978: 0.31489909888374223, 2082: 0.2894959582011647, 10483: 0.10778659367729028, 2528: 0.1642397439462169, 10487: 0.25591483283448935, 5812: 0.05758349739064584, 6076: 0.09340707195993157, 9080: 0.17302351077100267, 6058: 0.11893303473852657, 8724: 0.2343730481188834, 4599: 0.14068484286607344, 8995: 0.182562622157343, 747: 0.12127504422419585, 5948: 0.2894959582011647, 5722: 0.2894959582011647, 1828: 0.25591483283448935, 10430: 0.1175628196332404}, {4537: 0.26176390629273916, 5753: 0.4895226586927376, 3623: 0.46033610543964276, 11655: 0.18793917149507938, 9060: 0.4895226586927376, 3820: 0.452751950257046}, {5812: 0.21535305069550242, 10480: 0.366420693177079, 7323: 0.905184477975495}, {11259: 0.24403192797989848, 925: 0.7034815040298531, 7588: 0.5800211301752125, 8646: 0.3303599251794604}, {4993: 0.5024174783274881, 3615: 0.2601560185592583, 4412: 0.11024720933286244, 1287: 0.4618870927894494, 6236: 0.5024174783274881, 10480: 0.1563220795427398, 9068: 0.4213567072514107}, {11655: 0.20516967037725956, 9729: 0.25729290837221513, 6570: 0.22191504232989762, 8272: 0.6207342777365257, 6226: 0.26756631196374203, 1326: 0.3647052987339922, 10114: 0.5025402967591337}, {2998: 0.5521344930782849, 11288: 0.5521344930782849, 4599: 0.24667252042005752, 4947: 0.23935055179537218, 10674: 0.29462640957750874, 8803: 0.14832096685530252, 1592: 0.4041721832360709}, {4821: 0.3752319569142602, 11112: 0.5007990277321493, 4311: 0.5278960354741499, 8027: 0.5742186761709374}, {10430: 0.3324413724367288, 4418: 0.3644979222555217, 6018: 0.2716317172073491, 2722: 0.33631603443918784, 9912: 0.38058135858612, 1198: 0.6518355338938043}, {6018: 0.17804558459572978, 3042: 0.409434334603019, 10888: 0.5836697955990234, 8803: 0.1567923567405333, 6675: 0.43441365686464767, 7356: 0.20672440798697167, 9912: 0.2494584622604209, 10480: 0.18160291022927438, 2038: 0.32977215543786553}, {251: 0.22908145651370207, 8185: 0.19175996467281775, 5029: 0.2305569160890487, 11655: 0.11212650608225068, 10365: 0.10018040167296356, 6737: 0.2854342998072921, 2622: 0.19690491838990642, 3900: 0.13153735126667143, 2753: 0.3094674257925342, 8803: 0.09912596741084825, 4680: 0.1841615407335419, 8682: 0.2229358818570278, 9084: 0.11956779204457456, 8525: 0.44135511754226303, 7444: 0.36900289236030315, 114: 0.3392351590764187, 2743: 0.2188427136364455}, {4108: 0.6339940685399978, 10485: 0.7733379087152656}, {7346: 0.20973657282695973, 10785: 0.267512186680068, 4826: 0.2963057074823959, 7348: 0.6690097187629435, 9987: 0.3079869494586896, 8339: 0.5042030968505297}, {9450: 0.390741458463764, 4152: 0.42860637262391976, 10656: 0.5760285105720793, 10004: 0.5760285105720793}, {1455: 0.6022919681110522, 10453: 0.6853088784319356, 6018: 0.409385058705466}, {2835: 0.5207738094032649, 815: 0.3876014428843675, 4770: 0.5207738094032649, 2074: 0.252224856874705, 4325: 0.12051302743708543, 5373: 0.478762605208117}, {10220: 0.44634189341985053, 3241: 0.39038999112183526, 2228: 0.4129886666440608, 5165: 0.4955302265808435, 9084: 0.19757434448651415, 5415: 0.4395783242828282}, {1838: 0.24524646536554673, 4928: 0.22546225358574126, 4621: 0.19410501980842382, 1654: 0.17432080802861835, 4818: 0.10523724686686256, 9223: 0.09182212869417798, 3329: 0.19904561122332654, 1954: 0.24524646536554673, 970: 0.15974052463920393, 9304: 0.14544710380735376, 2130: 0.1472315963621219, 3222: 0.12481081131255366, 6742: 0.10523724686686256, 5769: 0.21388923158822928, 1570: 0.16274778603110637, 1361: 0.1897051610464798, 8964: 0.21388923158822928, 7012: 0.17952473641900943, 2674: 0.12609866513670784, 10480: 0.0763059390251935, 5775: 0.21388923158822928, 7299: 0.19410501980842382, 6018: 0.07481122138799642, 2622: 0.13086682041793934, 11631: 0.22546225358574126, 4771: 0.11496817268920191, 6038: 0.1768043375846712, 2199: 0.1768043375846712, 6075: 0.24524646536554673, 10382: 0.21388923158822928, 3617: 0.24524646536554673}, {2068: 0.6361537916898957, 5429: 0.7715622809071013}, {4785: 0.18075366613467217, 11185: 0.29457389021100966, 2224: 0.3556755059409195, 316: 0.2625738122496698, 2226: 0.47787873740073905, 4668: 0.47787873740073905, 409: 0.47787873740073905}, {5858: 0.6132285092379149, 10260: 0.45398949100496, 2080: 0.6464088006168401}, {6228: 0.5205697719621871, 7399: 0.3148180157810457, 1326: 0.32240384637209507, 371: 0.31018511514838093, 6527: 0.21762271520682822, 2075: 0.5487365262088731, 8142: 0.2850723807632238}, {7346: 0.17504110591513355, 1136: 0.2588040905033576, 7133: 0.20071577963938667, 1691: 0.4301761647257408, 5040: 0.6522361265058768, 5812: 0.08536238343260323, 8366: 0.22418212760053446, 595: 0.24909597259020272, 9084: 0.15126008884566874, 11259: 0.11442413997580353, 7922: 0.22114620701935087, 5058: 0.2171622038883583}, {9373: 1.0}, {9987: 0.6691570798087665, 1696: 0.7431209878221744}, {6018: 0.172687339566065, 5604: 0.5661043738653978, 6371: 0.1820638545414669, 4325: 0.1310030395308644, 4953: 0.46006643625907234, 5970: 0.4937222208048016, 3831: 0.3794379001347791}, {5327: 0.5841590990958491, 11294: 0.4828754668609679, 9777: 0.39490767158389445, 9290: 0.43246862803886654, 4818: 0.287416504629946}, {1837: 1.0}, {3770: 0.4153897917740051, 316: 0.3117941629781144, 2866: 0.5674587258200154, 741: 0.5674587258200154, 11315: 0.188813394035056, 6226: 0.2248697668365795}, {4080: 0.45449160902279245, 8598: 0.3597161029034139, 7366: 0.18637082077712366, 6931: 0.4959048446844321, 1725: 0.5436060304003358, 10365: 0.12338968851343628, 404: 0.2695430824602456}, {8803: 0.22754005403031363, 10055: 0.7787019732710603, 4930: 0.546572007961607, 11259: 0.2076241815118262}, {7917: 0.5798871898494814, 11655: 0.2733532058285705, 6117: 0.2961112383594005, 9515: 0.3048533029836482, 1112: 0.4356993665477507, 11259: 0.16948688111030075, 11545: 0.4356968317653098}, {7346: 0.27315123434418775, 1257: 0.576325148979355, 958: 0.7702192712664172}, {11640: 0.15442109408196572, 4872: 0.35490783914665686, 5360: 0.4029961633258804, 2258: 0.34869730109334374, 5513: 0.5091747998536799, 1742: 0.2525380307257375, 5362: 0.4270237926354493, 1566: 0.2466071424837207}, {4412: 0.2614502913350772, 11640: 0.36134827700595645, 9084: 0.38607413202280977, 7780: 0.6834592431122128, 5812: 0.2178777517760545, 10480: 0.37071644249204394}, {8072: 0.32081170815794047, 3089: 0.48749400550215083, 956: 0.46006385885258855, 97: 0.5343861509230038, 7281: 0.40274325566993135}, {4663: 0.831946752482569, 6758: 0.3750796145508001, 9704: 0.4088763673558674}, {10430: 0.30155201854576347, 7334: 0.4879933100358936, 2995: 0.46577141692551277, 3796: 0.38471749022561424, 7942: 0.5531530975786834}, {3013: 0.3291811196364181, 3392: 0.3165438838765266, 10253: 0.22803366046600085, 1223: 0.3165438838765266, 8417: 0.34699230307202, 11329: 0.2529839402311205, 6295: 0.24809804462611312, 3308: 0.37744072226751346, 3398: 0.37744072226751346, 8310: 0.3291811196364181}, {3796: 0.13384348574076185, 705: 0.18303381063691546, 870: 0.20570294660403765, 10564: 0.0827703589878743, 10365: 0.07629076507463853, 6761: 0.28426068865353377, 9080: 0.20088151884771968, 11555: 0.17138829297870847, 10480: 0.08743283412618498, 1844: 0.1317327759927936, 6205: 0.13769553870267104, 6210: 0.12156656392390529, 4127: 0.11824585086691732, 10430: 0.1364912644686882, 7334: 0.16977321617239974, 2479: 0.12818495452744935, 4448: 0.10398440913626479, 6250: 0.22536310826840067, 5771: 0.11670393197794394, 4412: 0.0616626006682537, 11031: 0.2676257037283193, 11321: 0.22837208257115987, 11023: 0.1997401838941602, 11315: 0.09350126623345395, 8157: 0.22837208257115987, 9729: 0.10708093298529663, 8716: 0.25833905029292026, 6656: 0.25833905029292026, 4785: 0.10628901414746002, 8415: 0.17321899496512877, 7067: 0.09591680098033348, 10549: 0.1687008364920673, 3858: 0.2810081862600425}, {11640: 0.3700170952249321, 11259: 0.29906102032910653, 8258: 0.8795736781879147}, {4269: 0.5352318452121456, 11640: 0.16232360114624697, 10160: 0.19054578472880618, 6923: 0.2110637078793889, 8611: 0.5352318452121456, 7067: 0.1826912128746404, 8358: 0.5352318452121456}, {4537: 0.20629804583780964, 9704: 0.23956348297160046, 8448: 0.48744333882758023, 11511: 0.26551330752092456, 9576: 0.25150887432927693, 8766: 0.48744333882758023, 1728: 0.3004696993988114, 9776: 0.44812092807786297}, {6784: 0.602072783157975, 5564: 0.5789592591691598, 5609: 0.2969207471537148, 7626: 0.46276625839875957}, {2656: 0.26364480760486525, 10785: 0.23629904655914902, 1829: 0.487238902825187, 11464: 0.20224183391768955, 9175: 0.6428057278160738, 11033: 0.39104473111746335, 6128: 0.17388353216353372}, {6570: 0.18322840493309397, 747: 0.21470429126893484, 5771: 0.1779589800937029, 3236: 0.276670603610947, 10785: 0.15751997085744104, 5812: 0.07835739911779664, 10480: 0.13332419674375998, 1360: 0.27435062608463556, 5749: 0.19101042450604844, 3232: 0.2511954624093132, 2127: 0.33145897341381164, 4852: 0.37371416115101547, 536: 0.37371416115101547, 6252: 0.2700113231761056, 6130: 0.1810028719008365, 4325: 0.09916039619749313, 10600: 0.1487552367688446, 2713: 0.24842416484932414}, {10699: 0.8391009146882945, 7356: 0.5439757852784144}, {5777: 0.16230269143282822, 371: 0.14049330473133625, 3478: 0.18964635378266126, 11227: 0.11046787348986438, 521: 0.2485412878596683, 2698: 0.1690710191322463, 2429: 0.23578361447556884, 1752: 0.23578361447556884, 2784: 0.2703506745611603, 7662: 0.2949850043807084, 4043: 0.3067615549187872, 4531: 0.23578361447556884, 11485: 0.2703506745611603, 10299: 0.2703506745611603, 11655: 0.12134506653375354, 6308: 0.17309320592143923, 5925: 0.2485412878596683, 8791: 0.2485412878596683, 9496: 0.1690710191322463, 4134: 0.22673190115817635}, {9515: 0.3955910028416428, 11655: 0.2726417513856211, 11454: 0.46797287953496314, 2171: 0.2310066446249706, 1384: 0.4035934255189974, 3796: 0.4273582678732266, 4947: 0.38895861706514373}, {4475: 0.5433414913829384, 8803: 0.2987534521531637, 11528: 0.5282741232198498, 2528: 0.5800455579561236}, {8493: 0.3884609920520242, 4017: 0.5038251951551987, 5984: 0.7715297987639771}, {1420: 0.36501096126275373, 2058: 0.7023890553952694, 4325: 0.1625408766261738, 8902: 0.5890645774824494}, {6128: 0.10438244171021507, 1384: 0.1025426074308537, 3213: 0.139559156065172, 4538: 0.22796811964410524, 5636: 0.19118740966436729, 10564: 0.06714752104507042, 3040: 0.20957776465423628, 10480: 0.09228200267982246, 1662: 0.1727970546744983, 1057: 0.12638785446368173, 10008: 0.22796811964410524, 906: 0.22796811964410524, 2026: 0.19882009661021272, 1209: 0.13145686971077733, 11331: 0.1269709916503914, 4325: 0.06863510278795364, 10555: 0.15440669968462933, 10335: 0.1436490316406058, 3604: 0.12262962913095633, 7067: 0.07781258280329925, 10483: 0.06872877585395049, 8515: 0.22796811964410524, 3824: 0.14052405054242767, 3723: 0.23474450595796906, 6186: 0.0633508161204745, 650: 0.16434794411418832, 3779: 0.22796811964410524, 4376: 0.10525984896185155, 1841: 0.12116880107530303, 11019: 0.11237734158511838, 1423: 0.18526703771817649, 5809: 0.1436490316406058, 2852: 0.22796811964410524, 2734: 0.10686832166086785, 1400: 0.18042974162034373, 3900: 0.08126310999738695, 4789: 0.12756723413445037, 2887: 0.22796811964410524, 2535: 0.1727970546744983, 11234: 0.11009714505968426}, {7356: 0.08277364257322616, 4437: 0.13860211296851221, 11315: 0.0777617544441954, 4412: 0.075750681266657, 10600: 0.10555376082707792, 10365: 0.08254824374227861, 4087: 0.2639157529090433, 9084: 0.07572721222918423, 8579: 0.23370474494113108, 2740: 0.18992912633363002, 11259: 0.05728557478853864, 10811: 0.11384497303677507, 1780: 0.1440602196543225, 2171: 0.060169743257192425, 4160: 0.11318953599310302, 8493: 0.1407410014070235, 4448: 0.06647058134712314, 3822: 0.09583399180424175, 4998: 0.2038232365121949, 10157: 0.10560872065812042, 11298: 0.11318953599310302, 4152: 0.131808796057409, 6117: 0.10008386713851267, 5812: 0.0427360275663017, 2110: 0.2038232365121949, 9539: 0.17107599358421788, 7862: 0.1807773099208635, 6018: 0.0927510485299024, 10480: 0.07271485030699901, 11381: 0.17107599358421788, 11429: 0.2038232365121949, 895: 0.12676175695781292, 7346: 0.06735663106772519, 4032: 0.14963048864803627, 5058: 0.10872060453957601, 11640: 0.09221351212870624, 6527: 0.08520772534454676, 4325: 0.07036224721293738, 10430: 0.08725013464420917, 3140: 0.1684836213974484, 8646: 0.10089584746968792, 5099: 0.13140265518727612, 9912: 0.09988460382148989, 6720: 0.10152114675833993, 3236: 0.15089580149192733, 4: 0.1347648708708868, 2179: 0.24537490865349978, 6143: 0.16192417717145136, 10638: 0.10927571199282717, 495: 0.13549022908705663, 10122: 0.14726383826395847, 1523: 0.21485161219171894, 7490: 0.15361800362029895}, {5722: 0.4508548023625877, 9084: 0.15890956998075775, 8624: 0.4904171093564351, 213: 0.21392862029515414, 10854: 0.21959373043764538, 215: 0.27950627566372815, 6351: 0.4904171093564351, 768: 0.34401917721511094}, {9769: 0.5056673094821118, 5577: 0.2609119987132421, 6521: 0.33238350636935904, 4367: 0.21484566275351344, 4680: 0.3283381576844121, 6919: 0.2698308939237199, 4421: 0.28857228202315593, 10842: 0.5056673094821118}, {27: 0.1957350870442634, 213: 0.12083373032589391, 215: 0.14942216065486602, 245: 0.21291075327324352, 8211: 0.232311162489071, 8719: 0.13994976813743507, 9515: 0.13865843872490716, 10880: 0.21291075327324352, 9785: 0.17855942081528328, 10118: 0.21291075327324352, 10498: 0.21291075327324352, 6371: 0.0890865475549734, 6186: 0.07697735830730135, 6210: 0.09210702735160478, 3245: 0.15846517947757108, 3708: 0.11187243971317475, 3006: 0.16469256214569764, 10950: 0.21291075327324352, 6226: 0.08437123136377728, 5952: 0.12947169315420368, 2622: 0.11361204848407189, 900: 0.17855942081528328, 8215: 0.15133663391744706, 8892: 0.10006910617195806, 6904: 0.18568796637540727, 4587: 0.27700327640790295, 2479: 0.097121566421129, 1881: 0.11104638827151322, 3518: 0.1957350870442634, 6166: 0.18568796637540727, 455: 0.21291075327324352, 2734: 0.09980963523050673, 2908: 0.1386787588498063, 10910: 0.09229690757953375, 3195: 0.16138375458630316, 4718: 0.14270585578856895, 11568: 0.12553018955958883, 11296: 0.12781913101319378, 10564: 0.06271240605246002, 1742: 0.10559843567947484, 6076: 0.04854210748501773}, {7760: 0.20326312753916007, 6230: 0.2461806032351473, 5812: 0.0649735032156825, 2789: 0.22903799084538037, 4448: 0.1010582119314964, 1063: 0.14631327603102595, 527: 0.18921977514400729, 11315: 0.11822468980296648, 6058: 0.10314624826833375, 7346: 0.07871095476741385, 7271: 0.1812319176116481, 7697: 0.17633234349089713, 7797: 0.2731004107004043, 6018: 0.08330792966009827, 2528: 0.14243909139162297, 2188: 0.2731004107004043, 1018: 0.17208813933724632, 10624: 0.22903799084538037, 10453: 0.13945712630641802, 7610: 0.21125098507151924, 11655: 0.08298524346406531, 9515: 0.1566542654991615, 4730: 0.2525550961322233, 10480: 0.08497241032851978, 5962: 0.16196630489971137, 705: 0.177883105550329, 7338: 0.17633234349089713, 2883: 0.19411934926475827, 8719: 0.1795134278948748, 10430: 0.10195791108563607, 10892: 0.177883105550329, 5149: 0.1315220228397912, 2171: 0.07031257152873109, 9841: 0.17633234349089713}, {7067: 0.17842160285576025, 1992: 0.3435945839525967, 5968: 0.1462873537808551, 7336: 0.2152513056522991, 11642: 0.20579766456713985, 395: 0.2678403576889899, 6186: 0.14526126427403901, 1867: 0.3045416047003275, 4821: 0.20179927815656623, 6923: 0.15843678874891462, 3161: 0.36936479546794104, 8549: 0.35040522757986714, 5709: 0.32651899692996533, 2444: 0.35040522757986714}, {2171: 0.12611489638928483, 7393: 0.37650300742720305, 10630: 0.37650300742720305, 5749: 0.21835313788826186, 5617: 0.24985082170908499, 7392: 0.37650300742720305, 9378: 0.427210504199575, 1177: 0.2578399060226862, 8646: 0.16254522354187853, 10632: 0.2578399060226862, 4537: 0.1593453607779521, 6747: 0.28538468826728214}, {650: 0.340136362327138, 6169: 0.4337445100293826, 501: 0.4114803186801147, 6570: 0.1550654358166231, 6099: 0.3834307548429198, 4869: 0.47180539653376535, 1011: 0.3649549794678105}, {6464: 0.5022272653692422, 924: 0.3442371878182113, 3402: 0.4439696065552022, 4325: 0.12641949799898394, 6128: 0.19226278310797054, 307: 0.39989938893946714, 3372: 0.33004974661990616, 1508: 0.3322052033566002}, {4537: 0.16141170761177825, 8442: 0.30994747185296795, 10747: 0.25562790197924284, 7356: 0.1350792358543584, 6922: 0.28385745730456874, 5420: 0.350618756100529, 859: 0.27495031177598356, 11640: 0.11566548456080973, 8493: 0.22967681900213058, 2623: 0.3632226379842952, 9991: 0.27495031177598356, 3750: 0.30994747185296795, 6071: 0.38138539492518203}, {10483: 0.12820826455880963, 5086: 0.3932650148556429, 1247: 0.4117893305023787, 7389: 0.4825307591747613, 11405: 0.4825307591747613, 3820: 0.4050049958647882, 10480: 0.17214500428078555}, {4046: 0.15833289754223584, 1095: 0.13203625158279783, 2533: 0.29967840648638494, 7780: 0.18698670154317432, 4325: 0.09814236861652405, 9879: 0.2470851145675089, 4898: 0.25215103397798844, 5812: 0.059608883126273035, 4704: 0.12347931544587203, 2418: 0.3259750524458229, 11432: 0.1850894041711038, 1384: 0.19076641256027804, 4367: 0.13849882100458705, 11585: 0.173944682895107, 10910: 0.1413103322696607, 4966: 0.28429585470537316, 5293: 0.2733817605269469, 9729: 0.12421599957777843, 1261: 0.3259750524458229, 11424: 0.3259750524458229, 6593: 0.22323693365084782}, {9929: 0.46372670653947556, 865: 0.43865653396183507, 4325: 0.24754685681644817, 3874: 0.5826861660254551, 4418: 0.43787597839741466}, {2938: 0.30341020413212855, 747: 0.11610916650954617, 9309: 0.30148553760497226, 8616: 0.262937571300674, 1384: 0.13561156348076212, 8124: 0.1930274386320707, 6018: 0.09196667224307019, 5812: 0.05513064892701629, 1702: 0.1439888435324433, 7726: 0.23861651241962226, 2656: 0.1608767475997986, 10288: 0.13372547116268632, 6142: 0.22069272881059668, 1095: 0.12211676934510533, 2488: 0.1858416386920775, 10406: 0.24501378769164844, 7517: 0.2020741131688117, 7906: 0.19146772091962405, 805: 0.2771644787239205, 10033: 0.22438960499637575, 10480: 0.09380415336536203, 4412: 0.06615601687626928, 1844: 0.18387753159642276, 7748: 0.10977131186312965, 10886: 0.2528434198428688, 7156: 0.24501378769164844}, {5609: 0.17218285024091137, 213: 0.1746286340101163, 10854: 0.17925302902717946, 215: 0.21594456891816863, 9988: 0.3076981395004066, 5477: 0.26835582939298697, 1742: 0.15261062061542707, 758: 0.1705909919976331, 2075: 0.28287590547940045, 2171: 0.10306775302391841, 6018: 0.09386179572671374, 3222: 0.20373311678356068, 8621: 0.19700508426191748, 1076: 0.23323143743738822, 5812: 0.05626670598879575, 9001: 0.23323143743738822, 10444: 0.3076981395004066, 7453: 0.3076981395004066, 8909: 0.25805367145839436, 6128: 0.1082906337665106, 10480: 0.09573714114854666, 7067: 0.10502690900672883, 3881: 0.21319108462406322, 9987: 0.13022544330904698}, {274: 0.26148722945782504, 7014: 0.2504632185161174, 8342: 0.28718238215661857, 8501: 0.18876974120886666, 6058: 0.14111597667045434, 6971: 0.2873200705104201, 5273: 0.24084795642251577, 213: 0.1850449417681358, 10854: 0.18994517426148352, 215: 0.22369956206975936, 2622: 0.15324439101239745, 4162: 0.28718238215661857, 3313: 0.1924872604272092, 7328: 0.28718238215661857, 4785: 0.1086243525008882, 6993: 0.24084795642251577, 6186: 0.07980606373139516, 6371: 0.09236023225686375, 5659: 0.28718238215661857, 10279: 0.11861485921248249, 865: 0.11776326605971395, 4787: 0.22214379308394555}, {3902: 0.29418815312695373, 7336: 0.3543634943799537, 6592: 0.2916219141827005, 1384: 0.29752114293791915, 11347: 0.31853981310312185, 8784: 0.6404882313640112, 9256: 0.32035119171987136}, {4695: 0.2759999307140848, 11528: 0.22973296163163034, 1663: 0.44462136145884645, 4785: 0.1829315627071397, 5968: 0.1760928036464728, 5946: 0.3241629942132694, 3041: 0.44462136145884645, 4127: 0.2035101977021911, 8373: 0.3437682084855742, 2022: 0.3827835338317392}, {4537: 0.12423828739058139, 1163: 0.12213878417085047, 1537: 0.29355162027805487, 11415: 0.21488496144986408, 11007: 0.21488496144986408, 9084: 0.09511936034767601, 8489: 0.2698705963121047, 2856: 0.19120393748391393, 8646: 0.12673315431192897, 11240: 0.20338955712193044, 2488: 0.18095101538392488, 1209: 0.1692753230606281, 7661: 0.2698705963121047, 5967: 0.21848455034863487, 5968: 0.10688256131108667, 11241: 0.29355162027805487, 8606: 0.29355162027805487, 7336: 0.1572699914179747, 6923: 0.1157593554720774, 4356: 0.2698705963121047, 3215: 0.25601808531334486, 8892: 0.13797071216433762, 8927: 0.23233706134739468}, {4325: 0.309572389069496, 5086: 0.9508758782963236}, {10469: 0.7071067811865476, 5698: 0.7071067811865476}, {11640: 0.2615479924016637, 1136: 0.4781265841195014, 2171: 0.22203543787508312, 10775: 0.5905998762606459, 7880: 0.5521590963820179}, {1081: 0.381168060374513, 6058: 0.17036583128186655, 11315: 0.11536195967935893, 4947: 0.15029815802345572, 4223: 0.31873898936389666, 3816: 0.27440885052924385, 7338: 0.22385857278964308, 2494: 0.26818871162429586, 6865: 0.2628005822224247, 2477: 0.27440885052924385, 9987: 0.14673546026992296, 10980: 0.3467081929346327, 801: 0.3467081929346327, 7331: 0.29076978579316065}, {495: 0.5021558432898171, 4346: 0.6657493700257942, 4087: 0.42280474080164293, 4862: 0.2828606120047902, 6371: 0.21411051046616567}, {8366: 0.33681814774120244, 6527: 0.2557089037390789, 8491: 0.3982278252544608, 3900: 0.25000789910725413, 4870: 0.4293570774015046, 1526: 0.6447709993191862}, {11575: 0.69589350591013, 8375: 0.7181449912323471}, {10430: 0.14667384454590215, 6288: 0.27925475281164364, 1818: 0.24756126060825867, 6769: 0.24390138431829128, 6553: 0.39287473387767713, 7940: 0.272206360339925, 6758: 0.17712588372977003, 8463: 0.36118124167429216, 9080: 0.21586776840487368, 9752: 0.26610076506413716, 829: 0.29240874055915006, 2179: 0.27925475281164364, 8038: 0.32948774947090714}, {10430: 0.43466151216578525, 2246: 0.9005938984035772}, {9365: 0.5885475671269682, 7067: 0.29971817870019896, 4818: 0.28961215325457423, 4325: 0.1561834181688453, 9366: 0.6749165245879722}, {11259: 0.2440622823937721, 10007: 0.9153643834605257, 6371: 0.320221248209225}, {6643: 0.29917395787765966, 2364: 0.408697749193007, 4077: 0.3757278027682452, 8956: 0.408697749193007, 7263: 0.3757278027682452, 11170: 0.408697749193007, 7435: 0.356441620458974}, {2385: 0.34657556246245613, 575: 0.3895521117080451, 11655: 0.17084407170143873, 4821: 0.28239473105999624, 6923: 0.2217139464396214, 6537: 0.5444336425254017, 10931: 0.4903515558051742, 9084: 0.1821821543450342}, {6960: 0.32627437667861625, 4196: 0.32627437667861625, 5334: 0.1852593611769728, 5952: 0.198408466138857, 5758: 0.1297210792962646, 3190: 0.2428394379543397, 7748: 0.1187969633298454, 5837: 0.2999535840143375, 10483: 0.07560667700782689, 5539: 0.24731199868578005, 2734: 0.15295294399508436, 5968: 0.1187969633298454, 5872: 0.2428394379543397, 1384: 0.1467618602756105, 316: 0.17927373665936308, 11232: 0.19712112668530526, 9411: 0.284556907316478, 2527: 0.2736327913500588, 11536: 0.23883859604744356, 6130: 0.1378208877203601, 8725: 0.17480117592792271, 6058: 0.12322931984678473}, {7074: 0.2760259991153797, 5201: 0.37707561633201436, 10253: 0.22781307896003736, 5468: 0.37707561633201436, 11310: 0.37948284562309886, 1691: 0.20452630366677646, 1728: 0.23243685586598736, 538: 0.37707561633201436, 3497: 0.328862696176672, 8072: 0.20811076783885535, 1780: 0.23243685586598736}, {2171: 0.21839274882720958, 1881: 0.4424196702399864, 7067: 0.2895364866340367, 2130: 0.5092439175504957, 9435: 0.6429686601926093}, {5968: 0.1982566659676735, 10279: 0.22489858957128148, 9242: 0.3819640305001388, 2792: 0.40526740738831435, 6470: 0.500583481638515, 8950: 0.2300051191062648, 3079: 0.5445094579680404}, {6720: 0.2870361402015462, 10117: 0.5008524646636358, 10785: 0.24290129376077202, 9014: 0.6074613954705382, 10417: 0.4196398090869802, 4704: 0.2502980383269758}, {9912: 0.23696951057574092, 11331: 0.3088103479005069, 1395: 0.4505940894734285, 8062: 0.4649932218787077, 9612: 0.509721012444229, 2518: 0.36793727087130745, 11315: 0.18448453702719075}, {8803: 0.06897228548966261, 4537: 0.10866455926888517, 2025: 0.2913334967974061, 11648: 0.22392527287483782, 10932: 0.19461628014301924, 10480: 0.07988634159517692, 4412: 0.05634038550691139, 8142: 0.15953890909774954, 1574: 0.17038418496425492, 7356: 0.11831199987874698, 6018: 0.07832148930086158, 10785: 0.09438417412080757, 7281: 0.17789392504314117, 4448: 0.07302623253857078, 3232: 0.15051346272692262, 8922: 0.1720921841197178, 10331: 0.19461628014301924, 7346: 0.09627578875325683, 7922: 0.15825004465602763, 2385: 0.2059115940825771, 3512: 0.1825002325536371, 6058: 0.0969723748163397, 9809: 0.22392527287483782, 9872: 0.22392527287483782, 2743: 0.1522717257295944, 5919: 0.14045904978266271, 4376: 0.1185510962091349, 6644: 0.17038418496425492, 11227: 0.10491207699118578, 2722: 0.0969723748163397, 4865: 0.13542029116282667, 11467: 0.19860644520374154, 10427: 0.2567538406248204, 410: 0.2567538406248204, 11547: 0.09310697757830076, 1330: 0.2567538406248204}, {7346: 0.1254772074631998, 4325: 0.10074810345922242, 6210: 0.1883420040786093, 7845: 0.3184754338486077, 6018: 0.13280548310179344, 4704: 0.16491564848258253, 434: 0.344576795430813, 11454: 0.295424268251603, 2623: 0.31869375885733287, 10480: 0.1354589178970791, 2479: 0.1985958182016011, 9016: 0.344576795430813, 578: 0.344576795430813, 1783: 0.4353635017260205}, {6923: 0.20706423150930578, 5783: 0.4035953530826354, 747: 0.15543405639228888, 5293: 0.33847868828723754, 7310: 0.35199161714626787, 5987: 0.3121925583719678, 8687: 0.4035953530826354, 6644: 0.26782954881220145, 5968: 0.14694964050910156, 11655: 0.1226378918677998, 6707: 0.4035953530826354}, {5788: 0.5073951788391768, 2399: 0.6931461181115054, 7114: 0.2945008196855952, 11669: 0.4187694576282076}, {4862: 0.36437620864550985, 1149: 0.8576069663386976, 4537: 0.3629604246477293}, {4437: 0.1936825892930515, 2171: 0.10939216036360913, 9006: 0.3110262984592591, 4607: 0.42519786646370766, 9084: 0.13767656126858668, 10480: 0.101611730051444, 7235: 0.2583763756251927, 2314: 0.2738882704083382, 11259: 0.08005086081700313, 2820: 0.24306616659536104, 3790: 0.1854323297999855, 5578: 0.24306616659536104, 8856: 0.2526182084890703, 2629: 0.28482258574759517, 8950: 0.1379495651001153, 7894: 0.2654069565218074, 1891: 0.2475429031625926, 3469: 0.1667656574317718, 9386: 0.13957874295214975}, {523: 0.4273191350351036, 2528: 0.24243096780202972, 2722: 0.22840186513796382, 6018: 0.1417898823684073, 4490: 0.33039063400065516, 8087: 0.34595332114484434, 10020: 0.3350976733602304, 10910: 0.20149803138965203, 10400: 0.2976006106340178, 10688: 0.46481619776131616}, {10781: 1.0}, {4325: 0.07494592514901922, 11590: 0.14039534771260928, 4019: 0.19094740088080922, 3878: 0.17147209091581364, 9808: 0.2977379884743916, 6371: 0.10415746125945378, 7012: 0.23707442400092996, 4624: 0.1198864800158531, 3952: 0.2454852381786039, 3232: 0.18985479128186059, 5476: 0.2563286635852067, 6117: 0.1386946505850216, 10786: 0.23348195535528182, 1629: 0.3238643636222854, 5763: 0.3238643636222854, 11640: 0.09822066877442591, 7893: 0.27161161332649775, 3819: 0.3238643636222854, 6721: 0.3238643636222854}, {3381: 0.40744619459622594, 9729: 0.19104663830824306, 6226: 0.1986749061550416, 5867: 0.28910486170057215, 8719: 0.32954961782224185, 865: 0.26747647018746207, 6570: 0.16477765786620774, 9929: 0.21733805653139956, 2171: 0.16793613868651702, 10632: 0.34334277279256703, 6896: 0.24789735682103106, 4160: 0.24282029090941132, 5601: 0.23264686682311403, 6018: 0.15293617142181162, 3227: 0.27547356383566074}, {11640: 0.4489167038741155, 10160: 0.5269670276751384, 4325: 0.342539692576946, 6742: 0.6351740735698529}, {6990: 0.5367051960185983, 1160: 0.4934087952100195, 5219: 0.5367051960185983, 2492: 0.42478562351222887}, {10365: 0.26662387675862803, 1691: 0.5326804531106392, 9713: 0.4997991232718942, 5888: 0.6287798339583498}, {1248: 0.2417399150337495, 1241: 0.40085585150143216, 1485: 0.40085585150143216, 4479: 0.40085585150143216, 8951: 0.4360307729490378, 6021: 0.2815313526298453, 8973: 0.4360307729490378}, {3796: 0.23104139006368, 11467: 0.2884035501538515, 10812: 0.3251699289014641, 11547: 0.1352039851986189, 7645: 0.2884035501538515, 3489: 0.2884035501538515, 1617: 0.3030039241994304, 1384: 0.16770825955804652, 6896: 0.18435286158427802, 7644: 0.21856600349431213, 2171: 0.09599195593084403, 6186: 0.10361015171484571, 6371: 0.11990890452585347, 5273: 0.3126866824197289, 859: 0.2687907822893963, 1274: 0.22111924033189068, 8372: 0.22672606135709397, 10059: 0.2583261559342311}, {4152: 0.4807020062185756, 7137: 0.5602394130009907, 7809: 0.6745794106980603}, {11259: 0.1795014436207039, 9800: 0.2727271714314855, 11642: 0.1799617240923462, 5514: 0.3839797853517754, 4624: 0.1692070569801349, 5812: 0.08358686687094359, 11227: 0.24300055847391963, 8185: 0.23754155578873387, 10157: 0.20655878837511937, 10480: 0.1422220748693161, 10514: 0.2439148376805172, 3126: 0.2384065962339479, 6186: 0.1270250933973138, 3185: 0.45710021532056594, 10483: 0.10592259401967061, 2019: 0.38335099776493375}, {5183: 0.2349327642079019, 8553: 0.38112465654480343, 435: 0.31963344680729255, 3831: 0.25545313911200007, 11139: 0.38112465654480343, 4376: 0.17597690346427744, 8509: 0.2094116062908145, 5419: 0.2836633949868691, 3676: 0.30973557280573455, 9686: 0.2888878419385371, 878: 0.2888878419385371, 6434: 0.23176309240868775, 6371: 0.12257284563542024}, {2614: 0.5633190922486643, 3201: 0.604528240547717, 7366: 0.2842382224983281, 1908: 0.4862364029558187}, {3121: 0.502565717910908, 9987: 0.41700264672680953, 5098: 0.7573219208523739}, {7288: 0.5744941760145934, 6826: 0.3919158378233277, 6971: 0.47879941731130193, 892: 0.5358260312553975}, {4704: 0.3378960573538274, 9926: 0.7480979198900448, 2087: 0.5711179884060097}, {1663: 0.36383849988301636, 10480: 0.09464677937801766, 9447: 0.30419372852521803, 213: 0.13269468663366213, 10854: 0.13620861578473084, 215: 0.17337090105180528, 9448: 0.30419372852521803, 11296: 0.18262007644985878, 2722: 0.11488976440446565, 1053: 0.19640832061921923, 6408: 0.30419372852521803, 10114: 0.22640525687698093, 41: 0.30419372852521803, 9656: 0.23057513665363422, 5946: 0.2038893089584603, 7918: 0.23530255644333778, 6076: 0.06935396375865016, 6240: 0.30419372852521803, 4325: 0.1039803477050603, 4624: 0.16633109608557953}, {10194: 0.39253109392209823, 7297: 0.36577325040635206, 4973: 0.28583603922243417, 3728: 0.26692559668087307, 11518: 0.3118401782111952, 2612: 0.39253109392209823, 7667: 0.3244726231748368, 5191: 0.450078120416062}, {10378: 0.4245752175158447, 7922: 0.201138044661005, 4985: 0.4245752175158447, 7550: 0.4245752175158447, 1800: 0.3218228372855708, 2722: 0.1603561879755397, 10187: 0.3903244241057534, 557: 0.37028899434090273}, {7356: 0.3017006886897029, 8336: 0.742912948071516, 8841: 0.597542505627715}, {1638: 1.0}, {11547: 0.1495351436231334, 3790: 0.23413976894921404, 6923: 0.24019599266591588, 1127: 0.37909589243113057, 4785: 0.1559722722065014, 9084: 0.1336172198499831, 3912: 0.32412722091090945, 11655: 0.12530156957834676, 5695: 0.4246188908745111, 5995: 0.35963682541438285, 2171: 0.1061667738247325, 3199: 0.4123613872154254, 7238: 0.2598403410614983}, {4704: 1.0}, {8012: 0.36773273601424916, 895: 0.34216089426187163, 8621: 0.4038891778826624, 10387: 0.6308260980392749, 9328: 0.4320075500766174}, {6018: 0.14024971909855838, 4991: 0.23398470045374067, 1312: 0.4226774693161444, 6923: 0.1813049368446925, 10127: 0.3855877106780196, 9294: 0.24533799157946123, 4440: 0.4009813513520428, 4266: 0.4009813513520428, 7621: 0.3148615987375896, 6186: 0.12776623837500842, 2970: 0.2610569172117828}, {7346: 0.18450567024303047, 9080: 0.35174715409266294, 6210: 0.276944063388014, 10480: 0.19918309422311767, 6844: 0.49519161221326546, 1215: 0.6401723174216372, 5812: 0.11706404083240057, 10430: 0.23899866005973838}, {6062: 0.7675600849367318, 8072: 0.42362197851051625, 7489: 0.40958040983658117, 6570: 0.2522693465581007}, {4862: 0.32403047903453625, 5749: 0.4092421718210241, 1170: 0.38532010435649255, 6956: 0.4746561253928324, 9401: 0.26016471656685647, 9403: 0.43300526296240566, 6434: 0.313967889705573}, {2614: 0.5633190922486643, 3201: 0.604528240547717, 7366: 0.2842382224983281, 1908: 0.4862364029558187}, {6570: 0.12808659186626214, 4473: 0.31672008881861347, 2174: 0.2770116560280195, 4325: 0.09018544072265065, 2081: 0.25384225650116304, 10480: 0.1212570935919007, 6076: 0.08885310339894283, 7853: 0.33988948834546995, 10462: 0.3897189915203658, 5132: 0.31672008881861347, 1048: 0.35828007536164064, 4278: 0.2668905856437176, 9929: 0.16894335861083365, 4842: 0.33988948834546995}, {11655: 0.3272921383212721, 4599: 0.4812081190905014, 8509: 0.591821524482278, 3615: 0.5577328083176958}, {237: 0.44135553318220705, 10533: 0.4288565428545022, 9322: 0.6451456252824788, 5096: 0.45285150011810016}, {11640: 0.17620035005018822, 10099: 0.562588179446127, 4108: 0.3660965437019106, 7741: 0.4252933422072879, 9081: 0.5809878404522365}, {5777: 0.219725877324828, 4325: 0.1101932810147598, 10019: 0.20781655277114358, 213: 0.23583180093079223, 10854: 0.24207693599281305, 215: 0.2850954480908985, 7075: 0.36600156552483903, 8466: 0.31920457444111405, 11616: 0.36600156552483903, 10499: 0.2896789603887738, 8201: 0.36600156552483903, 8893: 0.33647595147249876}, {1303: 0.25116458716735546, 652: 0.2879865749687442, 7394: 0.2879865749687442, 4145: 0.22793249947781402, 1632: 0.22793249947781402, 10430: 0.10751543554700563, 6925: 0.2116929113105221, 10195: 0.3444535294368808, 9199: 0.19111051167642532, 7133: 0.12382657127390856, 6592: 0.12697107738930447, 6923: 0.11356483153257908, 10052: 0.3746791723828701, 6128: 0.10135338734984721, 930: 0.2647544872792028, 7675: 0.17182613652103718, 1946: 0.23404333785951117, 6499: 0.25116458716735546, 10333: 0.20761675624859036}, {8486: 0.44980797245952997, 881: 0.5815013926369397, 7610: 0.44980797245952997, 5280: 0.5071505754556462}, {6119: 0.5802093924348453, 763: 0.5060237705085253, 2737: 0.5060237705085253, 11329: 0.3888919493780478}, {9765: 0.26326033555611505, 11454: 0.2675754107055599, 7721: 0.347314737433779, 8835: 0.347314737433779, 1921: 0.22622237710910276, 6130: 0.14670850318913317, 6058: 0.13117597312548707, 7114: 0.1475655308318135, 5574: 0.29127846951533637, 526: 0.23524220159689374, 595: 0.18533182651379979, 6234: 0.2223697849608121, 1053: 0.22425019949094202, 5668: 0.347314737433779, 6186: 0.09651644318270428, 6839: 0.3192966034745577}, {9084: 0.19323557680801243, 8366: 0.28639387341056133, 213: 0.2601392750825645, 10854: 0.26702810390639037, 215: 0.33988233940775203, 2791: 0.5201023451795724, 814: 0.5963519567417116}, {10365: 0.09929208695368756, 10453: 0.1867583374392531, 1482: 0.4178824934939741, 10122: 0.23045717093741355, 9398: 0.2533995325479311, 11432: 0.20766286149233706, 9385: 0.27220612361183705, 4534: 0.3657308881033314, 10552: 0.256553935021902, 8558: 0.2322687189950965, 8559: 0.3657308881033314, 3878: 0.19363859423791321, 5771: 0.15188964156348972, 2081: 0.23821767973417454, 11331: 0.2037004718561273}, {10785: 0.37178310176312257, 3878: 0.535473145689391, 4704: 0.38310450971116405, 4818: 0.4339844174808622, 1566: 0.48983088460372215}, {9886: 0.26206450869099873, 4624: 0.115672489475923, 7896: 0.12701113817512683, 7151: 0.1608407065331782, 9432: 0.19845046489313148, 3878: 0.1654448744283021, 6527: 0.11392905058566609, 9552: 0.26206450869099873, 10674: 0.16674385216648188, 11257: 0.17485902757510416, 4012: 0.1887875022672773, 10453: 0.15956638090500053, 4638: 0.28727254594373264, 2165: 0.26206450869099873, 4376: 0.1442818366099633, 8608: 0.26206450869099873, 2132: 0.2368564714382648, 4325: 0.10681298345424305, 2357: 0.225275102080572, 6186: 0.08683626464175147, 11545: 0.15134280747175954, 1917: 0.22874129602964252, 4561: 0.31248058319646654, 7150: 0.26206450869099873}, {5914: 0.5077731293297342, 7092: 0.8262168320519627, 10365: 0.24399220390149343}, {4052: 0.6282072315069499, 6158: 0.5167147970506312, 3157: 0.4521974013730302, 11590: 0.36589479769843286}, {7637: 0.7250400250206795, 3822: 0.3756470824317048, 6252: 0.5772401853462389}, {859: 0.21080002248614949, 11545: 0.1416182566271946, 5950: 0.29240210460870564, 6371: 0.09403893822194001, 10586: 0.1734134646920207, 3642: 0.2688138129127709, 11019: 0.1875307285218385, 8988: 0.19404069732451337, 6878: 0.29240210460870564, 5896: 0.21404349571358183, 5419: 0.21762898902044808, 8509: 0.160662380030838, 1384: 0.1315257338247788, 11535: 0.2688138129127709, 11528: 0.13889434633845826, 3472: 0.25501554681457683, 5949: 0.29240210460870564, 11464: 0.11969032892984297, 3564: 0.29240210460870564, 2192: 0.22618139781319277, 2697: 0.22618139781319277}, {8803: 0.1458344086154869, 11590: 0.3061816784827179, 10861: 0.24145706749619, 3636: 0.29445779548625095, 9515: 0.3114025809651362, 417: 0.34208255084757533, 8532: 0.49908374261249283, 10253: 0.3279839111687061, 2104: 0.4114949928092048}, {10365: 0.1796383528451617, 5919: 0.36197522211441874, 7346: 0.19070360069241757, 4418: 0.2708473758377249, 6100: 0.6616770413348696, 5132: 0.5377372308269094}, {1018: 1.0}, {4862: 0.21692624168865754, 4011: 0.5105642235683249, 20: 0.5105642235683249, 5248: 0.4149296024946629, 4160: 0.24728007801011795, 7014: 0.44528343881612326}, {11347: 0.30577394754865944, 11259: 0.11962302733611203, 9546: 0.5537458914131373, 9713: 0.24836276918556746, 10634: 0.24057011794502595, 3337: 0.4725639672866384, 11019: 0.24057011794502595, 10892: 0.3178698915048045, 1274: 0.2894272146116696}, {6018: 0.159687072424668, 4689: 0.32498696041483155, 7356: 0.18540878497558597, 8965: 0.4565537707016562, 4046: 0.2542685030491953, 5784: 0.6810720845801166, 6094: 0.3123365097704075}, {10430: 0.19030307218982426, 4325: 0.11795927253120532, 2711: 0.3212002016773588, 10639: 0.5097382500210084, 5809: 0.3212002016773588, 3157: 0.2730917653606645, 10656: 0.3863753773820315, 5342: 0.5097382500210084}, {6947: 0.5594786515125194, 4368: 0.4385033687918423, 10480: 0.20756490820975781, 11259: 0.16352196315505235, 5812: 0.1219902069742043, 5547: 0.48833728949839317, 11561: 0.41415080779045227}, {6570: 0.1489323303641022, 8478: 0.31032629015269003, 3279: 0.4944351828465892, 1047: 0.21583166038690818, 8803: 0.12172913991456695, 1623: 0.4531446793031894, 4187: 0.25859886874454635, 10211: 0.2833861349467917, 9223: 0.16966079000584378, 1455: 0.20336488097393893, 9615: 0.27377077735920863, 4624: 0.16774281656666273, 6876: 0.22337857807939415}, {6355: 0.7407807639195162, 4127: 0.671746871825108}, {6186: 0.1846618670595931, 10365: 0.13866434805144467, 10784: 0.28447375812411135, 5530: 0.5107540461384257, 3227: 0.3651176498145116, 10157: 0.23080439122973284, 6049: 0.4790593091937008, 6570: 0.16786645371465253, 1136: 0.28316726106640555, 9223: 0.19123017200550096, 7366: 0.2094420422826367}, {11590: 0.42247544568763074, 7748: 0.3548414047309604, 8105: 0.7253509254149967, 6130: 0.41166504621986605}, {9461: 0.19742033231497425, 6018: 0.08984867339717942, 11014: 0.25688210078104656, 11547: 0.10681025723727176, 2076: 0.2707813615387338, 7114: 0.12514381587528664, 8129: 0.20407596067466097, 4376: 0.13599918514387832, 5052: 0.3522946736286257, 5272: 0.29454230474543813, 3481: 0.2123429465275199, 9734: 0.20171089918837665, 10038: 0.25688210078104656, 10097: 0.23937110315276822, 11567: 0.25688210078104656, 6180: 0.12121523541657674, 6130: 0.12441700854493618, 5048: 0.29454230474543813, 11268: 0.29454230474543813, 11640: 0.08932795763907563, 407: 0.1698465971456037}, {3126: 0.3454263245909996, 1593: 0.3315839505935493, 11347: 0.31895154980604185, 2441: 0.4353345911339114, 69: 0.5776100667858308, 6715: 0.381907124016547}, {4862: 0.14875488702885034, 4537: 0.148176899817693, 6715: 0.2982186822088689, 861: 0.3849124226136059, 7831: 0.2562896517156854, 5856: 0.3053484314358718, 3363: 0.3053484314358718, 5373: 0.321870086535911, 3641: 0.3501140424812422, 7968: 0.321870086535911, 3092: 0.27710447549054057, 6058: 0.1322332319288112, 464: 0.20642396611945787}, {7346: 0.11898956647289888, 1534: 0.2643313166517483, 2518: 0.27397339253963504, 11515: 0.36006608651943506, 5677: 0.4128535801507651, 6759: 0.3193540620223734, 10807: 0.31293797910535526, 9749: 0.4128535801507651, 2588: 0.4128535801507651}, {3902: 0.29291971803357625, 1057: 0.365125568612684, 10201: 0.60545533281398, 10430: 0.2458722965594312, 6069: 0.3668102106835545, 2130: 0.39537507391347937, 9729: 0.250959821386403}, {4710: 0.2605067058060621, 2722: 0.17985280376511242, 1163: 0.19813228146434053, 1170: 0.2731570957389798, 10332: 0.304886850426395, 4464: 0.47619642399490725, 8646: 0.20558522152857264, 1049: 0.35442339400343303, 6291: 0.31917607337773435, 10130: 0.3609510822886514, 3764: 0.25721338293054635}, {11640: 0.09478251298242621, 8185: 0.16241145831504267, 8379: 0.20356393851226084, 5601: 0.14502386584719631, 9929: 0.13548089249731754, 8015: 0.21168033716781273, 5968: 0.113791773459652, 9294: 0.16676898642358964, 2674: 0.16069272950207386, 7896: 0.1270302832923888, 3915: 0.16880912815762578, 5590: 0.26210401117732535, 6210: 0.13520217088657188, 4448: 0.08888949570396028, 6419: 0.3218176171821971, 10480: 0.09723980508345363, 4412: 0.10129954816945493, 1844: 0.14650868393622368, 2856: 0.20356393851226084, 11315: 0.10398890753644481, 4325: 0.07232248784347964, 2618: 0.2473560319552489, 5945: 0.20542962910987855, 2262: 0.312527685186838, 10600: 0.14115448337328967, 10634: 0.15406114893554934, 4818: 0.13410816384127525, 1420: 0.16241145831504267, 6193: 0.22877577551701714, 11568: 0.18426340126548, 7905: 0.16951542204340359, 6451: 0.1630029021141801}, {7346: 0.21668188348214884, 2677: 0.6109895962687113, 4862: 0.3194269004490866, 4198: 0.6911636057342104}, {6371: 0.2849321346476829, 9730: 0.49345164763984367, 11613: 0.6214858551454794, 11528: 0.4208412317386016, 6058: 0.3346149124134593}, {9205: 0.35392199434380983, 6692: 0.4360456376496316, 6822: 0.5939714154234175, 4325: 0.15760291422184394, 6158: 0.41693043548732406, 3157: 0.3648721897589084}, {6643: 0.5361572097956401, 6018: 0.17173044225921777, 11561: 0.45470609415804186, 4325: 0.13027712380345274, 7468: 0.3773353539627951, 1884: 0.5629674690285553}, {11486: 0.522118042799661, 8660: 0.8528732317191409}, {4448: 0.07273796032068157, 11296: 0.11800764030639914, 10564: 0.05789855554113921, 2246: 0.24359424649882003, 3294: 0.14389092453585522, 2038: 0.11106024454675414, 2972: 0.14899581863493375, 3685: 0.159748173965622, 4039: 0.1903423478329251, 2470: 0.11589439968585746, 6018: 0.08857109173699895, 9929: 0.08521212892547605, 3912: 0.15450743246987472, 11547: 0.1052914944040992, 8774: 0.1807103174944673, 11259: 0.08186067445174806, 865: 0.08060535785160007, 9256: 0.12386195232425211, 797: 0.14899581863493375, 10533: 0.11395996551271781, 4276: 0.14171049686228065, 2255: 0.159748173965622, 10910: 0.08521212892547605, 9729: 0.07490400454532843, 7512: 0.1807103174944673, 8042: 0.1378887713138549, 2760: 0.12483631667603433, 11626: 0.1807103174944673, 661: 0.1292071207839902, 2280: 0.159748173965622, 747: 0.07570279005650665, 3517: 0.16485306806470051, 6761: 0.13461502825485971, 4745: 0.10215039087663237, 5400: 0.16485306806470051, 8124: 0.1258532474325139, 8043: 0.13971992235393826, 10179: 0.1714344212134718, 5215: 0.10252238566006848, 3900: 0.07006984940622508, 3222: 0.1000371502560907, 3697: 0.14171049686228065, 4390: 0.12292878100700992, 1837: 0.14389092453585522, 11655: 0.059729706581222536, 5968: 0.07157052992468597, 5194: 0.1378887713138549, 2331: 0.1807103174944673, 4376: 0.0907612539750952, 2905: 0.14630127550270955, 5419: 0.14630127550270955, 7045: 0.13971992235393826, 11098: 0.159748173965622, 7067: 0.06709460121710724, 7346: 0.056653231788388075, 10480: 0.06115999573610331, 10160: 0.06997924661408089, 8682: 0.11875777882509295, 2425: 0.1378887713138549}, {10157: 0.18711409518926841, 3900: 0.14760245273810266, 5315: 0.4140704632968781, 9446: 0.3611273788836973, 1273: 0.4140704632968781, 1940: 0.34726372889263607, 10181: 0.27753544108556816, 5616: 0.3081842944705166, 2792: 0.3081842944705166, 11518: 0.2868919887665851}, {10686: 0.8402328617308403, 11655: 0.2927467070839342, 7922: 0.45640782591791296}, {17: 0.5865910403492497, 2688: 0.38557601636502675, 10785: 0.21563420728311833, 4930: 0.2909345596320374, 11240: 0.4064242322877254, 8339: 0.4064242322877254, 6527: 0.21386852144675547}, {4046: 0.16425019710707325, 4117: 0.22665395492587442, 7133: 0.14539876192672224, 11259: 0.10784106253131161, 5812: 0.08045129403221908, 11020: 0.2835987261589254, 10480: 0.10521428012390274, 6076: 0.1003061229418224, 4448: 0.09617917572996393, 9990: 0.2342951359154446, 1837: 0.2475372972020321, 2441: 0.2222765720721717, 10061: 0.1776824837921515, 1871: 0.25631930824174587, 6058: 0.12771743460114152, 7970: 0.16425019710707325, 4771: 0.2062439819964483, 8803: 0.09083992609236323, 8491: 0.1920066618462849, 8381: 0.31087814407610487, 1604: 0.3381575619932844, 6287: 0.22903989032456637, 10422: 0.3381575619932844}, {7346: 0.20926813567592376, 7222: 0.5130669685046815, 6188: 0.39668834627574234, 7175: 0.5580883652160348, 1593: 0.2794138181770199, 6262: 0.38219469380328813}, {10279: 0.5775594365739394, 10453: 0.5488408509437099, 7845: 0.6043168188622406}, {9615: 0.3955266422103562, 1403: 0.43444806849258477, 940: 0.3859900649223539, 5482: 0.6546746704273495, 6570: 0.21516797780132182, 8803: 0.17586653489459372}, {2184: 0.6361537916898957, 8810: 0.7715622809071013}, {6742: 0.4841461378189205, 6521: 0.741624959125048, 6180: 0.4643220189026077}, {11375: 0.6724960644474918, 2734: 0.43729494833904514, 5577: 0.4813142968087277, 4704: 0.35335353280716875}, {3995: 0.3680323193307257, 1236: 0.42171473978829505, 4418: 0.24608226041243428, 2674: 0.3091074507918493, 11019: 0.2963509871360826, 5267: 0.5526788279647404, 11185: 0.37057684856396345}, {6345: 0.4588074716598019, 3708: 0.2764202345209875, 2341: 0.35631683465342484, 4325: 0.12173884588165662, 6742: 0.22574130918557547, 356: 0.36902973212135126, 11259: 0.12895021284648953, 3900: 0.18752692847850835, 4074: 0.4163689434642845, 8357: 0.39875536284894225}, {2674: 0.4922190626748805, 5746: 0.6129192490735089, 1499: 0.6181022475728655}, {6186: 0.11148995975041022, 4695: 0.22895355819639987, 11163: 0.4798616665263289, 7336: 0.21494089003413513, 3900: 0.14301348297664157, 9704: 0.19717604006520398, 218: 0.4011969859261796, 1204: 0.2301357128544913, 3674: 0.3498999054170237, 7906: 0.2547925620092537, 5029: 0.19267205800383747, 2860: 0.4011969859261796}, {7067: 0.18460799187518412, 11618: 0.20879623826976895, 4826: 0.16926460489035464, 4862: 0.22979284223149132, 786: 0.41570713137221255, 11547: 0.1507484151575752, 7224: 0.3486363345459707, 4222: 0.33784034762354603, 2464: 0.3625547824406721, 4947: 0.18020922895882668, 324: 0.28468799869200556, 4991: 0.21156163966581334, 6720: 0.18058283199666142, 2179: 0.2954839856144303}, {10365: 0.1621867188973597, 3867: 0.41906301114223266, 388: 0.3735975354192276, 3720: 0.597395971428857, 4457: 0.5492036013674392}, {2005: 0.24290239450075446, 5585: 0.42123323226255005, 11464: 0.17242538042998917, 4785: 0.15932797396123705, 4325: 0.12682205844912603, 2722: 0.15909396636349094, 2711: 0.2654307365601081, 4818: 0.18075459552930226, 1047: 0.20063231917270452, 7259: 0.2238923837508945, 6511: 0.21022859892896206, 370: 0.3192896532182703, 10572: 0.3872520392477901, 6918: 0.42123323226255005}, {6205: 1.0}, {8251: 0.2652101833370751, 5385: 0.338063134870171, 6927: 0.309907568469038, 5845: 0.34149498940646356, 7259: 0.20812050263071358, 213: 0.17080534208676476, 10854: 0.17532849132463857, 215: 0.2231639925703858, 6811: 0.3915598670867774, 9210: 0.23362276239964955, 4964: 0.2681512692868218, 6570: 0.1286916213425822, 6573: 0.3915598670867774}, {11259: 0.12623541036517408, 2758: 0.49868581043856375, 1702: 0.2459607513891603, 11227: 0.21043209917489228, 5404: 0.47345045472193226, 868: 0.43190538935292233, 9412: 0.33851509386211265, 7238: 0.3245129532840226}, {6186: 0.1385255164320244, 10822: 0.3413762988131376, 4603: 0.37784519131051014, 9491: 0.4582714139970562, 3205: 0.4347482518239169, 7346: 0.14366947609376235, 4865: 0.26291688333648683, 6825: 0.26192514540361034, 5557: 0.4051125723295499}, {6018: 0.31281478836360826, 1163: 0.4266702855115842, 10480: 0.31906478365399626, 6186: 0.28497147141668255, 6076: 0.23379989881980373, 4451: 0.6945688643070486}, {4285: 0.2013178551792886, 5616: 0.20469017826666663, 1922: 0.2750177684843303, 9477: 0.3142340846445275, 5379: 0.2528318839016778, 3556: 0.2528318839016778, 9748: 0.2528318839016778, 3557: 0.2750177684843303, 11511: 0.14980382645689938, 667: 0.2528318839016778, 11264: 0.20975216971326827, 9515: 0.12125322984047984, 2786: 0.2750177684843303, 51: 0.2127341162010189, 842: 0.20846011473637283, 2184: 0.20846011473637283, 1921: 0.1791319705966361, 10946: 0.20846011473637283, 9922: 0.21766808879284597}, {1733: 0.572191629745512, 6826: 0.37825750426018234, 2171: 0.17565760486338985, 6018: 0.2081231147155671, 3245: 0.5077998623045803, 2081: 0.4443945028217951}, {983: 0.5391694117679025, 11590: 0.2337301831992883, 10735: 0.3946815151384568, 3838: 0.5426114386310813, 6359: 0.45217902997618287}, {9729: 0.16217508242373135, 1384: 0.19143493361900482, 10514: 0.29546432868477257, 8067: 0.30250839139414837, 2171: 0.10957250263049363, 6434: 0.2588023310239596, 2602: 0.3912569269477286, 5812: 0.07782472118768254, 9080: 0.2338431511604343, 6177: 0.31675771619113924, 10664: 0.27248557536481044, 9412: 0.2797470654615819, 6834: 0.18011999527903855, 4504: 0.28825906659715744, 8153: 0.31153905172730323}, {11227: 0.23834416817341073, 3329: 0.3205012264257147, 6128: 0.20528726975043243, 4862: 0.24783218520996286, 5096: 0.35709207436818496, 9604: 0.3834163075741251, 7978: 0.45120342036447203, 9916: 0.5087239184182888}, {1844: 0.16884155261734746, 3209: 0.262214556354442, 6570: 0.11837406597678285, 7114: 0.15302633527914966, 3401: 0.3601674534584001, 10910: 0.15613275353144668, 1326: 0.19454133726654352, 1209: 0.20768906668730175, 2488: 0.22201433039207158, 5967: 0.26806537141418113, 6210: 0.1558115453392634, 4704: 0.13643139333896626, 5511: 0.3601674534584001, 7848: 0.20154382084067227, 6758: 0.16237994705045788, 8646: 0.15549277981856274, 3952: 0.2730025375670342, 11024: 0.2785998349056124, 5771: 0.1495791227597245, 4325: 0.08334687616167444, 269: 0.2526512811941208}, {5006: 1.0}, {7599: 0.4590647971060051, 9515: 0.24904768669399313, 4325: 0.1307178246939989, 2171: 0.14543220154534334, 5554: 0.3868402317170468, 7936: 0.5193032457035284, 9516: 0.5193032457035284}, {11655: 0.3502192735795647, 7669: 0.9366677428060615}, {4046: 0.5714529034623299, 1136: 0.5013460098964263, 11227: 0.36950036764038074, 6742: 0.3880369296576507, 11259: 0.2216583435799869, 9084: 0.2930156237170105}, {7356: 0.14842743581219314, 2674: 0.21547524334423326, 4019: 0.24708150362585626, 1861: 0.35513159063669614, 1384: 0.18850370334360692, 11359: 0.1766798659744101, 6834: 0.23075330560443122, 2171: 0.10789474073516038, 4818: 0.17982760842813458, 4947: 0.1816683204636004, 5968: 0.15258506189275936, 6527: 0.15279216657757103, 1919: 0.3618172674490066, 11618: 0.21048678885504296, 10785: 0.15405360964833342, 3337: 0.3119075574969416, 1057: 0.3022791423053517, 917: 0.17436477675692144, 11257: 0.2345063838547321, 5758: 0.16661620262344998}, {5924: 0.6480618496406501, 8892: 0.46763448545080716, 3372: 0.601110494882179}, {5786: 0.4113939293362259, 7990: 0.4502275121721362, 5887: 0.48973477473270083, 2960: 0.23017776422937186, 7170: 0.427117245068061, 4930: 0.24289626199970926, 699: 0.3062687289497674}, {8797: 0.4431177465588553, 6229: 0.5953644423312661, 3900: 0.21222777220441774, 9963: 0.5953644423312661, 9223: 0.22290894330401614}, {4616: 0.6755058953077376, 10365: 0.32298692993354267, 6117: 0.6628508342731392}, {5627: 0.35791234691201346, 6292: 0.39234000680093667, 2674: 0.1686599360723663, 8803: 0.08811743918296704, 1384: 0.19196486499024074, 4046: 0.1593275927994136, 9704: 0.2381314876092833, 412: 0.2301024477775025, 6618: 0.20364059831695028, 3780: 0.2665804148316642, 66: 0.3015610770762482, 11642: 0.12914360882543136, 10279: 0.13548321786818343, 3775: 0.17645155774613114, 2258: 0.22463937573696055, 2734: 0.15377264013933564, 1047: 0.1562364871817047, 4412: 0.07197920814432428, 10600: 0.11387360535297024, 747: 0.12632933870981758, 5931: 0.3280229265368004}, {2171: 0.11018320832852031, 7151: 0.16931304172907796, 7475: 0.34779939819621364, 1757: 0.4279615834347817, 8091: 0.35891363538240517, 9223: 0.1602320487340782, 6210: 0.18513987041450772, 10480: 0.1732398023506301, 2038: 0.2417973567898054, 6076: 0.09757213697839208, 5898: 0.39343760940859346, 4826: 0.17425428351258962, 6992: 0.4279615834347817}, {5263: 0.15633753603233858, 8509: 0.14969408946819618, 4448: 0.07748772278221072, 917: 0.10451471344077719, 6130: 0.09345694059305382, 2960: 0.10398768495311128, 5812: 0.031097021713590026, 4325: 0.05119933821652234, 11518: 0.11782470061457731, 8246: 0.14831269943657105, 6186: 0.047257448872272546, 8185: 0.11497612198444547, 6349: 0.11285076529737363, 3815: 0.1382026009899261, 5260: 0.15633753603233858, 9693: 0.08283969755998619, 6353: 0.17005608480405354, 11259: 0.04168405124484132, 5259: 0.1382026009899261, 3284: 0.12890043848890867, 1095: 0.08961656805874137, 9084: 0.05510317399873322, 213: 0.0965123215975803, 10854: 0.09906809431842281, 215: 0.11934647373736994, 5128: 0.17005608480405354, 9386: 0.09456066264989833, 6854: 0.15633753603233858, 1527: 0.1498550934793297, 10279: 0.07023821728123344, 4247: 0.13154324938629228, 4624: 0.10085048633982593, 5758: 0.09987019738543092, 3795: 0.17005608480405354, 5403: 0.17005608480405354, 2438: 0.17005608480405354, 2126: 0.15633753603233858, 2518: 0.11285076529737363, 11448: 0.17005608480405354, 3900: 0.0606193811078974, 1612: 0.12448405221821113, 6499: 0.14831269943657105, 4901: 0.11782470061457731, 4905: 0.08743793823981637, 10940: 0.17980572936667444, 9924: 0.13941454035793416, 4771: 0.0797199555779963, 6134: 0.12656931406908858, 11640: 0.06709948847033022, 11590: 0.0737193894681014, 9426: 0.1345941506648561, 9080: 0.09343850434971125, 4: 0.09806213526045293, 1096: 0.17005608480405354, 2397: 0.09913221652565868, 11574: 0.13154324938629228, 3764: 0.09185432451051394, 6527: 0.06200170292136225, 7615: 0.10209170109536092, 5749: 0.07580465131897887, 10600: 0.05903520126870008, 4583: 0.09754803668646136, 2656: 0.09074421960723751, 4819: 0.18555158037729055, 1248: 0.09428083071629617, 11016: 0.14831269943657105, 2743: 0.1008543180538825, 10785: 0.062513585305633, 2782: 0.1097998640188098, 1430: 0.14831269943657105, 8646: 0.07341722051045259}, {7490: 0.10860673430629957, 3724: 0.0822033995842387, 5749: 0.10879326084683655, 11545: 0.0800241219918566, 3341: 0.16522743781178126, 11294: 0.11911661050682933, 9856: 0.12297544950142036, 6383: 0.16522743781178126, 2488: 0.1018494553462399, 10811: 0.08048751088625042, 3995: 0.10115011614703703, 9071: 0.16522743781178126, 8415: 0.1018494553462399, 3677: 0.13856940123922115, 6076: 0.049010655248454806, 1300: 0.10860673430629957, 7845: 0.13722553306778612, 6923: 0.08476978779223732, 5521: 0.21496585269983248, 1534: 0.10578759222054927, 4422: 0.13856940123922115, 2143: 0.14130061906360408, 656: 0.16522743781178126, 806: 0.10493279778037656, 8167: 0.15279739540931267, 1165: 0.16522743781178126, 4314: 0.16522743781178126, 758: 0.11917926483800871, 4325: 0.049745561771119606, 10453: 0.08437242406596386, 10173: 0.1518984195255012, 4247: 0.12780815271959714, 3173: 0.1472146946101837, 3684: 0.19762440009662843, 7229: 0.16522743781178126, 5228: 0.1518984195255012, 8366: 0.07934932613814444, 1384: 0.07432114770604671, 4818: 0.07090043331005885, 10157: 0.07466454448460938, 747: 0.08278840233356448, 3022: 0.1441014436566008, 7665: 0.1518984195255012, 9023: 0.1518984195255012, 2666: 0.1518984195255012, 11259: 0.04050045602741027, 738: 0.16522743781178126, 3784: 0.16522743781178126, 4030: 0.1518984195255012, 3511: 0.1518984195255012, 11019: 0.08144919669994115, 6210: 0.07147881400914585, 9617: 0.0974165525699852, 4: 0.09527771602001951, 11547: 0.059916639650742766, 1878: 0.11590420914140163, 10430: 0.06168516689563556, 1095: 0.06692540236932167, 4412: 0.03625642958244062, 8803: 0.04438536920712371, 10279: 0.06824384256063239, 1651: 0.12094939748932015, 519: 0.16522743781178126, 1136: 0.09160377949409654, 10483: 0.038287706348976325, 661: 0.10860673430629957, 7622: 0.13856940123922115}, {6720: 0.3635496648421835, 9730: 0.46612796431828296, 8943: 0.6343617411242818, 11259: 0.20514101918331806, 3636: 0.4539368961869356}, {6058: 0.6594683748421122, 6570: 0.3582087712838933, 6117: 0.46674606330988694, 11640: 0.3305398607138392, 11655: 0.3311788816068828}, {8621: 0.4743048294926184, 3329: 0.4070414949146331, 10365: 0.2011212598988006, 1170: 0.5528644294072604, 4152: 0.3211400985292289, 3328: 0.40014041131480244}, {7346: 0.13827542676457893, 11534: 0.29573908369127666, 4160: 0.23236511604913876, 8280: 0.37111505255549543, 7845: 0.26975455773241147, 10585: 0.37111505255549543, 9523: 0.23869305914995384, 1829: 0.36365904922843334, 11298: 0.23236511604913876, 10822: 0.32855937593099116, 4818: 0.2058727669102393, 5924: 0.3124960484453864}, {4421: 0.2005679394756456, 11090: 0.323104393845075, 763: 0.3065193815071237, 10325: 0.27816711564819274, 3228: 0.2533739347316095, 1472: 0.323104393845075, 10493: 0.28562473705243846, 6851: 0.17886329482251323, 395: 0.18008439067579635, 2171: 0.09048621150480839, 2480: 0.24350952073451948, 5649: 0.2406874588555562, 6018: 0.10721011590610208, 11234: 0.16973590388880758, 7729: 0.35145665970400597, 1742: 0.17431375777552413, 5936: 0.22320342849653185}, {7235: 0.4888724881435959, 7356: 0.28473634364164063, 8015: 0.5445156794366605, 4437: 0.47678291833957653, 9704: 0.395107110253799}, {1508: 0.17018779242213294, 5133: 0.18070110156306932, 11640: 0.0848771713506817, 7017: 0.3175591617053757, 8057: 0.2440828902974748, 6649: 0.20829911476437105, 1417: 0.3641149270320505, 6570: 0.09198209012952246, 11655: 0.08504126135095698, 1750: 0.2164848770961731, 5446: 0.20829911476437105, 11356: 0.3641149270320505, 291: 0.27986666583057856, 6688: 0.2572896171516211, 10553: 0.27986666583057856, 11357: 0.27986666583057856, 8739: 0.23471256847266356}, {4021: 0.22621484824991356, 6128: 0.10503279297823842, 10480: 0.09285696257226463, 7869: 0.21213159753313574, 3724: 0.14847950097344279, 6178: 0.21515381328697597, 5917: 0.19804834681635786, 743: 0.22621484824991356, 10634: 0.14711722558751456, 7356: 0.10570205395378862, 3915: 0.1612004763042924, 9490: 0.21515381328697597, 10778: 0.22212382293562308, 3435: 0.26028254977166615, 4049: 0.20213937213064834, 747: 0.11493675004858522, 10064: 0.16945566126280506, 8366: 0.143324344335628, 5812: 0.07100236574391457, 5215: 0.15565595146757935, 6371: 0.0959811859471606, 316: 0.16398064529460527, 11315: 0.09930186600801227, 6624: 0.29844127660770925, 6076: 0.06804244644452116, 2603: 0.21515381328697597, 2123: 0.18663822780948588, 3495: 0.2000336625848027, 4785: 0.11288293588069523, 10402: 0.20935142854282285, 775: 0.2308528702234097}, {4680: 0.29981288039275544, 3892: 0.4754612497822295, 6186: 0.16693954077008355, 3008: 0.4754612497822295, 11315: 0.19988473921554423, 3908: 0.5038094270118813, 2656: 0.32055895335603113, 6018: 0.18325047365585015}, {10160: 0.18585654976469826, 8803: 0.18245906629006764, 2703: 0.43783012723182363, 10645: 0.5220600614015255, 9515: 0.23017228655631725, 7356: 0.18490344701855085, 2676: 0.45530941785293094, 6210: 0.2258476831979424, 2462: 0.3499166983817162}, {4448: 0.11850957680828439, 2171: 0.10727589056029244, 8892: 0.1958366287711535, 9099: 0.2922863539900178, 6371: 0.13400429644737322, 7922: 0.1973926980939951, 4237: 0.383056280258037, 6076: 0.09499757764376598, 10514: 0.222340354721189, 6434: 0.2533778993193175, 4238: 0.24156385253969997, 11480: 0.2625548746247078, 868: 0.3494432632155745, 4382: 0.26461873217892007, 7299: 0.3297809087096328, 2758: 0.3101185542036911}, {5812: 0.05262346135317495, 2373: 0.24134480894864913, 8797: 0.21418499394434826, 4624: 0.10652703417423934, 8892: 0.19978890265839955, 1172: 0.28777481936951815, 1699: 0.19288434821243405, 4628: 0.26455981415908364, 4967: 0.21366325249805193, 11259: 0.07053920082539744, 9713: 0.14645433779156505, 3822: 0.11800620345929412, 1593: 0.14407800998344306, 6448: 0.2535899994834435, 10480: 0.11649188796092257, 11528: 0.13669633288886762, 7785: 0.28777481936951815, 6471: 0.28777481936951815, 5030: 0.1788143874938267, 4718: 0.19288434821243405, 1070: 0.2181298037382146, 3728: 0.17066918360512645, 899: 0.24134480894864913, 7549: 0.24134480894864913, 9523: 0.14317276101480442}, {4456: 0.5611825642706633, 10634: 0.3298554273388987, 8104: 0.6691430893824372, 7336: 0.3584927510360608}, {612: 0.7662108358082612, 1821: 0.6425892584614263}, {9450: 0.2863819827760343, 7346: 0.1605279968498709, 7644: 0.3265099029294604, 1414: 0.5605336098087176, 9515: 0.31949041290221014, 8850: 0.6077296716435794}, {3955: 0.6418056858210174, 7845: 0.392526592520598, 6210: 0.30201439481805187, 5675: 0.5854875243202607}, {9450: 0.4785696536306685, 4418: 0.38099211723477483, 9521: 0.5487670313005143, 5022: 0.5697989457575419}, {9713: 0.3535527376896866, 3754: 0.5268836296600905, 6226: 0.27529704265227395, 3227: 0.3817143237689928, 7114: 0.295166093561078, 3489: 0.5373795614058295}, {8307: 0.3161763297011917, 4674: 0.5031935152749348, 9631: 0.5192735088238714, 2171: 0.1594122250085659, 6220: 0.4789469199223009, 2038: 0.3498305706630588}, {11640: 0.11643499716890998, 4412: 0.06475292176777456, 2413: 0.20700152424064883, 2603: 0.2127387790309819, 2123: 0.18454327208106136, 6570: 0.09698590077178434, 4801: 0.3842017325650138, 5812: 0.05396138953852696, 3822: 0.12100645888899605, 6076: 0.08753159540639278, 5331: 0.2950913666274995, 8155: 0.2127387790309819, 10480: 0.11945364259013194, 5771: 0.12255273853945772, 8971: 0.2950913666274995, 6186: 0.08200391763174172, 3732: 0.2474808937666675, 10785: 0.10847726702563894, 1704: 0.23503255983107166, 4785: 0.11161586023418814, 4160: 0.1429207390399836, 7137: 0.19396844783070577, 7834: 0.2474808937666675, 4862: 0.125377098829775, 9386: 0.1261210347019459, 1403: 0.19582531607106246, 9159: 0.27128613019708353, 5058: 0.13727796490496083}, {4475: 0.35889643364662577, 4448: 0.27183213283769286, 10480: 0.22856362773980765, 6371: 0.23625377620745758, 1598: 0.48286544007672744, 4088: 0.6753402324416098}, {6058: 0.1859050651141782, 10451: 0.4292855827584873, 1856: 0.31260018014097146, 1274: 0.2919190660270255, 8927: 0.38957775445761406, 4325: 0.11390558050328155, 8892: 0.23134630314331123, 6210: 0.21293904868798402, 10101: 0.4922210016004457, 8307: 0.25134977356796523, 8803: 0.13222628866534553, 4412: 0.10800976108976776}, {1204: 0.486849218274851, 6896: 0.4196563421145724, 8307: 0.4333975767157086, 10033: 0.6316905363270331}, {9084: 0.16440285463125914, 11545: 0.3629775892156699, 10365: 0.1792112308974902, 10111: 0.5073701523952979, 5807: 0.5073701523952979, 7129: 0.5073701523952979, 4785: 0.1919085490841836}, {10160: 0.34617041151328887, 4325: 0.332378981791747, 5777: 0.7594831053345009, 10600: 0.43917612795695665}, {9373: 0.7717179932660632, 6773: 0.6359648880790513}, {4339: 0.49249482227549873, 5853: 0.3665539631222988, 1106: 0.49249482227549873, 6315: 0.4527649046604457, 9223: 0.1843937807005398, 9521: 0.2903703427229791, 4046: 0.23921503087537094}, {6186: 0.10444884936967881, 3512: 0.2671603701826918, 2563: 0.3455386640951195, 915: 0.290738103063544, 10514: 0.2609394042374035, 3126: 0.2550467030998606, 2171: 0.09676898624099757, 1276: 0.25192412618684906, 11259: 0.0921304745281108, 7911: 0.26365900129064634, 1507: 0.2848969380425662, 8185: 0.19532315636259392, 11568: 0.22160326316057238, 11346: 0.3278020962352451, 4583: 0.2156015704044953, 8498: 0.3278020962352451}, {1704: 0.2775829909410422, 9709: 0.3684959117163384, 1860: 0.3802715242855319, 5771: 0.1883107660573784, 1326: 0.2449153837419999, 6769: 0.28149377735288883, 1060: 0.45342831150731, 7544: 0.32229614207565455, 11298: 0.21960760873116483, 11381: 0.33191751810544934}, {9681: 0.29587162448985876, 6186: 0.11404894037068126, 10003: 0.4104054860834547, 11680: 0.22076715213642056, 6371: 0.1319898028893948, 6923: 0.1618395923218719, 2719: 0.344190064465315, 5771: 0.170443198003264, 10964: 0.37729777527438485, 7304: 0.4104054860834547, 2428: 0.3579310059663585, 9730: 0.22858280196427716}, {7922: 0.13873144525186215, 11192: 0.2928433238145936, 6220: 0.22652269349867807, 11259: 0.0933900014218635, 10660: 0.17265768583575672, 6024: 0.23177646652346742, 4325: 0.08816729138336929, 9294: 0.15626514580389433, 11315: 0.09743923103841375, 10453: 0.14953872933572482, 7356: 0.1037193687337865, 1047: 0.13948053171299785, 7970: 0.1422401243210782, 8339: 0.20289880832661478, 1592: 0.2143664759511236, 4785: 0.11076555669846146, 1696: 0.13763780932634728, 1231: 0.2928433238145936, 6570: 0.09624705009080714, 435: 0.31952718186677476, 3157: 0.20411927552861786, 7399: 0.15445505344557078, 5162: 0.2928433238145936, 4475: 0.14307143533834213, 10600: 0.1016609583921078, 4905: 0.15057159813567947, 10480: 0.09111521659496499, 9450: 0.15057159813567947, 10957: 0.2928433238145936}, {341: 0.7838548712441655, 9576: 0.6209440722213174}, {3975: 0.5742767774514672, 1759: 0.8186612137386003}, {8646: 0.2209575579763252, 10480: 0.20717912416982068, 1384: 0.23021485209118875, 6537: 0.3809248886977777, 6308: 0.3276843219975642, 11640: 0.1552182643037852, 4325: 0.1184371534364984, 3708: 0.26892341135499487, 4448: 0.14556770868235644, 7133: 0.2200618216811197, 3728: 0.3035326545136747, 671: 0.47051577193423133, 4734: 0.33045530259858685}, {5858: 0.6132285092379149, 10260: 0.45398949100496, 2080: 0.6464088006168401}, {8185: 0.509395324766304, 11264: 0.4416699714083204, 4412: 0.16532644990779474, 11315: 0.25069056836292347, 10480: 0.2344202144380398, 9336: 0.551519681769227, 6180: 0.31006232767164554}, {4689: 0.38154460122452355, 4326: 0.6145895157074248, 10260: 0.396821115397412, 11665: 0.5650101298505066}, {1638: 0.23537119342389273, 10795: 0.427950743060205, 149: 0.5407042393321956, 9084: 0.17520407938419422, 8646: 0.23343476604044514, 6789: 0.5407042393321956, 5609: 0.3025692557541067}, {7429: 0.2979916417967077, 2648: 0.5423384316878427, 11259: 0.13293768937801656, 791: 0.4985875936366322, 717: 0.35648804299389103, 6348: 0.47299499400154976}, {1372: 0.3170163367466723, 533: 0.3170163367466723, 2867: 0.34483437832172975, 570: 0.3170163367466723, 2243: 0.3170163367466723, 1087: 0.2613802535965574, 7831: 0.25242484446880176, 10248: 0.3007438255817616, 4772: 0.2613802535965574, 9263: 0.3170163367466723, 6069: 0.1920617805639229, 5771: 0.1432112294197534, 8189: 0.17432972853106116}, {4325: 0.14949807884382652, 2123: 0.40401032368474443, 6527: 0.23553865543479366, 7067: 0.22050907308416723, 3153: 0.5250187217606711, 3150: 0.5634260377251441, 747: 0.24880022777435198, 5968: 0.23521939064467093}, {6828: 0.3366316071084192, 10279: 0.1797462361171981, 11007: 0.31856645440257475, 1844: 0.20401090052158322, 3213: 0.2664174814613319, 8803: 0.11690582995182887, 865: 0.17845575139269051, 4332: 0.2860573346407565, 11635: 0.3137391095558272, 3942: 0.3490136020871117, 7640: 0.2509502718172128, 541: 0.3536735172261184, 11359: 0.18347458525934837, 5096: 0.2664174814613319}, {6587: 0.43972277389825276, 9523: 0.2886186231371553, 6923: 0.22876435653105132, 3853: 0.5059445287007185, 8207: 0.40694318516488925, 6511: 0.2895248929679277, 3042: 0.40694318516488925}, {4537: 0.394483148188652, 11252: 0.8568968928399656, 10160: 0.3318294152669148}, {8727: 0.8403177259678029, 404: 0.5420941979253242}, {10365: 0.16680271214501258, 1929: 0.6143984472729844, 7575: 0.5648344750464553, 6592: 0.2708835743646497, 8521: 0.44975049543946066}, {3018: 0.3606369518634858, 7259: 0.2421881597142593, 9515: 0.20089483697764027, 10211: 0.28495607916241467, 10776: 0.2367905597913101, 10784: 0.25378538478557755, 55: 0.41889704848274767, 6834: 0.19284446919248013, 10233: 0.41889704848274767, 962: 0.28937785802213223, 282: 0.3120454814048394}, {2074: 0.23461710459459592, 9740: 0.3936814592233394, 10704: 0.4453403110776387, 7218: 0.4844187239308741, 132: 0.4224809049707963, 6653: 0.4224809049707963}, {5968: 0.8985179621172974, 4325: 0.4389367514261923}, {9260: 0.15703109989793843, 5072: 0.22362480631083165, 9729: 0.13641948982925184, 8012: 0.16040524969084885, 10312: 0.17766650662350264, 5812: 0.05031790579724469, 6117: 0.11783993237758576, 8719: 0.18087165970058627, 4960: 0.3544853142351632, 11680: 0.1480189212292283, 6335: 0.2769233877448844, 3733: 0.35800018115925625, 758: 0.19847895778131344, 5617: 0.18260315182847991, 1856: 0.1747531533176929, 2772: 0.2529688367941264, 7346: 0.07930649641283119, 7786: 0.1983746144094201, 11232: 0.16624406169037734, 1384: 0.12377307376293492, 8907: 0.27516673893175747, 992: 0.27516673893175747, 7016: 0.20142690417320058}, {8803: 0.23601175724233614, 9773: 0.42310883746361583, 10861: 0.39076310823607274, 11655: 0.26696509930877477, 10160: 0.31277596343917896, 2717: 0.6659447332857321}, {9450: 0.3582441425754761, 8725: 0.3732783030149196, 895: 0.37791322116085746, 9427: 0.5662332997553614, 9987: 0.2948779781095337, 2582: 0.42653589339547626}, {6186: 0.2661694500774626, 9193: 0.9639262543604988}, {11315: 0.07981635930421024, 2400: 0.2092086115634026, 8923: 0.18182584410252328, 6058: 0.09059922372706851, 10279: 0.09907742204085372, 3728: 0.14226422909422018, 1956: 0.15234299097939877, 8337: 0.22052837668534825, 6060: 0.23987964297676073, 10067: 0.1898573452719901, 1826: 0.23987964297676073, 6896: 0.1186093878363341, 2041: 0.1855537703055165, 743: 0.18182584410252328, 5183: 0.14786654873668634, 9334: 0.19494739410308334, 1731: 0.22052837668534825, 8173: 0.16827144859569312, 11464: 0.09819106264602095, 10365: 0.06512479843413886, 10784: 0.13360533127636706, 4704: 0.09086638343004323, 213: 0.13613944643240444, 882: 0.18182584410252328, 215: 0.15522017971454996, 3689: 0.23987964297676073, 7112: 0.23987964297676073, 2882: 0.17559612781167086, 2722: 0.09059922372706851, 7151: 0.09490279869354211, 6641: 0.23987964297676073, 10754: 0.1458715591014261, 1204: 0.137600417182335, 4325: 0.05551089834717673, 7783: 0.23987964297676073}, {9996: 0.4470532798434328, 7984: 0.6325121994796737, 5810: 0.6325121994796737}, {9987: 0.2003064888180129, 7962: 0.4351060588315802, 4160: 0.22922542867859652, 8968: 0.4351060588315802, 11590: 0.20516987682536458, 9506: 0.3969256970295686, 9708: 0.32411992417238206, 9531: 0.4732864206335918}, {3995: 0.3680323193307257, 1236: 0.42171473978829505, 4418: 0.24608226041243428, 2674: 0.3091074507918493, 11019: 0.2963509871360826, 5267: 0.5526788279647404, 11185: 0.37057684856396345}, {11711: 0.5345756088831394, 10677: 0.5206179154979804, 3975: 0.46142608114641015, 417: 0.41448964424833695, 10785: 0.24180614321913355}, {4734: 0.17545221806748554, 4039: 0.2022485828297341, 3629: 0.22789479638270063, 10057: 0.2022485828297341, 9773: 0.13086555861009758, 3490: 0.1803273370051306, 1942: 0.35353836266989486, 1961: 0.40138892385180974, 1943: 0.35353836266989486, 7346: 0.07831808576210073, 9600: 0.24981604220730416, 7532: 0.22789479638270063, 9897: 0.13355056062671433, 9777: 0.16021376446532312, 7748: 0.0989399319992445, 5771: 0.11285368729080354, 5749: 0.12113033410923668, 11694: 0.24981604220730416, 11706: 0.27173728803190766, 9798: 0.19315044378161386}, {11259: 0.28974573563341505, 4821: 0.5937101902871613, 10627: 0.750703415891678}, {2074: 0.2739397999352786, 8360: 0.5656090954997798, 2166: 0.47435289644897866, 4947: 0.24519179802294808, 2504: 0.5656090954997798}, {371: 0.24856215381822802, 5758: 0.19016667694545847, 9223: 0.1790817924683109, 5002: 0.35012923406943103, 10480: 0.14882038274370482, 5061: 0.2836672208493127, 8987: 0.36998423761054944, 8820: 0.339979950478633, 6371: 0.15382752604830094, 4789: 0.267652839507663, 6758: 0.21564270105565223, 9148: 0.35599433182028273, 6355: 0.22195136244698158, 9260: 0.2729584657919824}, {1691: 0.14770043597859947, 6638: 0.25107208539732345, 10161: 0.1425500764443804, 4418: 0.06957622295566661, 3809: 0.10882654628688644, 1970: 0.1381358240885886, 11528: 0.08073955594015116, 6076: 0.038752820216183946, 436: 0.16997391689754918, 371: 0.08833045207610121, 11555: 0.07968143639261101, 1518: 0.10336166903009684, 6720: 0.07383652808073127, 3157: 0.09106335855491617, 5808: 0.16997391689754918, 4872: 0.11847616098585016, 1829: 0.12883815621779598, 10991: 0.16997391689754918, 1794: 0.16997391689754918, 8892: 0.07988858089593713, 6799: 0.14824103752553297, 9927: 0.1562619966709648, 6447: 0.16997391689754918, 5867: 0.1275201342928962, 6451: 0.08865211964006135, 7780: 0.09750090330709789, 2988: 0.16997391689754918, 4032: 0.10882654628688644, 11547: 0.08019270668743116, 7741: 0.12442390386200419, 11347: 0.08185748722616307, 7421: 0.16997391689754918, 7896: 0.06908775074727992, 8995: 0.09854230792393344, 2174: 0.1208171970723642, 878: 0.12883815621779598, 9773: 0.08185748722616307, 11639: 0.250341093031576, 747: 0.06546095038627091, 482: 0.16997391689754918, 3313: 0.11392695248919653, 5771: 0.07059091302514454, 6371: 0.054665019227067145, 7184: 0.1562619966709648, 4789: 0.09511462606030205, 3801: 0.1107119836354198, 10511: 0.13147969015858627, 729: 0.10974681078657007, 10579: 0.1265081581535168, 1858: 0.13452911729894856, 11654: 0.16997391689754918, 4412: 0.03729796594599369, 4069: 0.16997391689754918, 2839: 0.14824103752553297, 1923: 0.11254314347715406, 348: 0.1425500764443804, 6727: 0.1562619966709648, 8240: 0.16997391689754918, 11259: 0.04166391029411112, 4367: 0.07221775689645688}, {11640: 0.4560042968336536, 4865: 0.7930416663815781, 8803: 0.4039121150101524}, {11329: 0.35645087985039003, 9528: 0.3404928976636883, 10896: 0.5318087678637228, 8682: 0.32129628004984406, 3376: 0.4031045607622157, 3775: 0.2860729477153693, 7259: 0.28266509766357, 7896: 0.2161594688762314}, {4862: 0.36899763210969266, 2656: 0.46343481945048504, 9359: 0.6717973033778158, 7346: 0.2503079791137543, 4537: 0.3675638915680096}, {11640: 0.30344038916243055, 4680: 0.4993474452514213, 1908: 0.701860340199051, 4826: 0.4073918531359016}, {7067: 0.17842160285576025, 1992: 0.3435945839525967, 5968: 0.1462873537808551, 7336: 0.2152513056522991, 11642: 0.20579766456713985, 395: 0.2678403576889899, 6186: 0.14526126427403901, 1867: 0.3045416047003275, 4821: 0.20179927815656623, 6923: 0.15843678874891462, 3161: 0.36936479546794104, 8549: 0.35040522757986714, 5709: 0.32651899692996533, 2444: 0.35040522757986714}, {7346: 0.2526453208353391, 10104: 0.6524311415465864, 2081: 0.5709667022303097, 6205: 0.42953576375486086}, {6018: 0.25403644966016536, 5762: 0.3918582134673026, 2618: 0.5066158194213911, 8950: 0.27038139905362435, 6128: 0.22527383984737293, 5914: 0.3616525236389922, 1500: 0.5201981185721821}, {6770: 0.544574690605473, 10195: 0.544574690605473, 1696: 0.2784125388844403, 4865: 0.3124303110009002, 4698: 0.4814047898221492}, {1084: 0.7112966561106506, 11555: 0.4753446379596051, 3469: 0.5177881247854752}, {7837: 0.3837786162510256, 8167: 0.2727892464801342, 9934: 0.23656871737748084, 1696: 0.18037784614318636, 2740: 0.3118924149719233, 8520: 0.3837786162510256, 8892: 0.18037784614318636, 6186: 0.1066495112871259, 4826: 0.15626418442875686, 7067: 0.13099553306747083, 9264: 0.32185921277131574, 3671: 0.30374894821998905, 4325: 0.08881076981006981, 7544: 0.2727892464801342, 6177: 0.28563868366866246}, {371: 0.2098887658027501, 2400: 0.35224690552638477, 4785: 0.15276730954722145, 10187: 0.37130612209349273, 2353: 0.29565318646231453, 6923: 0.15926950903043335, 5091: 0.4038880680192765, 4325: 0.09346432739866843, 10838: 0.35224690552638477, 994: 0.35224690552638477, 8143: 0.37130612209349273}, {9450: 0.5616003211568719, 10365: 0.29653271464644143, 4152: 0.47348820928535673, 10157: 0.49357353671240906, 6570: 0.35898120834642694}, {4127: 0.20168921786066615, 10786: 0.34554602516706684, 10258: 0.40197673191219674, 10365: 0.13012739664931985, 2171: 0.12340318569482409, 6592: 0.21132374812041252, 11329: 0.32126243058306475, 3478: 0.33622715252334634, 1390: 0.3895289887746528, 1729: 0.40197673191219674, 7848: 0.26821358146159124}, {4325: 0.14543505463892506, 7970: 0.3052607958972344, 3495: 0.4212388654171594, 2209: 0.49722021498080293, 3935: 0.43039371537894006, 6178: 0.4530794818635025, 6130: 0.2654705262801886}, {3961: 0.4514639637509032, 3713: 0.4910797243837649, 9841: 0.31707472873941356, 6434: 0.3885228303889723, 2433: 0.4118482031180415, 4818: 0.2107262916480969, 11227: 0.20065987612097042, 10157: 0.22191377178232138}, {2622: 0.40169929634734935, 4624: 0.2786642927190616, 4704: 0.2851570585688449, 3900: 0.268345056483488, 11077: 0.5602869760826559, 6527: 0.2744642087798854, 11561: 0.4673416014198311}, {10160: 0.3497394414058265, 10480: 0.30566294694609497, 1768: 0.8567879994990752, 6076: 0.2239794854528909}, {595: 0.30628639667026186, 8366: 0.35863221515112004, 1765: 0.5276816615196624, 865: 0.23537100564178648, 5968: 0.20898893140208938, 7204: 0.5739854909979502, 8493: 0.2656848018702337}, {779: 0.563191808877319, 8072: 0.5109468558979943, 3648: 0.6494215093923571}, {3878: 0.19934031852706005, 5702: 0.48983766570335197, 11590: 0.2410852100842428, 11454: 0.19636844849303095, 1923: 0.19160829960186887, 1205: 0.2802212833621976, 758: 0.20873539193989293, 1742: 0.1867345827310357, 3708: 0.19782919334479412, 9765: 0.28538233429625237, 9289: 0.2756045659825004, 8803: 0.10113990371372066, 1880: 0.3764999018745629, 5569: 0.28538233429625237}, {6186: 0.16867061687864746, 10365: 0.16478373204312785, 4045: 0.6069617681943217, 7562: 0.41566458217693375, 11381: 0.44430671524291543, 4418: 0.2484505157041696, 9004: 0.3795797620608821}, {4905: 0.3556716336344418, 6291: 0.46364512216825193, 10430: 0.2582498471059888, 7133: 0.2974288569492413, 6076: 0.15771119713924356, 11397: 0.6917377823553532}, {2171: 0.3357515381760012, 3658: 0.5557139481723583, 3754: 0.5843091858441685, 4046: 0.4868631098751103}, {7346: 0.11650355380169738, 4862: 0.13200815130644547, 11618: 0.20303062018575155, 3121: 0.20618229481158037, 8162: 0.27097248865749674, 5188: 0.31069841413285026, 4872: 0.16645636933622684, 8157: 0.2525009852254436, 7456: 0.31069841413285026, 11640: 0.0942277368277335, 9729: 0.11839468630856906, 1373: 0.24033411695497783, 4624: 0.11501277510216347, 5968: 0.14717997227133808, 2882: 0.22743671685488076, 8914: 0.27097248865749674, 1118: 0.2856341457622874, 6128: 0.10934654408806288, 9975: 0.31069841413285026, 9591: 0.31069841413285026}, {10430: 0.16615568073830012, 11523: 0.4450580064525057, 6720: 0.1933328277113942, 9843: 0.4450580064525057, 2473: 0.4450580064525057, 2731: 0.3442651076061498, 4490: 0.31634654219691766, 4325: 0.10299152294959693, 7366: 0.18250243636912603, 6371: 0.1431343403973601, 3915: 0.2403942358063533}, {4475: 0.2657381592912084, 9401: 0.27407959900431655, 10364: 0.5439216126348199, 6742: 0.23340117436454585, 2584: 0.5439216126348199, 5895: 0.45616450380194895}, {5771: 0.1644950287727001, 7438: 0.3960830530795312, 804: 0.32189208758213955, 6018: 0.1208231765045006, 1876: 0.5153159328306388, 4704: 0.1500362186830234, 1481: 0.36413074341897267, 2171: 0.10197574558583956, 7399: 0.20890703033754848, 7479: 0.34543984045611564, 6158: 0.24247695996173663, 3157: 0.21220110554872612}, {11635: 0.2512221593121654, 6052: 0.18822409475633184, 4718: 0.23356726930350544, 9912: 0.14893569859161918, 10831: 0.23864342319623527, 8072: 0.19232417913488603, 1518: 0.21190686012865795, 497: 0.34847191779615305, 880: 0.34847191779615305, 2885: 0.2512221593121654, 7596: 0.34847191779615305, 6747: 0.2641374641592617, 9206: 0.34847191779615305, 3953: 0.25936062010729993}, {6786: 0.4340867845858874, 7346: 0.12510924161206807, 3902: 0.1930697124520391, 4325: 0.1306919192273781, 1829: 0.3290324891334178, 11486: 0.26574235647697964, 4537: 0.18371623581825808, 10160: 0.20105796119070227, 10483: 0.10058975408871665, 4695: 0.24772298243352228, 10132: 0.36405058761757436, 2171: 0.11176020575214354, 5773: 0.2714677711860963, 1057: 0.24066214802534275, 9929: 0.18817681691728616, 6825: 0.22808781093619565, 8892: 0.20402293386666626, 11259: 0.1064031069175551}, {1638: 0.35851750835117013, 9294: 0.4394845667659227, 626: 0.8236009420709995}, {3902: 0.3682060164572826, 7688: 0.5075508350576933, 3222: 0.4213109479104915, 9007: 0.6552202411767061}, {6018: 1.0}, {1359: 0.37315791457503344, 10006: 0.5013677638454836, 6286: 0.5013677638454836, 2908: 0.32656433810162167, 7132: 0.5013677638454836}, {7067: 0.3314084977800488, 7278: 0.7462779618862097, 6335: 0.5772673654400846}, {7346: 0.11246710326270803, 11253: 0.3272637137182185, 377: 0.26430459019490216, 6226: 0.15463559626827286, 1063: 0.20906150068578522, 3563: 0.28564999661074064, 10137: 0.340328912310171, 404: 0.23142752101104513, 10430: 0.14568379901377476, 3915: 0.2107754930657031, 7366: 0.16001648659488368, 8033: 0.39022283724153484, 7970: 0.18953938972944318, 8130: 0.39022283724153484, 2479: 0.17800441087894037, 8646: 0.16846839748772882}, {348: 0.47160327764771676, 1331: 0.4904308782059271, 8803: 0.1510599411593534, 9729: 0.2787867485750278, 10279: 0.23225921121869947, 6197: 0.3769084208613826, 11036: 0.4904308782059271}, {11655: 0.2512294081053541, 10480: 0.25724535424618394, 2142: 0.826783795161147, 747: 0.31841387186387177, 7356: 0.29283062422734646}, {11259: 0.2147068999167288, 9777: 0.6719023405764438, 4325: 0.20269970573687573, 4127: 0.368582910557211, 5594: 0.5705327853817022}, {7256: 0.47306145450397985, 11047: 0.40726818585700464, 6527: 0.18761083556821706, 6923: 0.13737317844036087, 6925: 0.1968235641456048, 9253: 0.41666663460131237, 2778: 0.3483616416959185, 4325: 0.08061487602810534, 6834: 0.14743524057498705, 5338: 0.2921564647120223, 8619: 0.2757175042710347, 7366: 0.14285070131378086, 6371: 0.11203598874972234}, {10617: 0.5951914437845944, 8012: 0.266680960453632, 4019: 0.2697242275665512, 11227: 0.18692952665354243, 10561: 0.2681808534292296, 4421: 0.2610712842789306, 1021: 0.3467620713415504, 6094: 0.27295204488009217, 10218: 0.2763882486810651, 5577: 0.23604703164936303}, {251: 0.8138665409077599, 2995: 0.5810518510346884}, {11640: 0.17901812950286422, 274: 0.3638595967357896, 1638: 0.25695130399280897, 9833: 0.5902789434416155, 6006: 0.4950424750276707, 8215: 0.41956935945906215}, {9205: 0.2980149931217258, 9906: 0.392727784741449, 8803: 0.15405209604899267, 9996: 0.3534976132409489, 10561: 0.33617726673925513, 6876: 0.28269269124652924, 2479: 0.26159414842451323, 4774: 0.5001452017982151, 1691: 0.3110503277194719}, {11464: 0.3576970376928649, 5133: 0.5642173065302659, 7819: 0.4662980198337542, 8812: 0.5798946602070076}, {6226: 0.14048338604099952, 8919: 0.2191855268396789, 8155: 0.21230771780179317, 1057: 0.11053268002101402, 10383: 0.22622101612670328, 10480: 0.08070535825765009, 8937: 0.1672032242825244, 1293: 0.19936984713080078, 6076: 0.05913816115575019, 10505: 0.1832865357066626, 4537: 0.08437823760397209, 7415: 0.19936984713080078, 4801: 0.16202555447627148, 1544: 0.1832865357066626, 9513: 0.1459422430521333, 6232: 0.1459422430521333, 1355: 0.15111991285838625, 2606: 0.1738784016361216, 11491: 0.1459422430521333, 9664: 0.1832865357066626, 2199: 0.14373073105774672, 1182: 0.16202555447627148, 1095: 0.08075479119114924, 2685: 0.19936984713080078, 9398: 0.13813497221712298, 7159: 0.19936984713080078, 9679: 0.1832865357066626, 9734: 0.13653410898159227, 5952: 0.12123742589361934, 11680: 0.10724591864736972, 4789: 0.11156410820947035, 8950: 0.08421540666463342, 4108: 0.12562846736370706, 1763: 0.1395301570727671, 8904: 0.1832865357066626, 10449: 0.1672032242825244, 7052: 0.1832865357066626, 4301: 0.1832865357066626, 8880: 0.1577950902119834, 2683: 0.12764741963360854}, {4818: 1.0}, {2743: 0.20427473790699116, 5223: 0.34443901682303646, 9260: 0.1965631379339779, 5072: 0.2799215803551292, 10160: 0.12262238007484431, 10701: 0.34443901682303646, 10480: 0.10716869079609792, 6076: 0.07852959758777407, 8052: 0.2799215803551292, 6058: 0.13008985322070446, 2554: 0.2664334389476783, 10664: 0.22052859221862356, 2171: 0.08867944557660713, 1410: 0.34443901682303646, 9817: 0.2416180530971679, 10714: 0.27261286756396985, 5030: 0.16450333270972411, 4044: 0.23329442692816804, 7748: 0.12541073457079727}, {11640: 0.2737463804124896, 4368: 0.5933121588444928, 2531: 0.7569964341913031}, {9386: 0.07622473866590383, 8850: 0.19459756618291352, 4325: 0.04127145544517643, 3095: 0.1251071014916417, 6851: 0.09076415570878192, 395: 0.11889306575147204, 4475: 0.08713297236397802, 7114: 0.09858570460130987, 7896: 0.10707789906139203, 6747: 0.13518457540942586, 1070: 0.13518457540942586, 1793: 0.149571929034273, 3768: 0.1267685130542757, 1068: 0.10454982767725723, 4625: 0.2320343234312156, 6186: 0.06448078696388786, 10860: 0.13795622826986936, 5777: 0.10706886203951835, 5823: 0.1449402356913552, 5946: 0.11953886295847621, 6076: 0.040661739508183255, 4472: 0.15554322030396997, 6919: 0.09516816965304303, 4412: 0.0391352207922076, 10600: 0.061913277497546444, 2171: 0.04591721626659009, 8922: 0.11953886295847621, 5812: 0.042430568728856734, 7133: 0.07668431235614144, 4624: 0.06601946014563362, 10483: 0.04132778265403407, 4826: 0.07261788563617454, 6558: 0.1783466362839673, 11640: 0.054088463743144535, 8726: 0.18857159421306025, 8923: 0.13518457540942586, 9966: 0.11835245069912552, 7223: 0.12213681971135786, 2209: 0.10845314580428009, 10785: 0.06556123924737527, 9987: 0.07548069614742117, 1420: 0.12058142249087091, 5226: 0.1783466362839673, 8234: 0.13273980432397267, 3217: 0.263439607162805, 9821: 0.263439607162805, 5720: 0.10027704639602376, 2179: 0.1267685130542757, 1742: 0.08845549372564251, 11547: 0.06467407157473098, 9219: 0.11953886295847621, 4068: 0.16395928265912016, 5895: 0.149571929034273, 10348: 0.1251071014916417, 10365: 0.04841923472649688, 10784: 0.09933340373136053, 3900: 0.06357468906018042, 4519: 0.16395928265912016, 9084: 0.057789556560518825, 8724: 0.13273980432397267, 8833: 0.1783466362839673, 7860: 0.15554322030396997, 9401: 0.08986804977226832, 2475: 0.10177817481681374, 2722: 0.06735905807241277}, {1384: 0.7492668860056182, 5758: 0.6622681734278374}, {10627: 0.4384765109728223, 541: 0.5611013190696978, 6117: 0.2956744494805393, 1705: 0.5340644113546238, 4821: 0.3467787241342308}, {544: 0.6838824458324029, 9939: 0.5735438513847916, 1593: 0.3423941749803395, 4818: 0.2934595027606419}, {6186: 0.16012448522909772, 10138: 0.502534419799221, 5968: 0.2097983268228195, 9848: 0.42179477005938154, 2948: 0.5297253253518658, 2575: 0.46827793012369867}, {246: 0.5155694917133267, 11430: 0.4739781236336296, 1213: 0.4189973256642767, 6128: 0.18144843871647817, 4962: 0.5155694917133267, 7748: 0.18771958320645304}, {11219: 0.4855784498738242, 9515: 0.2170913094486169, 1047: 0.2345244591253512, 3822: 0.2019119042065261, 1947: 0.49239073928542837, 282: 0.33720310180614743, 4818: 0.21128885877275186, 6511: 0.24574180600511575, 5504: 0.40016022332284523}, {10480: 0.07411928513250239, 9897: 0.1170771430954597, 5673: 0.18854266774805745, 9087: 0.1805667811457058, 10552: 0.2174102356316003, 2441: 0.15658502443101066, 3512: 0.16932540035364058, 11267: 0.23821858332895648, 2650: 0.17730128695599223, 3213: 0.1458343583319093, 6076: 0.05431210917770051, 4159: 0.1404514495494888, 7860: 0.20775993514247434, 10430: 0.08893530799060044, 6521: 0.15658502443101066, 1279: 0.23821858332895648, 10392: 0.2190013159345396, 8023: 0.23821858332895648, 3804: 0.15516293547006324, 3173: 0.1631388220724149, 9420: 0.1430124681078183, 10738: 0.23821858332895648, 4284: 0.23821858332895648, 5058: 0.11082046450793882, 7235: 0.14486146354859447, 4448: 0.08815052316595759, 4127: 0.10024035047853501, 6002: 0.1997840485401227, 3902: 0.1059529914690451, 2397: 0.13886675216715844, 4475: 0.11638399058302794, 10533: 0.1381071250229199, 11116: 0.2190013159345396, 9854: 0.18854266774805745, 4862: 0.10121324526184987, 7133: 0.10242765792313066}, {5577: 0.23364609295449584, 1969: 0.25841581284951204, 6371: 0.1456319190066637, 1963: 0.3370277025855829, 4421: 0.25841581284951204, 4771: 0.27617955846061243, 8803: 0.12164297079570342, 7623: 0.37976463706908525, 6210: 0.19589552684470052, 2622: 0.24163294304063923, 5771: 0.18805975509826675, 4934: 0.32186653379868674, 10099: 0.3370277025855829, 1248: 0.2510502012899339, 4704: 0.1715296489621461}, {11464: 0.2926233824016967, 6180: 0.2941981843673776, 5968: 0.2602872112984713, 10552: 0.5014729357519623, 4160: 0.3462335982748607, 3734: 0.6234715247348065}, {4238: 0.4526927086783092, 1499: 0.6559337662343084, 9307: 0.6040034816316634}, {6058: 0.22750534873092643, 6180: 0.3225205314252773, 4127: 0.25347052689108274, 6186: 0.16739351377667358, 1384: 0.2709510061722007, 9523: 0.3899019869989334, 7399: 0.3177073947564212, 1361: 0.46594742741799294, 11490: 0.4565861645289188}, {9897: 0.4740479252639647, 9401: 0.4860334572006032, 2528: 0.5030748575657276, 1248: 0.534757637364613}, {11655: 0.16904755733575233, 2171: 0.14323231421348262, 10638: 0.1999397689488473, 4046: 0.20769682936204517, 8982: 0.42760529173075695, 8344: 0.26002811164275635, 6069: 0.30985661701572353, 11259: 0.1048143670578339, 10811: 0.20830006219913408, 8506: 0.3931100600169902, 3516: 0.3586148283032235, 817: 0.3586148283032235, 7985: 0.23495071615568267}, {8430: 0.7371254684100929, 2734: 0.510180981935135, 4826: 0.44312685485411796}, {7356: 0.19162645538762682, 9223: 0.2025701869374737, 3928: 0.3556358353015534, 6371: 0.17400356718702958, 8803: 0.14534115176159665, 4577: 0.5680591372396846, 9773: 0.2605595859969345, 8209: 0.38457177122367964, 1479: 0.45374943894329123}, {9693: 0.46961417931251315, 4325: 0.2230898105475991, 10600: 0.3346676582718592, 6254: 0.7457122240205242, 2171: 0.2482021282577263}, {9313: 0.10900938944972259, 10480: 0.07107987177002348, 5386: 0.10749492457226552, 2245: 0.11541927144527295, 2550: 0.12853614860190427, 9730: 0.09779893741941824, 2171: 0.058816838886414015, 4412: 0.0617284514586242, 7832: 0.22844994155014742, 5720: 0.09872800303522333, 3219: 0.14270125161793273, 2551: 0.1564489799502257, 8274: 0.15314044517856565, 4448: 0.04994197223517968, 7615: 0.10541490117956476, 4049: 0.11893119021370908, 7748: 0.0639331514450318, 3469: 0.08966482399057193, 7316: 0.22844994155014742, 8509: 0.09648003311445187, 10483: 0.060103127493710315, 2795: 0.16142649926179442, 4325: 0.040633909004135105, 7970: 0.0852885120015503, 10279: 0.0725245505196746, 10050: 0.1755916022778229, 9083: 0.15314044517856565, 2622: 0.09369804636302938, 2516: 0.16142649926179442, 8307: 0.08966482399057193, 1403: 0.11652418506327997, 2722: 0.06631851982914658, 9848: 0.12853614860190427, 9386: 0.07504724661161205, 1871: 0.1731622698035774, 5538: 0.1755916022778229, 6919: 0.09369804636302938, 3774: 0.14726139624576598, 8493: 0.10574427125760595, 11123: 0.13897534216253718, 6130: 0.10955998482905982, 6851: 0.08936206402526661, 3772: 0.1755916022778229, 4624: 0.06499961552418018, 10365: 0.04767127198942238, 2802: 0.16142649926179442, 9993: 0.09369804636302938, 4165: 0.16142649926179442, 8892: 0.08252892078538278, 6117: 0.07519696100923816, 9912: 0.07504724661161205, 316: 0.09648003311445187, 1833: 0.14726139624576598, 3878: 0.09296811434601569, 3015: 0.1755916022778229, 5820: 0.16142649926179442, 11547: 0.08284309801914409, 7105: 0.15314044517856565, 2970: 0.09970132621867803, 7431: 0.13897534216253718, 10905: 0.15314044517856565, 2207: 0.12853614860190427, 11234: 0.11032984082594, 10288: 0.07788456432553474, 4221: 0.13897534216253718, 2280: 0.14270125161793273}, {4152: 0.5354168678294785, 11547: 0.4478862971683197, 495: 0.716049329622284}, {5945: 0.8590828353510032, 7067: 0.2508808251507903, 11655: 0.22334193513756123, 6825: 0.386204445321321}, {11640: 0.1719075914509579, 4599: 0.2532393696285756, 4710: 0.3100902485926596, 8997: 0.5211064139629698, 6672: 0.37258890529195626, 6995: 0.49435791883385233, 4680: 0.2828944981493884, 11298: 0.27453270002398344}, {6527: 0.2867773243326766, 4991: 0.40029744314959415, 11655: 0.23900769627414636, 9515: 0.346789491247916, 6534: 0.6859927567299442, 4127: 0.33097889789573726}, {5946: 0.5453507700369066, 2722: 0.3998067527738786, 9987: 0.3443520772087042, 6919: 0.43416871566963383, 2659: 0.48545470572690025}, {1209: 0.49954225577534855, 4184: 0.8662895212888566}, {8803: 0.16903784033369268, 7287: 0.6292544086146836, 2171: 0.1620081621335468, 7424: 0.4980354732804647, 5653: 0.548797886776835}, {1763: 0.45035143152578844, 7133: 0.27668431822245737, 9617: 0.3793966820086831, 8803: 0.1728624280233618, 865: 0.2638730206456339, 7489: 0.4467422670633641, 2171: 0.1656736989228167, 10483: 0.14911458440380132, 10910: 0.2789539362458673, 9515: 0.28371056082720586, 6570: 0.21149253380073668, 11259: 0.15773231787892614}, {5812: 0.10325455017882207, 7133: 0.24278630360432069, 5464: 0.35580427670684667, 11640: 0.17124673582318015, 865: 0.2315445837879083, 7780: 0.4214016624351536, 6570: 0.18558149898509513, 3613: 0.4133368441072004, 11259: 0.13840772278961025, 640: 0.5646542200515782}, {7869: 0.4864005349258349, 10632: 0.46862910205714525, 5720: 0.38475491928288125, 3844: 0.6290985109000012}, {4634: 0.289726521826639, 11479: 0.33653119635475603, 7443: 0.33653119635475603, 7819: 0.1953354361449414, 6923: 0.14435301555127714, 11309: 0.2974940492912609, 2171: 0.09424642187293739, 7067: 0.12494818629948823, 4747: 0.4153629119792492, 4418: 0.19494859894248615, 11591: 0.33653119635475603, 10358: 0.31925698359265614, 10359: 0.27245230906453916, 11547: 0.13274541292930375}, {5029: 0.224608746520844, 395: 0.2396458337504753, 8524: 0.3800927886854967, 10704: 0.4299685362005767, 7217: 0.33243907334964623, 7984: 0.40789816633356496, 6869: 0.4676980826925361, 10430: 0.17460801105275475, 4418: 0.1914450562256996}, {8803: 0.1808747324198869, 10160: 0.23970529839470908, 3442: 0.47859340161569175, 11266: 0.564683899637483, 6570: 0.22129537284320847, 2171: 0.17335280028363098, 5097: 0.35649259540659123, 7644: 0.39471045657031373}, {4376: 0.3507400706592035, 865: 0.3114933704655322, 6834: 0.32149019633597625, 4019: 0.4478652305308344, 6371: 0.24430028994000322, 9304: 0.45050444533706036, 1257: 0.4619276837322001}, {2823: 0.49988293989747057, 11315: 0.21502573768890565, 4325: 0.14954668404984367, 8493: 0.3891750882799114, 9713: 0.32888296209622564, 10365: 0.1754465869806132, 3902: 0.2874282900900311, 4710: 0.3535287219331383, 11640: 0.1959889786024696, 10160: 0.230064349621954, 6186: 0.17958498504893788, 4704: 0.24479456716008538}, {1881: 0.2619647152966574, 10514: 0.3486984854533676, 417: 0.411767298135508, 1558: 0.653466407777377, 4198: 0.4617501491876137}, {6067: 0.45937926714423705, 9704: 0.3131682392891486, 5022: 0.3900905678013418, 11234: 0.307739449281156, 7748: 0.2320083431637317, 2960: 0.29949091733683725, 11613: 0.44699047788349233, 4152: 0.276229963230089, 11259: 0.15619206867720972}, {7356: 0.18848177706723085, 4862: 0.17378777920721036, 8379: 0.2664218690039179, 641: 0.2948816036112306, 9912: 0.1748189640432281, 9735: 0.2035003362549953, 6076: 0.0932563797715699, 4510: 0.3324157261175122, 4412: 0.08975535864391292, 10600: 0.14199609237208835, 806: 0.2597685708922718, 9729: 0.15586575071213732, 6825: 0.21492306915359866, 7346: 0.11788820312834064, 5812: 0.07479699353681897, 10861: 0.1819261405968015, 6128: 0.14395393673026852, 4125: 0.3760352930217334, 8892: 0.1922471654419613, 4408: 0.40903222157853053}, {6911: 0.3424013530374107, 3656: 0.31278279779822926, 5888: 0.21783333342402936, 8160: 0.27650043167931065, 7748: 0.1238779882638995, 2688: 0.223638391513022, 576: 0.20828403893152156, 8293: 0.23866504289567317, 3658: 0.18862662645575037, 1707: 0.25322584182050395, 2844: 0.31278279779822926, 7845: 0.19129709899927094, 5952: 0.20689452786401968, 9515: 0.15000451638578066, 5771: 0.14129874022810826, 8258: 0.24527988385233396, 773: 0.34022934822653383}, {9494: 0.8606583478812604, 9576: 0.5091828828842337}, {2479: 0.18819074161731153, 7726: 0.32652330378081723, 4475: 0.20155695029809684, 8230: 0.32652330378081723, 7114: 0.1752838422411569, 1699: 0.27651861294672586, 7970: 0.20038581146809434, 9401: 0.2078837614499235, 5387: 0.3815170531315678, 11128: 0.3598042884736257, 10305: 0.27377419105564094, 8125: 0.412553401198802}, {2945: 0.4315102001085551, 4771: 0.28458988782961553, 457: 0.6070781368133019, 10160: 0.21612350050834983, 10279: 0.25074131354553375, 6587: 0.46015782453436227, 7067: 0.20721457834807896}, {4745: 0.27544597092593676, 9474: 0.3944977256838405, 1248: 0.2938593257514194, 4624: 0.19620736229775954, 8174: 0.38799827466226505, 2171: 0.1364642465698695, 6742: 0.22744425659118223, 395: 0.2715892320374212, 10760: 0.4872808457357009, 11480: 0.3339925957433098}, {6018: 0.07258770094716954, 10336: 0.1629598955050369, 4309: 0.20544647533859603, 7452: 0.23795731116296168, 6854: 0.2187611207853185, 1570: 0.15791063697478114, 6672: 0.15641328590447032, 6341: 0.16692296585253308, 10019: 0.13511272292418922, 5894: 0.23795731116296168, 5983: 0.17418894703266244, 10811: 0.11591653254654606, 9304: 0.1411241613078932, 6448: 0.16117254965238903, 3096: 0.2187611207853185, 4575: 0.23795731116296168, 7971: 0.23795731116296168, 7341: 0.2187611207853185, 7114: 0.13153704311351014, 9521: 0.1402974059231348, 2005: 0.13721709552682715, 6237: 0.19956493040767537, 5104: 0.23795731116296168, 11702: 0.23795731116296168, 3862: 0.23795731116296168, 7417: 0.2187611207853185, 10674: 0.12697722946050582, 6593: 0.1629598955050369, 6851: 0.12110121554549165}, {4448: 0.08397672071187445, 10432: 0.24761795721515872, 6142: 0.2811938479506692, 6592: 0.13017558148628644, 3187: 0.23368507434313157, 4325: 0.06832534389717815, 10600: 0.10249810507508256, 3900: 0.10524859000824673, 2081: 0.19231329726951865, 7356: 0.10457346579197895, 9223: 0.11054562623584638, 6920: 0.17407946870376795, 7399: 0.1557269432418673, 2994: 0.2952547983967742, 8892: 0.13877121429667075, 11486: 0.18075119693391647, 4012: 0.17838041439749147, 5952: 0.17954536383267505, 11259: 0.09415903932146479, 9929: 0.12799308828271186, 2995: 0.17025745365150313, 5812: 0.053991275215853236, 4736: 0.2952547983967742, 2556: 0.2952547983967742, 10483: 0.06841859420479278, 10160: 0.1051124998609895, 7506: 0.2952547983967742, 5773: 0.184645478505494, 9168: 0.19231329726951865, 10480: 0.09186552234210148, 7346: 0.08509612644619866, 4862: 0.1254465369882764}, {4983: 0.2123136727265736, 7531: 0.3064317731844363, 1032: 0.3064317731844363, 2761: 0.3064317731844363, 213: 0.1336709613128506, 10854: 0.13721074349648366, 215: 0.17464644286209832, 7230: 0.2569916225367533, 10430: 0.11440167154646574, 5553: 0.24903353578227688, 6371: 0.09855099581548221, 11528: 0.14555859948663244, 1758: 0.2178112301334621, 6914: 0.2817116978605948, 1566: 0.14841320499944088, 4286: 0.2817116978605948, 892: 0.23227154721291174, 3976: 0.3064317731844363, 10365: 0.08319300135187177, 10453: 0.15647759147517937, 7259: 0.16287352207889055}, {4475: 0.1320243780994774, 10600: 0.12205138313172259, 11331: 0.1958187291421427, 8012: 0.1575285097350574, 1018: 0.1702805755528474, 3605: 0.21961427413273676, 4325: 0.06253474535045626, 3604: 0.1453643201751235, 4850: 0.19781447362916782, 7748: 0.0983917974679807, 11298: 0.13088059881284825, 10564: 0.07959621029056388, 11227: 0.11041931793376811, 9592: 0.24990233082781527, 11640: 0.08195514963284985, 6076: 0.08015762931807717, 10157: 0.1588751906997129, 8139: 0.26649315096344134, 9398: 0.276564759142877, 5749: 0.12045926311123659, 9084: 0.0875630666388384, 9080: 0.19317794211854736, 3796: 0.12871074098644994, 6769: 0.16776319525038552, 10483: 0.06262009265949152, 1384: 0.12155330260562427, 5607: 0.20903206614134504, 4421: 0.15421487262202996, 3921: 0.22663224237777527, 11454: 0.14094295258697465, 9230: 0.27023184338491313, 5225: 0.2948553453180753, 3284: 0.20483244187420632, 4448: 0.07685966210409871}, {7985: 0.2644543189908674, 4418: 0.19701329222720987, 6058: 0.1817808144519824, 8803: 0.1682139232963093, 9025: 0.32960868511310965, 9246: 0.28214695053418337, 5149: 0.23178914242224907, 11387: 0.44247429806243527, 10198: 0.4036473765677518, 6958: 0.27608532253752843, 5724: 0.3911478997006272}, {6018: 0.3108026595416135, 479: 0.7242144927395985, 306: 0.6155607811806938}, {2171: 0.323984841983713, 7399: 0.6637138157153701, 3157: 0.6741793477950302}, {5601: 0.2455711547007459, 1373: 0.40935750720071046, 7067: 0.18063507498125886, 1696: 0.3236056558628265, 3227: 0.29077701361044855, 6851: 0.2693244733499214, 6466: 0.5292079622822076, 4695: 0.3020063715350838, 3942: 0.32621423800528787}, {9929: 0.6520992176851131, 2656: 0.6169693790616065, 9729: 0.44058528753783777}, {8803: 0.12586678327962303, 7356: 0.16595028482008922, 1276: 0.31404917915848907, 7133: 0.20146289460813174, 5812: 0.08568012384416847, 6469: 0.375765857302755, 8847: 0.4954100525097897, 10430: 0.17492506363953422, 9521: 0.3594109980018741, 1484: 0.3551531601648776, 3178: 0.37084077066537224}, {9195: 0.44188159231439633, 4946: 0.42467815357512817, 4501: 0.6129368775995506, 6134: 0.4561965557116885, 6570: 0.20145026901202995}, {4862: 0.1664897495799072, 9554: 0.36024409802572543, 11545: 0.18978615931166568, 1981: 0.39185535629364465, 10317: 0.39185535629364465, 4038: 0.36024409802572543, 11089: 0.39185535629364465, 4221: 0.31014143907046193, 7517: 0.2626455127583574, 4704: 0.14843476758694374, 4152: 0.1698694737036793}, {407: 0.3647546595052062, 5126: 0.4197628611860938, 11211: 0.6901828452073919, 11381: 0.4630343604091758}, {1593: 0.4545453677251998, 10549: 0.5450427774625193, 2171: 0.23374533557713728, 3887: 0.6645900973618527}, {11259: 0.17729042697387304, 1728: 0.44584518784604676, 7544: 0.5141075886430546, 4325: 0.1673756986454869, 10160: 0.2574927120320483, 6374: 0.5214320234355808, 5577: 0.3731957716356933}, {}, {2087: 0.7317377311641771, 10638: 0.5343908336292853, 4624: 0.42306776020361075}, {5030: 0.32214591879161614, 11227: 0.27561243633008237, 5577: 0.34803221645098054, 2307: 0.6745129214976625, 10160: 0.24013069305610746, 6692: 0.431859858337634}, {6128: 0.4401299202817866, 1276: 0.8382220206727631, 2171: 0.3219774795416879}, {11259: 0.17909013530830614, 3796: 0.2674760350898475, 9706: 0.5615735055959825, 7325: 0.5615735055959825, 1204: 0.32213133090263785, 7334: 0.3392788709507991, 747: 0.21627515597125652}, {4425: 0.7071067811865476, 7511: 0.7071067811865476}, {834: 0.4669147969430353, 2297: 0.49217841202015333, 3867: 0.37555064614803185, 388: 0.33480596496354287, 5732: 0.5353668472548435}, {6018: 0.29032056233731524, 9895: 0.8749533429934528, 4826: 0.38751853977419753}, {7246: 0.7841954734618198, 10170: 0.6205138672116781}, {9419: 0.7740343847633234, 11454: 0.4037077578492212, 417: 0.4877405226720639}, {3121: 0.5497541635180677, 6186: 0.34005599769590733, 7133: 0.46343102936528374, 4947: 0.4672325148401822, 7896: 0.33672469330066934, 6076: 0.18887619528334806}, {417: 0.5790883462304703, 6158: 0.5626009445667276, 9223: 0.3440811169883743, 2528: 0.4793172742189849}, {4439: 0.5025131663270344, 355: 0.6751669806902586, 9084: 0.2187739630607317, 10634: 0.3328249166351543, 6052: 0.36468560954208584}, {5771: 0.392138378174212, 9339: 0.7303805865674359, 6205: 0.4626725450766697, 11315: 0.3141748035100591}, {306: 0.1853351644100008, 3336: 0.29705111259917616, 4947: 0.13298332185371212, 7819: 0.16369462429711015, 7259: 0.1630513193004275, 3728: 0.18193235639722016, 11098: 0.24930538764724827, 7653: 0.1998112667697493, 8124: 0.19640845875696866, 1566: 0.14857521694067902, 7782: 0.20777804079085832, 717: 0.20164256363124108, 360: 0.21804899886985935, 2171: 0.07898020412635182, 9575: 0.2820192221071068, 2802: 0.2820192221071068, 6496: 0.25727216166835726, 5120: 0.2820192221071068, 10514: 0.16369462429711015, 3126: 0.15999796720683934, 10822: 0.2100822248487503, 4648: 0.2675431197473583}, {9929: 0.2567765245648621, 4659: 0.5165970551971869, 4947: 0.2567765245648621, 6018: 0.18068818321413144, 1257: 0.36019940721217186, 7748: 0.2156692030473602, 4325: 0.13707259181733603, 5149: 0.28526066433927677, 11369: 0.5165970551971869, 8803: 0.15911950945391998}, {10283: 0.5196371090630185, 3219: 0.4593599327837685, 865: 0.23178275435367945, 9380: 0.31629628680387345, 5579: 0.5652350326659555, 4785: 0.21379545978093273}, {1384: 0.2435938591839867, 11586: 0.3904147842420045, 11547: 0.19638186347158956, 4713: 0.3452425403905283, 7922: 0.2565521624441455, 4785: 0.20483563076654723, 11640: 0.1642388215796883, 8968: 0.4978599410562474, 11426: 0.4978599410562474}, {5472: 0.17333343832893444, 5360: 0.19029448564398238, 7429: 0.17187519537558446, 8129: 0.24606650992540807, 5609: 0.13454179442167766, 7675: 0.143452848296634, 5622: 0.22103613611114753, 10910: 0.1356029246827075, 10480: 0.07480795413076746, 337: 0.22234428768004147, 11655: 0.07305849348734735, 9515: 0.1060046106580417, 5380: 0.1953962541472131, 4376: 0.11101478413301877, 9748: 0.22103613611114753, 3073: 0.24043195806477594, 6767: 0.14926306618794807, 3915: 0.20805511617182063, 7057: 0.16865888814157648, 9747: 0.20164031415751915, 9380: 0.23923565997976526, 3685: 0.2542163876859068, 7772: 0.2542163876859068, 8719: 0.15803991234210477, 6923: 0.09481210995907649, 9211: 0.19029448564398238, 6570: 0.07902132242544273, 806: 0.19865915640591222, 1095: 0.0973870063533822, 2038: 0.1358435294151108, 3924: 0.20969030759761076}, {812: 0.4925863711166849, 8123: 0.4543322954725591, 4947: 0.2546166344083661, 9752: 0.39782240895086945, 7896: 0.2387349433831766, 6130: 0.24810144504721357, 9773: 0.28286117824144896, 4870: 0.35956833330674365}, {10400: 0.4113914457081538, 7140: 0.6425437339601247, 4510: 0.522187814598867, 2743: 0.3810702227033689}, {11640: 0.5480434288192283, 6186: 0.5021729878510097, 4624: 0.6689325006313066}, {10160: 0.26102601244341034, 747: 0.2823751094130026, 10480: 0.22812977533311646, 8297: 0.5557616906551935, 2171: 0.18877175643162114, 4448: 0.20853950362215454, 2690: 0.5803104597835537, 4412: 0.1608900749643147, 11640: 0.2223648368446639}, {3049: 0.48513767154687715, 8118: 0.5154052303175741, 3189: 0.4474620591601651, 7621: 0.3961516890734997, 7067: 0.1974492740108337, 6826: 0.32070892918083127}, {2674: 0.1595976920684792, 9730: 0.17288179725351877, 6503: 0.31039797144450054, 4988: 0.31039797144450054, 239: 0.235277877127218, 8701: 0.2177388445637748, 11106: 0.24010171593548235, 832: 0.31039797144450054, 8126: 0.2853579400054064, 597: 0.31039797144450054, 7320: 0.24010171593548235, 11464: 0.12705666175373406, 8415: 0.19133543890097132, 8444: 0.2603179085663122, 4656: 0.16189212205491618, 7653: 0.2021767560710166, 9848: 0.22721678751011076, 10430: 0.11588239172740775, 10365: 0.08426978243688256}, {8509: 0.2417549576510671, 9401: 0.22170836603901942, 3372: 0.2658225935485776, 5183: 0.27121782535326805, 6826: 0.24393432231983328, 10480: 0.1368980529008998, 5155: 0.34823762880750975, 8105: 0.32747483893979606, 608: 0.439988866112852, 3610: 0.3696056678702305, 10656: 0.33350619495640926}, {6094: 0.4907321154551085, 1574: 0.545807917116254, 9919: 0.5069954258053413, 7985: 0.45191962414419573}, {5215: 0.720140635168756, 3900: 0.49218661400338226, 7356: 0.48902944959851474}, {9205: 0.2716970634548467, 11405: 0.4559770003722684, 11257: 0.2925646137294234, 9987: 0.22127265558763345, 1047: 0.24902046547156245, 4382: 0.33203652342711437, 4577: 0.37162356682071374, 11617: 0.5228255148654124}, {11640: 0.17445694225246458, 11464: 0.23546539953634482, 6180: 0.23673259620052198, 9057: 0.48242933114233943, 8008: 0.528834304528441, 4367: 0.24440508925122517, 9604: 0.37811431470710427, 398: 0.37811431470710427}, {7235: 0.5025756471350193, 4020: 0.7597930145328148, 6511: 0.4124709613705609}, {10638: 0.4235648964690096, 3301: 0.9058657618428888}, {1818: 0.3792364288056463, 3495: 0.4033904946525127, 10279: 0.24857804420233648, 9110: 0.6018405734308518, 8923: 0.45618781536127584, 6180: 0.24768002971497804}, {5812: 0.12630095437622937, 8499: 0.4023971726672606, 1844: 0.2488671497615238, 10480: 0.1651765102539559, 9929: 0.23013477875230917, 4044: 0.35957105583204896, 6527: 0.19355488816477828, 9912: 0.22689437188794725, 5544: 0.46299773393758215, 953: 0.4023971726672606, 11259: 0.1301279077315524, 10634: 0.26169615339160235}, {10747: 0.3582480888312293, 3353: 0.3338692452730757, 8623: 0.305765601083425, 1868: 0.30072803452995656, 2945: 0.29201075585352765, 11355: 0.2813416690167347, 7626: 0.358293177339766, 9084: 0.13311800923666445, 8792: 0.41082075359610704, 1021: 0.31139712136674946}, {10430: 0.2345587424643753, 9515: 0.27700372593255596, 8621: 0.40225885489728336, 1041: 0.49726404559746895, 7346: 0.18107807792284117, 10401: 0.4762283740092648, 9729: 0.2394121702086529, 2130: 0.37718230738731784}, {3676: 0.7071067811865476, 2354: 0.7071067811865476}, {9084: 0.4767883214032448, 7133: 0.4862901414144192, 11259: 0.27722449779497976, 5812: 0.20681426037102055, 617: 0.6454217492974105}, {11640: 0.3481823038539182, 6527: 0.41857994114644576, 6128: 0.40404803214842894, 1360: 0.7350544904085669}, {9729: 0.3622944830887878, 10171: 0.6511038011067013, 1236: 0.666938188820998}, {9205: 0.1981899636572819, 8615: 0.3326133303879263, 5935: 0.38137611232407076, 6453: 0.4561544160424837, 10873: 0.23676290104909248, 5060: 0.3506102223336404, 6284: 0.3506102223336404, 5934: 0.3506102223336404, 10480: 0.11866129173070707, 11259: 0.09348269677025982, 747: 0.14687690454531643, 10910: 0.16532671671609836}, {6820: 0.7832482506255125, 6128: 0.27565473614648034, 4367: 0.3327830103917784, 617: 0.4469811095859354}, {4032: 0.15063970431230697, 8587: 0.1819966021557903, 7054: 0.2567198036452256, 9022: 0.28141373265578784, 4908: 0.21630072603527345, 5720: 0.132288929229017, 1033: 0.30610766166635, 4680: 0.15277186106111224, 4624: 0.08709514295476842, 2271: 0.1973204341950691, 343: 0.21630072603527345, 1198: 0.17222985316234157, 4433: 0.28141373265578784, 11655: 0.07149331083316722, 5626: 0.23528101787547784, 9084: 0.07623797102897009, 9511: 0.16112709418307825, 3892: 0.18621767521580576, 5583: 0.1973204341950691, 5968: 0.08566615232643154, 4318: 0.21630072603527345, 1085: 0.21630072603527345, 3192: 0.16603319295399194, 4195: 0.2567198036452256, 5405: 0.23528101787547784}, {4087: 0.38244978427335574, 4233: 0.5252082018750146, 6592: 0.26550817532782073, 3336: 0.4482100461208954, 2250: 0.5536259301937307}, {1373: 0.7448807196391474, 7281: 0.6671976570026802}, {10483: 0.09810929630490499, 6052: 0.1757733786919824, 9872: 0.28381270475632947, 8280: 0.25172251292429937, 1204: 0.18666892478411445, 2722: 0.12290703782530726, 5758: 0.12938181935621285, 7133: 0.13992240908760606, 5812: 0.05950758010560516, 1702: 0.20220660546080968, 11259: 0.10377929679726575, 6666: 0.32091882779492714, 1278: 0.25172251292429937, 1257: 0.1978895678634711, 3912: 0.19660559519277537, 895: 0.1765088108573374, 2602: 0.2991691144779825, 9729: 0.12400489922976733, 617: 0.18571004910064334, 3775: 0.17505195561369896, 1637: 0.25172251292429937, 10785: 0.11962663883139035, 1419: 0.2644659165419984, 6130: 0.1374604443262753, 6076: 0.07419364371756064, 8362: 0.2382139607282134}, {4325: 0.182515877990699, 2022: 0.6242374217866167, 11227: 0.3222733001769356, 423: 0.6878630063577695}, {3865: 0.3280060957717916, 4325: 0.10741946854674223, 5195: 0.35678848377958483, 5749: 0.15904298066548153, 5979: 0.3111694781060262, 11358: 0.2992237077639983, 9919: 0.21993146675890896, 322: 0.2992237077639983, 6758: 0.16085655310616162, 10747: 0.23914154231528048, 9932: 0.3111694781060262, 5236: 0.3111694781060262, 3124: 0.35678848377958483}, {2171: 0.20149042559528735, 7683: 0.7194736164204643, 11232: 0.6151503840191352, 6371: 0.25169292542580296}, {213: 0.23188982294156063, 10854: 0.23803056926187188, 215: 0.30297330336302425, 7937: 0.5315919697854086, 10365: 0.14432162501107068, 9487: 0.4458241435702384, 647: 0.395653181465995, 3016: 0.37290263445278937}, {4991: 0.49124456403221395, 3495: 0.6469824586630454, 10218: 0.5831744820301389}, {8706: 0.36650799363081443, 8950: 0.16840091177704508, 7964: 0.3343470813288085, 1881: 0.2079309825332286, 7354: 0.2311281330994384, 11655: 0.12114092445170631, 10160: 0.18465330036123528, 4905: 0.2666904143977844, 8803: 0.13933407612163423, 2183: 0.3552068712965989, 6767: 0.24749847637681946, 10365: 0.10823441213537606, 3736: 0.27965938867882534, 6800: 0.34769506594515925, 2283: 0.3021861690268027}, {10430: 0.210736343576126, 1170: 0.32379282543987764, 6374: 0.4069406021039206, 5563: 0.473397583540504, 6923: 0.2225935023585642, 1209: 0.325499281412563, 6407: 0.44676060956030367, 2005: 0.325499281412563}, {8896: 0.24310482556394475, 9667: 0.21219246035381556, 9693: 0.1412065154290335, 5682: 0.21219246035381556, 4785: 0.10964221851398774, 10480: 0.11734140968567616, 4704: 0.10980402417616719, 4862: 0.12316012471436755, 11655: 0.11459726061123841, 5812: 0.053007220037438435, 10776: 0.15063870661846634, 4412: 0.08275582691114945, 8293: 0.20334123041207267, 11344: 0.3289136372840179, 1163: 0.1206083894515961, 2739: 0.15231192965180387, 2722: 0.1094811851968111, 4656: 0.15118727831324633, 3316: 0.17295192088701566, 7356: 0.10266749005560359, 6018: 0.08842445584402514, 662: 0.25281018760536467, 9211: 0.2294258860463883, 5856: 0.25281018760536467, 5259: 0.23557676191279192, 4498: 0.2898734286818975, 10158: 0.2664891271229211, 658: 0.2664891271229211}, {5132: 0.5662329250628362, 5577: 0.35950112353675984, 4325: 0.1612337445627364, 4697: 0.607655232422553, 11259: 0.17078464584437258, 3222: 0.3545851373340056}, {7133: 0.14529809851906816, 6593: 0.17787409405551288, 5554: 0.17787409405551288, 5849: 0.2746264522403699, 2171: 0.08700194352450302, 7336: 0.13915294363083203, 10288: 0.08855056980697704, 8006: 0.21108387443458917, 3316: 0.1549701305651296, 9084: 0.0841618853623729, 10564: 0.0765044828019129, 6186: 0.07217870960750528, 4067: 0.19013083588385638, 8488: 0.13087943484655398, 9708: 0.17787409405551288, 11359: 0.10950360835770985, 11655: 0.07892408139024416, 11167: 0.17236272400990832, 7346: 0.09739376462196411, 10332: 0.216356969768923, 10162: 0.2947165261486416, 4349: 0.3106629149164258, 11692: 0.20557250438898458, 4862: 0.11035518142566102, 10873: 0.20978696084333484, 6117: 0.11123144117918896, 6371: 0.08353303084571727, 4325: 0.0601057303195897, 10305: 0.224249074071243, 2694: 0.2009127074113312, 5023: 0.16124682869918766, 3545: 0.2597353233187937, 4412: 0.056994622592330874, 10600: 0.09016747096338171, 4418: 0.10631868168092508, 3900: 0.09258706955171297, 10430: 0.09696825769254436}, {8142: 0.28987155731602077, 5867: 0.269007943118116, 6923: 0.18396175856695376, 6925: 0.2635740790066994, 7622: 0.39123806870684646, 398: 0.30664121081969015, 5615: 0.2986816202089533, 7611: 0.4665046041099947, 11547: 0.16916916845077373, 9912: 0.19938246257757938, 8803: 0.12531804259914991, 11259: 0.11434934448880076, 5888: 0.2986816202089533}, {10160: 1.0}, {6186: 1.0}, {4456: 1.0}, {8253: 0.4549929349593156, 8335: 0.36228896598879917, 7217: 0.3517872052106399, 1296: 0.4949183612352802, 7016: 0.36228896598879917, 9672: 0.4022143922647638}, {8366: 0.1817122618780464, 4017: 0.33553480118459195, 6310: 0.3731407565924978, 1455: 0.22092737938751839, 7133: 0.16269146297816442, 6128: 0.13316472140106558, 6262: 0.25912233247990696, 5853: 0.2816173516769216, 8089: 0.3173279541458027, 6505: 0.3783756322873182, 4204: 0.25628027600428727, 2199: 0.2727804982837424, 7748: 0.1377671043963704, 10483: 0.1295139321748201, 9496: 0.23662731328289235}, {6186: 0.10379797925375833, 6923: 0.1472931058514316, 1167: 0.3735173688739513, 3192: 0.20259630562763367, 3610: 0.24116841428096836, 7067: 0.12749305139397962, 5459: 0.3433854494669851, 30: 0.3735173688739513, 10385: 0.3433854494669851, 940: 0.2202223485554609, 9861: 0.3035532186500348, 6742: 0.16027933164565447, 2722: 0.14107234465008567, 6851: 0.19009042914849475, 10042: 0.24551915939223498, 699: 0.2335890684261183}, {4412: 0.18302038055113115, 3900: 0.2973143769136687, 10480: 0.25950884978934646, 3915: 0.5861264626606812, 5968: 0.3036819358148734, 5812: 0.15251874014006295, 9729: 0.3178262494988955, 2130: 0.5007199008712636}, {1566: 0.21061336287127833, 3929: 0.43485770852550537, 11259: 0.10659207535724333, 6923: 0.17148209917320698, 9031: 0.344176475699156, 7712: 0.39977741904314307, 10125: 0.39977741904314307, 1015: 0.3792567651815183, 3931: 0.3792567651815183}, {11596: 0.8649106775104882, 865: 0.5019258111796486}, {2698: 0.2711317629193486, 11315: 0.14425710267360833, 9080: 0.23821680030503922, 4127: 0.18243393417418696, 11655: 0.1317398619864196, 10512: 0.27758196710302235, 1141: 0.378115820135044, 3824: 0.2672484699759425, 10042: 0.284979271354678, 3563: 0.3173658741094705, 3874: 0.23615700796184738, 4870: 0.2654134348218928, 4044: 0.29365047538458994, 6258: 0.3353629447368952}, {6210: 0.2171858220242723, 4584: 0.5020376652797356, 8357: 0.3805384280549034, 1905: 0.5020376652797356, 8725: 0.2689661846705419, 11364: 0.4615379195381249, 10365: 0.13629794238461462}, {6527: 0.2192812282006259, 9602: 0.6014368216001792, 10533: 0.34868274822800904, 7468: 0.4031198753923929, 3900: 0.2143923749797791, 3953: 0.4476372959752685, 7896: 0.24446055006189343}, {6452: 0.6831676030071016, 8824: 0.43294336215476226, 127: 0.5711745745246428, 11259: 0.14000598838712142}, {8803: 0.28047554794751844, 1114: 0.9598611706926888}, {527: 0.32813292063641814, 2722: 0.17886967507079482, 395: 0.24266655732899264, 1521: 0.4735933933011392, 7041: 0.35248601075842523, 6958: 0.2716639381497983, 11315: 0.15758111015483692, 2470: 0.2792262369151649, 11347: 0.22807714177646396, 2003: 0.4735933933011392}, {11640: 0.12455598540703575, 3234: 0.34443720011286827, 2171: 0.1057390749050067, 6371: 0.13208457432173745, 9001: 0.31130571727667, 10483: 0.09517043629869132, 10157: 0.18559109312729322, 11031: 0.3006397621264194, 1593: 0.20562209965365313, 7133: 0.17659015305256717, 1163: 0.17088108340285993, 8072: 0.22666840058376903, 1274: 0.2435719086007208, 11655: 0.12479678503966353, 11482: 0.3775686829490665, 9442: 0.41070016578526475, 9929: 0.17803870711844155, 6570: 0.1349823479454282, 10480: 0.12778517219946806}, {9729: 0.36678337887232915, 5215: 0.502022652202741, 10638: 0.45006225014059154, 10365: 0.261318087270245, 2209: 0.5853204574608095}, {11655: 0.09551239289189851, 6923: 0.12395179622598593, 9031: 0.32367005833211043, 7712: 0.37595823560776837, 10125: 0.37595823560776837, 9773: 0.15137610730576503, 11659: 0.20661211968235435, 9704: 0.1544819294770098, 680: 0.3143266517866046, 5951: 0.3143266517866046, 7019: 0.19962286788275074, 1738: 0.3143266517866046, 4517: 0.2152597794926087, 10365: 0.08533637780202773, 9404: 0.2636127298736897, 5464: 0.19806593666899686}, {2532: 0.3628982305948593, 6720: 0.29948802395290797, 6371: 0.17042385655482736, 9773: 0.25519919060163276, 11585: 0.2827678046444914, 8509: 0.2911634576680553, 10636: 0.4016662897866163, 6310: 0.4016662897866163, 8969: 0.33927802415498515, 6052: 0.28622699937648866}, {10776: 0.6318291483047485, 2171: 0.3130272277878903, 2659: 0.5575722609813623, 2734: 0.4380856717584543}, {11227: 0.3515325174991369, 6742: 0.36916769430104934, 4469: 0.8603139558470304}, {6186: 0.22822009576395855, 11486: 0.50275917363655, 371: 0.42677989962549373, 3210: 0.716245571357643}, {7356: 0.3605966988856729, 10430: 0.2921517389060828, 802: 0.719417481297286, 4367: 0.3324846595988907, 8189: 0.3956132213945663}, {3192: 0.4939805450404916, 4139: 0.8372597464755493, 2171: 0.23447673243458}, {7199: 0.569661286664098, 5096: 0.42911814961265327, 10197: 0.7009590802245286}, {4818: 1.0}, {7517: 0.4800063483099246, 7170: 0.6245812561259657, 6018: 0.21845740563876018, 4710: 0.39177395486105004, 464: 0.42223416523582796}, {11640: 1.0}, {4862: 0.22848817020078313, 6923: 0.21206726012046648, 9263: 0.49439388906056303, 2038: 0.3038426747447225, 11315: 0.17893715484681885, 11452: 0.4510110467077692, 6143: 0.37260285304859825, 2171: 0.13845627253731874, 2507: 0.3876967548717447, 6186: 0.14944455776897184}, {1691: 0.16328645349056747, 1135: 0.2609718034240936, 5812: 0.042155831054649436, 11040: 0.16289764730206616, 9681: 0.1277420506860606, 3766: 0.14400174854763187, 6371: 0.09129557381722136, 3787: 0.15453607889942964, 6177: 0.13188031489907842, 6690: 0.14024188330171494, 3874: 0.14256798954628844, 3781: 0.13188031489907842, 1384: 0.10369582565080959, 3491: 0.16289764730206616, 2970: 0.10060994662084154, 6511: 0.08843270193659286, 6592: 0.07812252776027262, 9331: 0.10775091234544669, 4448: 0.050397114574988064, 10237: 0.13430925610663677, 7883: 0.16289764730206616, 7346: 0.051068905740091965, 2683: 0.2093224150673285, 2722: 0.0669229085814675, 2487: 0.12970755294991718, 940: 0.10447065395084318, 3900: 0.06316304333555059, 7639: 0.11344785508834591, 8862: 0.16289764730206616, 4608: 0.16289764730206616, 11555: 0.0830651009108562, 8587: 0.17832302554828716, 3166: 0.14400174854763187, 1702: 0.08462644259888714, 3872: 0.17719184289978085, 8141: 0.17719184289978085, 10160: 0.06308137128780081, 7934: 0.14860345170435146, 8469: 0.14400174854763187, 6186: 0.049240428333209185, 8593: 0.17719184289978085, 1962: 0.1617141693082858, 747: 0.06824074333655429, 7299: 0.14024188330171494, 10564: 0.052191477557048414, 10041: 0.17719184289978085, 4248: 0.14024188330171494, 8892: 0.08328104178555547, 6742: 0.07603445654657823, 3484: 0.18245889682393876, 9511: 0.12134598454728064, 9294: 0.09455195633391161, 4745: 0.09208139621191974, 6758: 0.07988618014082363, 2016: 0.17719184289978085, 7780: 0.10164126977076866, 5376: 0.13706296253168238, 7151: 0.07010182935150006}, {9268: 0.43623486300493824, 3615: 0.26934215870543315, 8024: 0.5201578996076239, 10643: 0.38714303364571817, 7461: 0.45365046662667097, 10400: 0.33303337812858813}, {11252: 0.3525651055001386, 7825: 0.3835025356129428, 10480: 0.11932290667215208, 7845: 0.21562784899077414, 11240: 0.2657127587971447, 2511: 0.49894830224563275, 9617: 0.22610950951027223, 2143: 0.25208257504152387, 6919: 0.20464212351879207, 2005: 0.22114514492871967, 3190: 0.2854332024179906, 11642: 0.15098609711721658, 395: 0.1965045149710461}, {2674: 0.30176614870240015, 9513: 0.42961983972824447, 8802: 0.5118573586430973, 6076: 0.13380853592859002, 1274: 0.3480688103103995, 3902: 0.2610359803803805, 4325: 0.1358149723931433, 7114: 0.24935868575382086, 10480: 0.18260739967222459, 10332: 0.37576415722982304}, {5101: 0.38703156522823484, 10796: 0.38703156522823484, 10542: 0.38703156522823484, 7922: 0.19944120184240322, 8082: 0.3256504547612523, 3198: 0.45935429460229255, 4579: 0.35068337141945577, 10634: 0.20752954628384626, 7151: 0.16655624675145048}, {9084: 0.2655619385931254, 6916: 0.48605309411926795, 9450: 0.4213947527974598, 6180: 0.3372802565618676, 5376: 0.6339540498459454}, {4448: 0.11790423194148966, 495: 0.18472283449547436, 10483: 0.14051304436950413, 11528: 0.15135062232034752, 5749: 0.14203121928312312, 6076: 0.07264423724024018, 6445: 0.22970491909495325, 8241: 0.2589430013219217, 10512: 0.2040011901688387, 10288: 0.14132786233088543, 5812: 0.07580433936814382, 6451: 0.21620925876340613, 10831: 0.2182035548453284, 7346: 0.09183177190512802, 11219: 0.24151402488049237, 10480: 0.14643735257795112, 3822: 0.1306568505078034, 5149: 0.19963816668187062, 6419: 0.25218203625612823, 5917: 0.21144258977953498, 4599: 0.14234952717963154, 3796: 0.15176037944262058, 10157: 0.14398338801888108, 237: 0.19010656702826326, 11655: 0.12596386148192995, 6058: 0.12034033600325171, 2038: 0.18002254634016612, 11259: 0.07810123152011288, 10279: 0.1316016823129805, 1163: 0.13257121838172128, 3874: 0.17355706205889274, 8131: 0.2264783073300137, 2618: 0.25218203625612823}, {5812: 0.07381716050325368, 6511: 0.20146512793291058, 5336: 0.3711092687998143, 1163: 0.16795766380314467, 920: 0.4036739409372378, 9670: 0.4036739409372378, 908: 0.35206015675114255, 8803: 0.10843971888134182, 11640: 0.12242509179523482, 10480: 0.12559903392411428, 4325: 0.0934147759380404, 10430: 0.15070556529780518, 1460: 0.4036739409372378, 9223: 0.1511385719666819, 11315: 0.13431645933671216, 4412: 0.08857957254369334, 3790: 0.22920701644765423}, {6018: 0.42095503946656304, 6521: 0.9070815039166571}, {8803: 0.14795541007657478, 4114: 0.658766608901856, 5213: 0.5507736840774167, 4017: 0.33065222212314543, 3615: 0.28519527078115986, 7896: 0.22386796573404327}, {2760: 0.35661644886204513, 6130: 0.23719436163973154, 2960: 0.26392146366366276, 4418: 0.22985335817353003, 3796: 0.2674548967500431, 5816: 0.4444329983676648, 9547: 0.4179348023312841, 3736: 0.39390303457533626, 9080: 0.308536102393753}, {6896: 0.38997652491684154, 10811: 0.260101571095594, 6186: 0.14837978722395814, 344: 0.4339310102013277, 1384: 0.3124742222378836, 4046: 0.2593483221190609, 8960: 0.413021858362273, 2470: 0.31480906907382283, 9704: 0.2624177005358544, 11234: 0.25786867412812187}, {10160: 0.10084208370916099, 1499: 0.18289171677309798, 6680: 0.28325945187142343, 3859: 0.4184085569595208, 1806: 0.3062824403673077, 7559: 0.18985787175171473, 1380: 0.18135810724926596, 1047: 0.1349157578363395, 3829: 0.1918564630274808, 3329: 0.15563888558461, 10029: 0.19398408249019597, 11550: 0.23755795744945213, 7840: 0.2604087046604378, 7699: 0.2604087046604378, 9223: 0.10605447790985402, 1534: 0.18135810724926596, 3830: 0.2604087046604378, 11650: 0.28325945187142343, 2171: 0.09488167616020245, 4437: 0.1679912770173808}, {6094: 0.48286133364027817, 4695: 0.4618438691583473, 9638: 0.7440061646222962}, {4896: 0.46832691641943514, 9773: 0.17335562417196415, 5389: 0.43054664145068117, 6226: 0.14264567239602952, 4821: 0.18079940824754556, 5444: 0.3018887864160877, 2919: 0.25950872751354775, 8101: 0.35996627132368636, 2885: 0.25950872751354775, 10253: 0.21747633906884054, 7875: 0.330927528869887}, {274: 0.2712473586072642, 10610: 0.2286741881182981, 2808: 0.4400367770601558, 10122: 0.2772793713859061, 4443: 0.3690405996001874, 2479: 0.20072758380662883, 6876: 0.21691701139134428, 895: 0.23867651895664083, 2821: 0.4400367770601558, 7905: 0.23867651895664083, 11547: 0.1595711060666683, 10634: 0.21691701139134428}, {8521: 0.271729253837408, 7636: 0.3237434710260459, 5231: 0.3412604366250821, 8615: 0.3237434710260459, 6923: 0.14638159448150045, 5830: 0.31131498614331465, 1702: 0.1772871323167193, 1923: 0.18891407003458816, 7297: 0.3016747043191754, 4973: 0.23574578655053718, 595: 0.19808046608821164, 4290: 0.3016747043191754, 11035: 0.3712058871068496}, {11125: 0.581365434957061, 9210: 0.4268174326564561, 1205: 0.6927056447133507}, {6848: 0.7024232947151771, 595: 0.3133771333416495, 4818: 0.2520038817981711, 3479: 0.587273642264574}, {1638: 0.4604432135024459, 7985: 0.5811874712320272, 8995: 0.6132294289967456, 2171: 0.27232854759423936}, {8307: 1.0}, {10453: 0.31430123223650497, 1574: 0.4084510884066905, 7356: 0.21799787616647603, 4010: 0.41254556533360054, 2191: 0.5002093042632171, 4228: 0.5161939347115563}, {6018: 0.3427295369285217, 11259: 0.358304778598323, 11511: 0.6119975712794358, 5968: 0.40908157232710596, 865: 0.4607226824032247}, {10160: 0.2736486972038485, 7067: 0.262368503532571, 3950: 0.7686630137358966, 2462: 0.5152051340846331}, {4865: 1.0}, {6205: 1.0}, {10788: 0.5673778079762605, 8535: 0.40091852367724656, 10785: 0.16031245577083753, 6834: 0.18456783156626994, 4870: 0.2669741749922812, 4020: 0.40091852367724656, 7704: 0.36573810277428065, 10160: 0.15525387055679452, 9210: 0.26019684006538313}, {405: 0.5235900557146924, 11680: 0.2816519013675593, 1566: 0.25358884121876724, 4368: 0.34416442462301194, 11618: 0.26298233967701934, 9223: 0.19603607092630654, 5917: 0.34745912536785095, 6923: 0.20647287629927266, 6710: 0.43911327002386075}, {5535: 0.4596480212771276, 9729: 0.19052303129314846, 699: 0.3126770608967106, 5688: 0.4596480212771276, 5133: 0.3228226023870811, 9725: 0.4596480212771276, 9897: 0.2457258164452879, 4745: 0.2598259230028012}, {9485: 0.4166439532367605, 605: 0.3633718221824867, 3827: 0.4166439532367605, 2231: 0.4166439532367605, 8505: 0.4166439532367605, 8196: 0.4166439532367605}, {10430: 0.2746696099328273, 4983: 0.5097487902309894, 4771: 0.34489500674465695, 6577: 0.569099788821529, 10664: 0.4710473871313644}, {7022: 0.7149992466072581, 4325: 0.2035946706511821, 9576: 0.453952818894665, 6742: 0.49117423146076933}, {7346: 0.1278694069660018, 10514: 0.2367448921937837, 3126: 0.23139856705896988, 3372: 0.2680427271595957, 6021: 0.3726925912017175, 2931: 0.3869367568651128, 4535: 0.40787295625016085, 10365: 0.12045000494620188, 10515: 0.40787295625016085, 5589: 0.40787295625016085}, {9193: 0.6263770101580528, 5577: 0.4178193927484754, 4113: 0.6580872253651135}, {8191: 0.48053613261985073, 11259: 0.11778874481441821, 7845: 0.2701858867075163, 6923: 0.18949496153497172, 7724: 0.48053613261985073, 9729: 0.18311301922878034, 1360: 0.30766536791992716, 9704: 0.2361688025135462, 6226: 0.19042450698529034, 847: 0.4417709312481232}, {4720: 0.5714807356021728, 7320: 0.44205670740566455, 11205: 0.4984111604757777, 4127: 0.31286418002845984, 11590: 0.24773715665579907, 6355: 0.26518721208548146}, {4017: 0.42715794624775677, 829: 0.5295737819594027, 3915: 0.3843241339982827, 3558: 0.46344966789327113, 4325: 0.21422117495510862, 10365: 0.19317157641055954, 4862: 0.30230959965342163}, {4624: 0.261777441126732, 962: 0.3451969476290866, 6919: 0.2900445148101912, 4368: 0.35728307990953934, 4127: 0.22872034717446382, 9629: 0.37223700679324623, 11547: 0.19710749696122398, 6692: 0.3480089610113612, 11400: 0.49969953988676824}, {6291: 0.4484768476666528, 4818: 0.2871194087785594, 8312: 0.5071752460439084, 9032: 0.6151300122219703, 4862: 0.2842873547552406}, {4748: 0.3940115499778563, 1706: 0.3575664803005837, 7096: 0.4153305490752305, 1691: 0.24504368192522497, 10073: 0.37888547939795786, 1209: 0.26051453486377557, 9185: 0.3169128353883039, 3962: 0.4153305490752305}, {5368: 0.3848595010056575, 1497: 0.4336617660825337, 11051: 0.39998375350155485, 2722: 0.19529766229494194, 3293: 0.47537577767875705, 2002: 0.39194775448631036, 11227: 0.21128783761035796, 11642: 0.20357979906706608}, {7114: 0.35866279993818756, 11669: 0.5100054606363313, 6180: 0.2670218749173456, 11315: 0.2158916437576764, 6018: 0.19792529500850292, 6919: 0.346229530242657, 7356: 0.22980625736483895, 6371: 0.2086721713997978, 4325: 0.15014890675432127, 9987: 0.2746049027189075, 2005: 0.37415063100185164}, {7346: 0.20394556033630082, 11359: 0.2983313148323293, 9756: 0.7076221658465579, 4872: 0.3791078783501316, 8484: 0.4742918110035361}, {10160: 1.0}, {1163: 0.485009663039666, 1384: 0.5243382255343195, 2171: 0.30011790695929647, 895: 0.6322691625597772}, {11640: 0.27514219226370823, 3254: 0.4775422976561081, 11248: 0.621296853448894, 7880: 0.44646017406482025, 1702: 0.33303702625107556}, {6295: 0.3288099753636591, 9168: 0.32582375725044443, 10365: 0.13580738790080304, 7068: 0.5002307647246979, 9815: 0.5002307647246979, 2935: 0.5002307647246979, 2171: 0.12878966873529166}, {1563: 0.9183014822667716, 6186: 0.39588178496446397}, {7346: 0.21426599404911487, 4418: 0.30431193752374575, 6076: 0.16949678070352694, 8932: 0.743430582780444, 10480: 0.23131085145122768, 4579: 0.47598469344451233}, {6186: 0.08651098154507507, 4624: 0.11523918772426169, 7404: 0.24639231194458852, 2622: 0.16611924035472062, 1828: 0.25299805531262737, 2183: 0.2131939259078046, 3644: 0.2861964413494113, 10248: 0.27150592150109976, 8133: 0.2244312359480158, 5268: 0.31131005090592256, 8288: 0.31131005090592256, 4325: 0.09372712893455482, 7253: 0.24639231194458852, 11329: 0.20865910503401347, 7782: 0.2108556126798776, 3804: 0.2027708361996049, 10636: 0.23596922223638883, 7110: 0.2861964413494113, 9328: 0.2131939259078046}, {9897: 0.8774766245268495, 8803: 0.47961940474606174}, {5300: 0.4696827279672711, 229: 0.3802504645884855, 740: 0.5108971770009205, 6098: 0.5108971770009205, 2688: 0.33582118499946306}, {6527: 0.3197848234013565, 11227: 0.3583892244453083, 11647: 0.8770945391026294}, {4021: 0.6050185878751425, 10365: 0.19086691738448652, 7675: 0.32240913172579505, 285: 0.4967769519379553, 8441: 0.4967769519379553}, {4991: 0.39982310441183583, 5234: 0.785630684532055, 2674: 0.4039486581886839, 10480: 0.2444409831590005}, {8313: 0.3377775332805664, 9686: 0.3497610446938633, 2826: 0.461433603910757, 11072: 0.4242094175051258, 2217: 0.40243466433797725, 1844: 0.21631373230982118, 4774: 0.40243466433797725}, {10638: 0.4752284024011793, 3822: 0.3203398830696482, 6252: 0.49225206882988287, 2277: 0.6551551156778425}, {10483: 0.11936385468343366, 5215: 0.26865968089071063, 10480: 0.16026963118298318, 10288: 0.17561273098082658, 8546: 0.3037010135509379, 2140: 0.4319970938538874, 10265: 0.515104866740309, 4537: 0.21800508683873815, 8671: 0.515104866740309}, {6742: 0.29124442315355376, 3125: 0.770130582512728, 9729: 0.25863307993337686, 8234: 0.5051578979138459}, {7346: 0.152498122707108, 10430: 0.19753781518268065, 225: 0.5291169452259488, 8452: 0.43000718346731753, 6221: 0.48643269416026746, 404: 0.31380076016100633, 9162: 0.3711664472433955}, {874: 0.7373181277846816, 3328: 0.3982560145087168, 5609: 0.412591174527044, 11298: 0.357103133632455}, {9604: 0.8407919116308352, 9987: 0.5413584407175767}, {1881: 0.19952420532900608, 3708: 0.20100842521246065, 1112: 0.2410556604395096, 11664: 0.2847246047325213, 1122: 0.49282310332148843, 1825: 0.3825504774543093, 6592: 0.16866357844437085, 8853: 0.2650531177873101, 2627: 0.3825504774543093, 9944: 0.3825504774543093}, {283: 0.28566912262897687, 5317: 0.2491434444795032, 3764: 0.1543017077150207, 3878: 0.15124937248238196, 5812: 0.0679637339619993, 10480: 0.08888303698981483, 1358: 0.18000803313786995, 11263: 0.32414309449087747, 9246: 0.21787580715872054, 9405: 0.26794198416886983, 5749: 0.12734062564382892, 6448: 0.1934885741734658, 11454: 0.14899446749844578, 6130: 0.15699386619166547, 11257: 0.1598557724136407, 1637: 0.22097324352283973, 2713: 0.16561655552970572, 11259: 0.07002305363231137, 10514: 0.152436895326152, 3126: 0.14899446749844578, 10483: 0.06619733153913306, 5809: 0.18000803313786995, 3184: 0.28566912262897687, 10160: 0.10169994112094127, 10632: 0.19563429316425898, 11533: 0.1674640922075955, 4360: 0.20594604664139626, 6128: 0.10053778806476685, 356: 0.2003919819427762}, {6715: 0.353221370681812, 1499: 0.3955008563334992, 6851: 0.31173621028765414, 6668: 0.6125447218514177, 3912: 0.3700735158722685, 1326: 0.3308607376382539}, {11590: 0.26236801347184335, 595: 0.32295954706333474, 8180: 0.5075823421886465, 10099: 0.4504614015172995, 1336: 0.605231235238931}, {10301: 0.7744913357411216, 10784: 0.43136703972997503, 8803: 0.20805311962621845, 595: 0.4132790187153586}, {371: 0.27766693057254854, 9515: 0.2355746479108624, 5780: 0.4481064145929177, 5096: 0.32709974552070686, 10388: 0.4050029693813942, 4679: 0.46599596070381105, 4012: 0.3228094150692129, 4704: 0.2023978235204305, 7067: 0.1823776866448103}, {3900: 0.6905790051940892, 10430: 0.7232569651134665}, {3222: 0.19602405912337006, 11640: 0.15198018253801862, 7035: 0.31302856406968604, 8189: 0.19472460252994891, 4785: 0.1895468821242447, 4325: 0.08913428609692921, 5771: 0.15996554323560533, 1597: 0.3541041490091996, 6076: 0.08781747780462174, 11723: 0.3851766295006696, 10480: 0.11984378392895138, 1946: 0.31302856406968604, 865: 0.1579472165361932, 9557: 0.30485543262583004, 1596: 0.39662606218732477, 3928: 0.25318298471001527}, {7346: 0.19713449260242347, 8803: 0.1837415859775924, 2171: 0.17610043167239592, 4505: 0.683990062781277, 8430: 0.4632781477340677, 11185: 0.42162498117214187, 6186: 0.19007626488798363}, {11464: 0.30765548768519985, 8803: 0.20190346315094812, 10332: 0.48121445443110483, 11259: 0.18423148161656866, 4048: 0.6909668094961355, 5601: 0.3487683624807515}, {404: 0.3071286303935899, 5563: 0.43431332510794723, 7067: 0.1767639492154119, 11298: 0.2508168628882689, 5737: 0.47609001029651243, 5560: 0.4516522160567688, 1598: 0.34040236505210036, 2656: 0.2763406507713044}, {1969: 0.5702455995390099, 9004: 0.6249048853473919, 6919: 0.5332108780542256}, {2171: 1.0}, {4710: 0.5217167499423988, 585: 0.7548071630626014, 895: 0.39758996393134466}, {7156: 0.644989040242586, 865: 0.3254470527487966, 5968: 0.2889686076518825, 4248: 0.6281484678129862}, {10616: 0.390208843292994, 2072: 0.5147956742756526, 9412: 0.3383837318943255, 3792: 0.34867989963210777, 10806: 0.4732667306147664, 6018: 0.15703587450451037, 8372: 0.3130488552233438}, {7356: 0.17772989640772896, 6205: 0.24588772456189037, 4537: 0.21237672756578865, 9787: 0.5018061162602167, 10480: 0.1561318604643201, 9539: 0.36733092410031504, 3933: 0.4376451438482167, 7795: 0.33988185020898376, 1867: 0.380362916721792}, {7346: 0.13980864625662137, 2022: 0.3839327976338294, 2817: 0.48508875066978624, 8160: 0.3942259821562127, 4473: 0.3942259821562127, 9729: 0.1848478391100121, 11359: 0.204511915796982, 6094: 0.2894264286675596, 5914: 0.35657860052822316}, {3544: 0.5319495344636873, 895: 0.31384850648147317, 5812: 0.10580981793348496, 6076: 0.13192295703581286, 1112: 0.3646094595647991, 8803: 0.15543793385397592, 9729: 0.2868664971784745, 508: 0.5319495344636873, 10480: 0.1800341657892186, 9084: 0.1874924506296073}, {1063: 1.0}, {4325: 0.1196420633829523, 4390: 0.3233260863783556, 3377: 0.5170101061923318, 10019: 0.2935595585845699, 473: 0.4753025226886807, 4504: 0.35017977217772744, 5023: 0.32096612414408404, 2622: 0.2758835631758578}, {11513: 0.5960988271728855, 11185: 0.42131667344548324, 11187: 0.6834899040365866}, {11655: 1.0}, {4785: 0.2638291898989208, 3792: 0.47243869394280846, 5867: 0.40221902263838066, 6511: 0.3481148951439448, 4238: 0.40438392563487496, 6193: 0.5105931426466346}, {7067: 0.2729676590544566, 3222: 0.4069911174451026, 2171: 0.20589514673907536, 4870: 0.48957552568362167, 11359: 0.3371575545089266, 6186: 0.22223557363863836, 3964: 0.5609866432269867}, {10480: 0.24333474405998287, 11655: 0.2376441117889078, 4718: 0.5241948340769083, 7970: 0.37987029543693535, 1773: 0.6820790372362001}, {595: 0.44314814839056726, 8367: 0.8304665512540824, 6018: 0.2533297143695473, 8803: 0.22308985105469506}, {3173: 0.45678344870872245, 10020: 0.4808602004853964, 9637: 0.6670043626476957, 3222: 0.3394518062757241}, {11259: 0.17117743514102718, 9723: 0.698342976691485, 2171: 0.17979574023443362, 1878: 0.48987560114006384, 2441: 0.4590324169439762}, {274: 0.40703402445981623, 11613: 0.46320283734731443, 4475: 0.32260550795508747, 9912: 0.28221831378982964, 3874: 0.35968002871542204, 8407: 0.553782647748538}, {865: 0.2886817339772063, 4134: 0.5904082657631359, 9523: 0.35024735731215206, 4325: 0.1629116411898631, 5119: 0.647199754362715}, {1574: 0.350954676100288, 10220: 0.29755889160593296, 5771: 0.16881767237032647, 7211: 0.4064914276443842, 9359: 0.3144327549445066, 5929: 0.3736994667930627, 8372: 0.2471887050356972, 4680: 0.20287127597057342, 6569: 0.4064914276443842, 11415: 0.29755889160593296}, {11655: 0.15052064348588942, 11562: 0.19312004162743704, 9304: 0.22580448233726988, 7163: 0.3807415184441273, 3884: 0.49535613609046053, 11086: 0.3320598757350757, 4504: 0.25788273109262483, 5024: 0.3807415184441273, 8892: 0.17895039516562541, 10522: 0.3807415184441273}, {4448: 0.2285768589252939, 3902: 0.35744390726489395, 8142: 0.3838244826991643, 6919: 0.4288417971209254, 8892: 0.37772239740121916, 9336: 0.5882905258011059}, {7346: 0.17354956845147662, 11640: 0.18262088146475924, 6958: 0.34541171873453286, 6967: 0.5050052383136024, 9735: 0.29958379718737455, 4173: 0.4975389720211159, 860: 0.44817431284829184, 2171: 0.1550319962651089}, {10480: 0.29585759113649557, 895: 0.5157602319667931, 4537: 0.40243718895173825, 2569: 0.6960631995997869}, {2479: 0.27562711540190044, 6018: 0.18431804127791068, 1908: 0.42385850094124217, 11555: 0.2832557470151712, 2145: 0.5269750110389914, 9929: 0.2619349268560424, 4862: 0.2567234678937446, 6186: 0.16791208531532983, 10333: 0.43560616426821114}, {6935: 0.8701652994855291, 4451: 0.49275993300111093}, {5100: 0.3771386018991156, 10220: 0.300297309294944, 11315: 0.13649867018980902, 7896: 0.16674340710011645, 6843: 0.3440448584824405, 11655: 0.12465463147988962, 3506: 0.3053276606679554, 6362: 0.3771386018991156, 2898: 0.2722339172512803, 5334: 0.2329308939334666, 10400: 0.26265305954439994, 2839: 0.357780002991873, 6226: 0.1625648662053426}, {3775: 0.3277660863015552, 2258: 0.4172769566617356, 11655: 0.24088432842351312, 6288: 0.4331010046997528, 10767: 0.32644054850772125, 11257: 0.3409633819238876, 2171: 0.15687485810898097, 11234: 0.294269097973467, 1696: 0.3725910781350939}, {2960: 0.18535289458975007, 673: 0.39436371441817186, 6847: 0.39436371441817186, 1555: 0.25922180794602767, 4818: 0.16922466755106344, 3579: 0.39436371441817186, 2734: 0.18487228981671164, 1384: 0.17738920519100557, 269: 0.27663937079126344, 2722: 0.14894572106675635, 5946: 0.26432676834212526, 10564: 0.11615898685591086, 789: 0.39436371441817186}, {11705: 0.18498510169267499, 3450: 0.21193387740516714, 2920: 0.220394816032311, 10656: 0.19154791555389988, 5861: 0.2323198392564344, 2215: 0.2323198392564344, 4876: 0.195475170763668, 2919: 0.182181960103094, 8286: 0.2527058011077017, 5355: 0.20000885418104378, 10181: 0.16937893955749977, 7758: 0.16316418568827737, 6761: 0.17306007846855162, 8307: 0.12904273828464488, 705: 0.16459913984140775, 6371: 0.08127227829120587, 2247: 0.2527058011077017, 8287: 0.2527058011077017, 2710: 0.2527058011077017, 160: 0.21193387740516714, 6715: 0.14572175102042492, 9671: 0.2323198392564344, 7784: 0.18498510169267499, 4905: 0.12993404061700378, 2616: 0.2323198392564344}, {10430: 0.13635173229838304, 5611: 0.2700867454382938, 6128: 0.09879632718074352, 8124: 0.1797327994494524, 10892: 0.18284669899327433, 6291: 0.18815639016649974, 10633: 0.2580749556748342, 6875: 0.2089349170209357, 9336: 0.20549266077593314, 6371: 0.09028217170228105, 10413: 0.2089349170209357, 6165: 0.24482791723921435, 7259: 0.14920777983424086, 2722: 0.10602440828237882, 7133: 0.12070253169752762, 3328: 0.1516289774561194, 11547: 0.13244249827027374, 1881: 0.1464136663680561, 4461: 0.2580749556748342, 2123: 0.17555632765969095, 924: 0.17689003189123786, 7288: 0.228138622558592, 7051: 0.2089349170209357, 11259: 0.06881015238166935, 2171: 0.07227455109906128, 4543: 0.23788903999839298, 7970: 0.17739806555939902, 5749: 0.1251349005848797, 4656: 0.1464136663680561, 1170: 0.16102790080315837, 6143: 0.19449970339955852}, {3404: 0.5050300080085258, 3107: 0.5153845019444445, 2074: 0.32269609863464177, 4705: 0.6125281496326245}, {2744: 0.5151884011302551, 7346: 0.1484837420568542, 4205: 0.5845740731491617, 4207: 0.5151884011302551, 1112: 0.3246344929737055}, {10365: 0.6422273254564451, 9084: 0.7665142284635433}, {10188: 0.19992076213408103, 269: 0.20240944291550517, 7692: 0.251651714376211, 8295: 0.251651714376211, 7688: 0.1359729976228657, 4412: 0.0633164556166722, 10600: 0.1001688302097599, 10480: 0.11680370634753295, 6672: 0.18966548972342642, 332: 0.3754058529293499, 8686: 0.22837456865456507, 1707: 0.21475831128358036, 1057: 0.15997236091516315, 767: 0.2419908260255498, 4144: 0.2652679717471957, 9010: 0.23449725884896486, 4032: 0.18474227784610606, 9106: 0.251651714376211, 10157: 0.1303905092543473, 10181: 0.19340064928124473, 4376: 0.13323010046696823, 1197: 0.2652679717471957, 6851: 0.14684635783795297}, {1995: 0.47043937400856867, 6387: 0.511720218393481, 1198: 0.3745882216505674, 8124: 0.3276311156993569, 2487: 0.3745882216505674, 3906: 0.35896294223314956}, {6700: 0.28018568353457873, 9777: 0.19697511229252493, 8217: 0.33408791687019085, 213: 0.21526835545561854, 10854: 0.22096894354040345, 215: 0.2602364397684465, 8218: 0.33408791687019085, 9429: 0.28018568353457873, 9729: 0.12730748635368544, 7019: 0.21217287083844938, 9399: 0.33408791687019085, 9131: 0.2913714075991573, 9132: 0.33408791687019085, 10861: 0.14859300106397555, 4475: 0.16322187978605687}, {9729: 0.15277076951371366, 9280: 0.3362267507299998, 6349: 0.26604764808763826, 9325: 0.3173080584467385, 4325: 0.09277521157763463, 2529: 0.4009101868816776, 5601: 0.18603646305556626, 5432: 0.26604764808763826, 7782: 0.271543314578322, 6058: 0.15141823318151598, 5390: 0.32581504304446085, 10692: 0.3038850326541609, 4842: 0.3496497765225774}, {1589: 0.7316027171170958, 8903: 0.6817312258558226}, {8293: 0.38262057486953577, 8616: 0.4757047014932236, 1163: 0.22694488084163145, 8189: 0.27574787715705756, 685: 0.36559098499679127, 6618: 0.3386190957436039, 11607: 0.5014439167581585}, {6158: 0.7525247604749981, 3157: 0.6585639565539909}, {404: 0.33129257758005937, 9515: 0.24628725978467803, 5601: 0.2592151421031722, 5407: 0.40891300762103827, 11547: 0.20256978401479506, 2813: 0.48718683971416477, 3362: 0.4321015003954379, 3236: 0.36067746697069836}, {5456: 0.4493671896275695, 7517: 0.23150386591339567, 3658: 0.19148963630053703, 8584: 0.3453934122389198, 8682: 0.20867203628828562, 10520: 0.2528340671521266, 10160: 0.12296215063425298, 11150: 0.2896671337386432, 10394: 0.2570693505112378, 927: 0.2896671337386432, 11259: 0.11014863115171192, 7012: 0.2528340671521266, 758: 0.19148963630053703, 2175: 0.3453934122389198}, {8853: 0.23646167435809018, 2171: 0.0675367115589755, 970: 0.1708604838305246, 9046: 0.2623187185889546, 5812: 0.04796847403161778, 9693: 0.1277837446232571, 10290: 0.2411572756518033, 11315: 0.08728262571719936, 8803: 0.0704671895219581, 6451: 0.13681575884488487, 6076: 0.07781039549027603, 4412: 0.07488924586780428, 10785: 0.09642985495444072, 307: 0.19202192676767593, 4325: 0.0789771492683483, 4826: 0.10680902709255635, 6128: 0.09231989615192102, 1202: 0.1512649999749118, 6052: 0.14168918877684097, 6130: 0.11080550985837342, 10430: 0.09793272927491437, 5500: 0.14749112662045077, 1918: 0.20291103760150175, 2974: 0.18401207465114358, 6069: 0.14610318267175643, 7866: 0.2623187185889546, 6295: 0.1724264429120631, 4785: 0.09921987811929293, 7246: 0.21999583271465198, 4332: 0.1724264429120631, 8297: 0.19883438977750068, 10586: 0.20240391691106843, 4367: 0.11145280284307173, 6186: 0.07289661787898626, 7640: 0.1512649999749118, 10365: 0.0712167713008573, 6894: 0.20291103760150175, 8892: 0.12329109402793571, 2520: 0.19883438977750068, 5577: 0.1353500609660467, 6698: 0.19883438977750068}, {7970: 0.13487626780543557, 3810: 0.24217812236391692, 5953: 0.2066735953602534, 10209: 0.24217812236391692, 3: 0.19016493043793523, 4376: 0.12821456692009917, 4973: 0.17635096010143358, 1170: 0.1592850811478688, 1207: 0.24217812236391692, 10749: 0.2552817868944879, 8850: 0.34399336328160623, 6491: 0.2552817868944879, 7520: 0.19478960815153232, 3569: 0.23288092442139527, 6130: 0.11729535622737011, 9479: 0.23288092442139527, 51: 0.21479547784540848, 6330: 0.24217812236391692, 6049: 0.20018839744341826, 7918: 0.21479547784540848, 6610: 0.23288092442139527}, {7346: 0.14574074498649245, 2171: 0.13019034754175932, 2535: 0.3832925487611516, 4418: 0.2069887416331097, 4096: 0.44560236875745957, 11315: 0.1682544961721536, 7238: 0.3186373955754982, 8803: 0.13583942247681913, 7067: 0.17260122427750121, 6128: 0.1779648296103692, 4808: 0.5056711283607991, 4152: 0.21920866223256408, 4160: 0.24491021951088449}, {9735: 0.30363503232860944, 2782: 0.3940523242155946, 9752: 0.41336746412390024, 806: 0.38759070315244004, 5371: 0.5610678117518316, 9380: 0.34151461165325914}, {11347: 0.3288158579209693, 3530: 0.5548820400730788, 5193: 0.5954739827082464, 5194: 0.47895384861342777}, {6234: 0.48289896584364544, 4258: 0.5055311357003015, 4325: 0.17453742847170872, 8573: 0.6933855678388354}, {833: 0.4055585370937873, 7921: 0.4055585370937873, 5537: 0.4055585370937873, 3150: 0.35370378828392607, 6925: 0.22913968470200813, 11484: 0.4055585370937873, 11055: 0.4055585370937873}, {5812: 0.13532655711762634, 7675: 0.5744612038760951, 9729: 0.28200031069490505, 11259: 0.18139869449986168, 8892: 0.3478234322623787, 4017: 0.4442778651525733, 1856: 0.4699866222418787}, {7067: 0.5814521135843538, 1702: 0.8135806288305343}, {5616: 0.43372535910267873, 7399: 0.3073585930014385, 6291: 0.390591477269697, 1409: 0.6970067135096881, 6186: 0.16194094226673786, 6923: 0.22980008399452082}, {11590: 0.18978405223401668, 4624: 0.16206051237577862, 8669: 0.43779435931343263, 1696: 0.2057655123102033, 3126: 0.22833737451018316, 9646: 0.43779435931343263, 9576: 0.2258912118144188, 10388: 0.3318427855639257, 1658: 0.43779435931343263, 11659: 0.2877694909057068, 10160: 0.15585744848974464}, {3633: 0.2750988677384647, 213: 0.19225199483589686, 10854: 0.19734307953674876, 215: 0.2293436752992675, 3607: 0.2750988677384647, 3927: 0.2750988677384647, 7896: 0.1118169325746003, 10109: 0.2750988677384647, 10910: 0.11925548325079507, 7067: 0.0938997674692917, 3657: 0.23992470327214885, 8073: 0.2750988677384647, 9773: 0.17236625226747795, 11259: 0.06743207873697118, 10549: 0.16515322817874323, 7403: 0.21279684879552696, 3902: 0.12235631485721503, 10430: 0.10270400481897193, 3664: 0.21773227635333428, 4325: 0.0636610305607832, 2306: 0.2235696481819346, 8335: 0.20137722126312002, 3640: 0.2235696481819346, 3665: 0.2750988677384647}, {1384: 0.2071892806012022, 9523: 0.22916308141788685, 4566: 0.4606139037465056, 865: 0.1888813557794116, 2722: 0.17396750136638706, 4624: 0.17050773647616504, 3056: 0.4234558431528053, 10480: 0.14331532322412377, 4752: 0.2542161546227759, 4173: 0.2925271144839818, 4821: 0.2313514566843259, 434: 0.3645617105122659, 1063: 0.24677344523118702}, {9223: 0.1502583747188179, 2148: 0.2937755364612332, 5053: 0.40132303416854276, 7970: 0.194930987428579, 2038: 0.22674663483108276, 9907: 0.36894801140003936, 5885: 0.36894801140003936, 1691: 0.21767813456957985, 4046: 0.194930987428579, 3697: 0.2893238567490797, 3240: 0.3104348546476577, 2674: 0.20634873910144444, 5022: 0.24568480911065085}, {10784: 0.3613029436569679, 11561: 0.4027182128321725, 3728: 0.38471881516844114, 6749: 0.648695828998081, 6958: 0.37210667644367007}, {1112: 0.28500309348478553, 4704: 0.17132898324972287, 1841: 0.24040178166117385, 11655: 0.13743568378351176, 6923: 0.23204911476516377, 4250: 0.5132092332283587, 3902: 0.20116784971836157, 6371: 0.14546155001823605, 661: 0.29730045585439113, 4087: 0.2872434091075795, 3043: 0.41580726698890946, 9199: 0.3001465258234688}, {11315: 0.4580848587451028, 5786: 0.8889084667098629}, {6923: 0.3328037900200152, 2250: 0.7758678069807864, 4973: 0.5359764765725628}, {1500: 0.2545028225930369, 350: 0.2878986640806496, 9791: 0.3131616429194936, 546: 0.2878986640806496, 9785: 0.2626356852418057, 8307: 0.1599141600664189, 5578: 0.23307989468733412, 1696: 0.14718751971196864, 3227: 0.17206885344803807, 9326: 0.27312076880341385, 9623: 0.19441453378683357, 3845: 0.3131616429194936, 1881: 0.16333355106190073, 5357: 0.2478577899645699, 8661: 0.27312076880341385, 5828: 0.3131616429194936}, {9929: 0.4837894521200041, 1923: 0.5679580914633306, 7675: 0.6658613762328892}, {9223: 0.24857535877344075, 8312: 0.5032406256493619, 5446: 0.49413967513864887, 5113: 0.6639165197225964}, {11359: 0.32492896053509396, 10638: 0.36036874916082406, 6186: 0.21417516223683264, 4710: 0.5485435267118487, 7966: 0.6463625410071347}, {870: 0.4031776604748183, 1901: 0.5507758881484689, 11320: 0.4803536365664183, 8412: 0.5507758881484689}, {7758: 0.19513107149437608, 4349: 0.27783559837155014, 2397: 0.17617303812826668, 1499: 0.19513107149437608, 4771: 0.1416744955964027, 1057: 0.16755138709369577, 10082: 0.30221554770112324, 4241: 0.2148143435851219, 4152: 0.13101057624586465, 9294: 0.16126629082935695, 10019: 0.17159870129798174, 6894: 0.23377237695123126, 4695: 0.172467210412138, 7845: 0.16992348793263784, 5577: 0.1559358517236848, 8892: 0.14204280086740326, 10811: 0.14721875196840867, 7852: 0.22493293678741283, 6193: 0.22122710907128412, 4818: 0.12968314203029158, 10042: 0.1986512901676693, 4930: 0.14989139150852315, 6117: 0.12942356275620798, 8950: 0.1276582473042035, 865: 0.12392783179819208, 4947: 0.13101057624586465, 6076: 0.06890295287868406, 3869: 0.2456070584008572, 10564: 0.08901694184734829, 6851: 0.15380351208578105, 10400: 0.19349483080004992}, {778: 0.2011715675964917, 2183: 0.18820308746663234, 5029: 0.13197939517098717, 4579: 0.17595338382025, 7748: 0.13018313907303428, 720: 0.21257953299423, 11669: 0.16603332425297077, 3570: 0.2748179268729544, 10365: 0.07461017480096449, 9365: 0.18419984353779403, 3808: 0.21750992031577188, 10269: 0.2526481636592928, 10335: 0.17317039388844876, 2363: 0.18613887401830811, 4704: 0.10410100168069156, 9367: 0.2526481636592928, 8950: 0.11608527469629497, 9107: 0.2011715675964917, 11259: 0.06736321467104732, 6018: 0.08383184947452085, 5030: 0.13125245007363714, 2171: 0.07075476412164762, 7474: 0.2748179268729544, 8552: 0.23047840044563125, 3057: 0.2748179268729544, 6442: 0.2748179268729544, 429: 0.23047840044563125}, {10910: 0.1542478377399453, 617: 0.20305760839634637, 1737: 0.3558193246689348, 7663: 0.28162003052781326, 9304: 0.21102400058914583, 4325: 0.08234066933129398, 5480: 0.3558193246689348, 5780: 0.29841091420230614, 10279: 0.14696395643681587, 5601: 0.1651127130570153, 5867: 0.26694759345423097, 11347: 0.22294255146057745, 10811: 0.17333084714690836, 2722: 0.13438803810967123, 9511: 0.2436751351884822, 9642: 0.3558193246689348, 5771: 0.14777332580686448, 10767: 0.1906298527121989}, {7356: 0.4456930463264448, 10430: 0.4697966296163499, 4418: 0.6701579851746335, 7346: 0.3626804518636859}, {6197: 0.8693286747264102, 9729: 0.4942344133085263}, {5771: 0.25915179266536126, 6985: 0.6240044700181553, 1163: 0.2596311586119051, 6896: 0.30854134714199055, 9607: 0.573665572909146, 6527: 0.22750929386765195}, {5771: 0.2303363640628013, 5896: 0.40599205180793035, 2995: 0.3198196619537246, 10260: 0.3581010682644448, 7346: 0.1598485928804115, 10365: 0.15057365369816889, 3412: 0.43896510770699826, 3615: 0.28718723246545785, 10099: 0.41279292085347297, 4152: 0.24042827699501856}, {7429: 0.15316884478351409, 2316: 0.27876403030122643, 4084: 0.22654828314114955, 3874: 0.15184443771694017, 3657: 0.24312123784004394, 4204: 0.18881163724469655, 5779: 0.24312123784004394, 5962: 0.16532519966208156, 9977: 0.27876403030122643, 1752: 0.24312123784004394, 6205: 0.13659589008461964, 3512: 0.19814504131177904, 7219: 0.256275932037094, 705: 0.1815720866128846, 11502: 0.22654828314114955, 7114: 0.11844001326335725, 6348: 0.24312123784004394, 9256: 0.13501296796225168, 8794: 0.27876403030122643, 9795: 0.21129973550882902, 11545: 0.13501296796225168, 7122: 0.16532519966208156, 7217: 0.19814504131177904}, {8803: 0.14720944850081583, 8633: 0.42389120586024687, 1816: 0.4595821889038174, 2881: 0.547996793357109, 5520: 0.3508573520329498, 6186: 0.15228464464580915, 10831: 0.3752837000306031}, {9729: 0.19263957069594712, 9050: 0.4647542971003093, 10785: 0.18583801520275944, 10365: 0.1372477790612987, 10057: 0.37626045598211183, 6760: 0.4239723156680583, 7615: 0.30349433659063657, 3900: 0.18020699681251842, 2734: 0.23698846009608132, 3117: 0.28704469215790673, 1068: 0.29635395377475054, 7346: 0.14570187957906344}, {4865: 0.12689340359550064, 1482: 0.18610097195555322, 1148: 0.171008904698459, 9256: 0.11652282398015412, 3989: 0.2877601479837084, 10674: 0.12838049498581777, 1384: 0.14079583269541912, 9084: 0.11515235877951682, 4412: 0.06868508292973032, 11167: 0.15965575737068397, 4368: 0.15814185860008714, 9800: 0.143545384927818, 377: 0.162953718477026, 6262: 0.16476081685402397, 10279: 0.09936960327076283, 9457: 0.15045754278815598, 2722: 0.11821991536313008, 3329: 0.13219223760004295, 2995: 0.13873352505087913, 4882: 0.19552229773100704, 2697: 0.18610097195555322, 8430: 0.162953718477026, 3710: 0.2405870526738581, 11259: 0.058972562164170156, 10365: 0.06531685272170626, 5914: 0.13593115752936216, 7151: 0.09518266887862487, 4448: 0.06842805549305629, 7235: 0.14630173714470868, 9038: 0.2405870526738581, 8438: 0.19041723824766701, 9729: 0.09167806250547889, 4967: 0.13729729708338295, 6018: 0.07338989058962428, 7644: 0.14103621701270216, 2171: 0.061941665728854206, 4947: 0.10429459585337937, 10564: 0.07086440072326614, 3558: 0.156705630632591, 4785: 0.09100005585489787, 8779: 0.18610097195555322, 3739: 0.16876768434543846, 4418: 0.09848062998496693, 5022: 0.14728430485713717, 6312: 0.22117871912465006}, {5096: 0.5536780000293655, 8950: 0.49704125107351155, 3790: 0.6681247391129598}, {1518: 1.0}, {5609: 0.29159386490959044, 3115: 0.4234843840202439, 4771: 0.2442801802589072, 9826: 0.35685767733811496, 7399: 0.27484013578749394, 10855: 0.5210907450822029, 3995: 0.31900506408780377, 11259: 0.1277294684647552, 3658: 0.2888980325610363}, {4382: 0.36409021440455797, 2171: 0.19203388154031664, 1593: 0.28702837604010256, 2603: 0.4133044498570128, 2123: 0.358526808744955, 11577: 0.48080070363409494, 10512: 0.3670561344750939, 4821: 0.28794869704844217}, {4905: 0.35182303616746974, 6063: 0.6842527314678879, 3697: 0.4932949828547354, 1274: 0.4058063708112329}, {6570: 0.27989859792691685, 4826: 0.34675864718701793, 9084: 0.2759516926017568, 875: 0.851625433409431}, {7067: 0.11457529057970342, 5096: 0.2054941538137362, 11077: 0.24983397842493207, 2823: 0.2596519825585956, 11051: 0.2596519825585956, 8595: 0.26567412789304257, 5623: 0.2927530422654723, 5461: 0.33567210610601256, 2581: 0.2927530422654723, 1969: 0.19156007092832972, 3190: 0.24983397842493207, 3135: 0.30859319173358285, 3087: 0.2927530422654723, 2225: 0.33567210610601256}, {199: 0.5847048372775636, 11642: 0.23020004601794114, 9623: 0.2790034301931975, 4120: 0.32898107717862396, 7562: 0.3077733855874932, 1939: 0.39195439697274503, 9352: 0.3194448204956703, 9576: 0.23188814737118887, 10365: 0.12201195212506152}, {11640: 0.12896440880523843, 10564: 0.12525238803475366, 10483: 0.12820195248361438, 8366: 0.32716738085927466, 5812: 0.10116821282580062, 6825: 0.2906986200000446, 8189: 0.27969091182645145, 4390: 0.26593277216370276, 3313: 0.28501935022408986, 3222: 0.31966548324191, 7748: 0.15482907478771507, 2531: 0.35662790301025965, 8892: 0.19986307822500438, 1881: 0.2217874610329337, 1361: 0.3289323117830132, 6076: 0.09695075130193734, 9987: 0.1799704166323968, 747: 0.16376842473615352}, {10160: 0.20080588080748102, 5720: 0.31714337072799875, 11275: 0.386278655837971, 1483: 0.5185493732962597, 2130: 0.3386236534442878, 7114: 0.2396518243547933, 4676: 0.5185493732962597}, {9966: 0.45176521665684205, 10430: 0.2541552194475097, 7334: 0.4112923780089603, 6128: 0.23958878343925646, 10480: 0.21181467297706608, 9084: 0.22058953055763397, 4187: 0.3884992573068497, 6076: 0.15521063949286024, 1593: 0.3408359283423772, 3222: 0.34645745167282954}, {6018: 0.40333289829096886, 11640: 0.521707033743469, 3902: 0.5880811299555649, 7356: 0.46830004130790304}, {4028: 1.0}, {7356: 0.18073346776510352, 3: 0.34945859687001296, 1708: 0.5567837167638038, 4457: 0.46912127724692804, 11642: 0.26137862741862483, 1170: 0.2927119150311865, 6130: 0.21554905266806332, 9966: 0.3386307376857109}, {4416: 0.42846521934918824, 4518: 0.3293277024950688, 3222: 0.19984465709223123, 8803: 0.10548744192191895, 3004: 0.2874515357438556, 4187: 0.22409534123800942, 6186: 0.10912422926158734, 1455: 0.17623094261367728, 6702: 0.2922667045207531, 8467: 0.3610057997479919, 11184: 0.342475300760834, 705: 0.25577343849093254, 1837: 0.2874515357438556}, {8803: 0.17988878016646997, 7639: 0.5578099823974011, 11569: 0.5616057949411156, 8319: 0.584026524565955}, {3902: 0.4129086075430632, 9929: 0.4024444148095168, 11315: 0.30889783990029507, 4905: 0.4773358832861728, 5601: 0.43079170611047146, 4818: 0.3983667403087813}, {4537: 0.26438842909814536, 3623: 0.46495157219359273, 10430: 0.2332222849798866, 251: 0.38782128017833156, 10782: 0.3447764329936169, 11075: 0.6246999485797322}, {4695: 0.33329800681622573, 6758: 0.23192206706826005, 1202: 0.22800060683938903, 408: 0.3129398309257008, 2115: 0.33159807866589475, 6128: 0.13915309126022854, 1018: 0.24914686054831248, 5758: 0.1572006792200502, 10428: 0.39539104904328315, 8428: 0.39539104904328315, 11568: 0.23311886587930955, 3878: 0.20934235909919516, 7346: 0.11395664655673877, 7133: 0.1700076398378546, 5812: 0.09406775160308392, 4412: 0.08676202885561515}, {4471: 0.44590555601772475, 251: 0.2618132966458528, 7838: 0.21390879564099444, 11359: 0.1777988499372751, 2150: 0.28881065958479174, 1455: 0.18926512923402997, 3708: 0.22159351504149585, 6018: 0.12864577338691086, 3724: 0.2098162612345875, 8867: 0.31966401681290546, 3213: 0.2581759306884526, 10861: 0.2770672962004216, 7133: 0.18133145175814552, 6180: 0.17355657147556183, 6570: 0.13860651169706772, 11242: 0.3138829922914531}, {3016: 0.42444674874248733, 8398: 0.5562592660294621, 9084: 0.1960607279757099, 3900: 0.2156877567474532, 1192: 0.4098247616024821, 1628: 0.507447764553802}, {213: 0.30917548924768556, 10854: 0.31736286126705454, 215: 0.40395010918550345, 3904: 0.7087642107517772, 6186: 0.1969608349544945, 3902: 0.315238581834935}, {11547: 0.35642846563849606, 9897: 0.4830627834858082, 10861: 0.6457446751756722, 9084: 0.3184868753932396, 7356: 0.3481221348579363}, {5368: 0.3848595010056575, 1497: 0.4336617660825337, 11051: 0.39998375350155485, 2722: 0.19529766229494194, 3293: 0.47537577767875705, 2002: 0.39194775448631036, 11227: 0.21128783761035796, 11642: 0.20357979906706608}, {7346: 0.13889624453144678, 10873: 0.2991836399989983, 10400: 0.3085533318868143, 11259: 0.11812869910243963, 4947: 0.2089138487365867, 3316: 0.287537610057996, 1039: 0.48192302505249135, 2743: 0.2858116961950343, 10067: 0.38142722336291324, 8143: 0.44304594205359227}, {4448: 0.07273796032068157, 11296: 0.11800764030639914, 10564: 0.05789855554113921, 2246: 0.24359424649882003, 3294: 0.14389092453585522, 2038: 0.11106024454675414, 2972: 0.14899581863493375, 3685: 0.159748173965622, 4039: 0.1903423478329251, 2470: 0.11589439968585746, 6018: 0.08857109173699895, 9929: 0.08521212892547605, 3912: 0.15450743246987472, 11547: 0.1052914944040992, 8774: 0.1807103174944673, 11259: 0.08186067445174806, 865: 0.08060535785160007, 9256: 0.12386195232425211, 797: 0.14899581863493375, 10533: 0.11395996551271781, 4276: 0.14171049686228065, 2255: 0.159748173965622, 10910: 0.08521212892547605, 9729: 0.07490400454532843, 7512: 0.1807103174944673, 8042: 0.1378887713138549, 2760: 0.12483631667603433, 11626: 0.1807103174944673, 661: 0.1292071207839902, 2280: 0.159748173965622, 747: 0.07570279005650665, 3517: 0.16485306806470051, 6761: 0.13461502825485971, 4745: 0.10215039087663237, 5400: 0.16485306806470051, 8124: 0.1258532474325139, 8043: 0.13971992235393826, 10179: 0.1714344212134718, 5215: 0.10252238566006848, 3900: 0.07006984940622508, 3222: 0.1000371502560907, 3697: 0.14171049686228065, 4390: 0.12292878100700992, 1837: 0.14389092453585522, 11655: 0.059729706581222536, 5968: 0.07157052992468597, 5194: 0.1378887713138549, 2331: 0.1807103174944673, 4376: 0.0907612539750952, 2905: 0.14630127550270955, 5419: 0.14630127550270955, 7045: 0.13971992235393826, 11098: 0.159748173965622, 7067: 0.06709460121710724, 7346: 0.056653231788388075, 10480: 0.06115999573610331, 10160: 0.06997924661408089, 8682: 0.11875777882509295, 2425: 0.1378887713138549}, {8803: 0.11101492146738341, 5601: 0.19176735142062232, 851: 0.3799222900670773, 8072: 0.22808135770954327, 337: 0.29374478228299084, 4325: 0.09563316948472016, 6341: 0.2898950015607115, 6094: 0.24657024684007567, 3480: 0.4132603008738277, 9328: 0.2830123399766393, 6193: 0.302513826147225, 3942: 0.2547418099418195, 8413: 0.3270827930897412}, {4704: 1.0}, {6018: 0.3086906985599238, 11568: 0.5966369464171111, 7741: 0.7407660945215107}, {2847: 0.5149667881773583, 5749: 0.22955296109291937, 11547: 0.1867430729905266, 10861: 0.2290428855386654, 1441: 0.4318812930883158, 6541: 0.4491230911423662, 7114: 0.21879678363097071, 775: 0.39834155138245014}, {8765: 0.44967261989852014, 8701: 0.2890952810929667, 7748: 0.15005357049309426, 7688: 0.19420611707945398, 10514: 0.28611331320748523, 11259: 0.10101868436333752, 10160: 0.14671731264124335, 4964: 0.3671917856531404, 2740: 0.43574791027202897, 7970: 0.20017542477753542, 6922: 0.3067327960261955, 10188: 0.28554077365648906}, {8892: 0.4396167074929687, 9290: 0.603922751847247, 5086: 0.6648416806235076}, {7356: 0.4456930463264448, 10430: 0.4697966296163499, 4418: 0.6701579851746335, 7346: 0.3626804518636859}, {5707: 0.8004672420807492, 7346: 0.21143842223080456, 2742: 0.47367526675474686, 4418: 0.30029606994598956}, {408: 0.4055591598306258, 3329: 0.2815489464639409, 2669: 0.5124130769170625, 11620: 0.5124130769170625, 2734: 0.24021220867504595, 7967: 0.4164321440940352}, {10034: 0.2672912798266277, 11330: 0.31871288264385794, 10520: 0.23330344910250986, 1841: 0.22039595602655188, 6834: 0.1754922338162809, 5812: 0.05828089858352822, 2972: 0.2415804784180126, 10233: 0.29300208123524285, 6681: 0.200829822321869, 6371: 0.10250070231741362, 4977: 0.31871288264385794, 10480: 0.09916426625483347, 8747: 0.19931561837839198, 8805: 0.31871288264385794, 7521: 0.2522514251390992, 8682: 0.1925527930063662, 6670: 0.31871288264385794, 388: 0.19931561837839198}, {10533: 0.3006865159482089, 865: 0.21267946256487247, 4771: 0.24313550189160885, 10910: 0.17281273760490445, 9084: 0.12917266974329603, 8803: 0.10708875385582758, 11590: 0.17281273760490445, 11315: 0.13264311638832846, 3822: 0.16347006854083437, 2619: 0.3986448853334101, 10480: 0.12403429451109972, 10483: 0.09237689883295296, 526: 0.27000898716269683, 11019: 0.19651279538652527, 3928: 0.26203589256943505, 6577: 0.30836322986269693, 5334: 0.2944900999181142, 10274: 0.2873931119775168, 8725: 0.21357360465535752}, {4611: 0.4578457315183541, 10803: 0.42663555574371403, 11545: 0.25425631931757053, 9260: 0.2995868467332849, 9693: 0.25572861981607775, 10966: 0.524968211367209, 6692: 0.3361132013444899}, {4818: 0.2227868141883381, 11655: 0.15776159242228002, 4325: 0.12014553178055372, 3893: 0.3935364654796076, 10059: 0.3597219433046653, 5519: 0.5191857477881744, 7686: 0.4773026536853188, 8853: 0.3597219433046653}, {11445: 1.0}, {8072: 0.6132923499868671, 3988: 0.7898559953862389}, {2171: 1.0}, {10288: 0.168152382756052, 6018: 0.11564300306710894, 9579: 0.3485190002140059, 9201: 0.31793661402802825, 8803: 0.1018387455808865, 1420: 0.1970078554086089, 6120: 0.3791013863999836, 8722: 0.41364507157030644, 213: 0.16537073237708894, 10854: 0.16974996602973486, 215: 0.21606345820737408, 11683: 0.3791013863999836, 10480: 0.11795353393528397, 3042: 0.26593310988186236, 7259: 0.20149861545457978}, {1196: 0.2665025018373949, 7534: 0.3055731048327027, 6655: 0.3055731048327027, 8187: 0.3055731048327027, 2654: 0.2665025018373949, 2488: 0.18836129584677921, 915: 0.2363695435012962, 6527: 0.1114106142595605, 1758: 0.32083205116001295, 2734: 0.1432484722388058, 5749: 0.13621307753257567, 1560: 0.22029532740913946, 3900: 0.10892672567124485, 6127: 0.2809222989129849, 1380: 0.19564452148942163, 3186: 0.2809222989129849, 7086: 0.23162068707354927, 11298: 0.1479973286665527}, {1372: 1.0}, {11653: 0.2859966256046807, 1919: 0.23979450750664735, 11491: 0.2645138640880127, 8028: 0.3321987437027141, 6172: 0.3321987437027141, 7389: 0.3151469165907523, 7457: 0.36134903468878565, 1506: 0.36134903468878565, 8775: 0.3321987437027141, 3708: 0.1898682780326966, 4818: 0.15505780077998413, 4930: 0.17922012961202255, 4325: 0.08362030759901913}, {3141: 0.6417124925291602, 9773: 0.3995215694280718, 10861: 0.36897903436132384, 9223: 0.31060518727393716, 6919: 0.44268101645999036}, {7346: 0.18040376773788566, 4418: 0.2562190063828591, 10631: 0.6259401020696792, 3414: 0.524950010144307, 746: 0.4841825862468074}, {7074: 0.5906730641804271, 11049: 0.8069109809958622}, {10784: 0.27852312800446805, 10160: 0.17802798243352558, 278: 0.41938804039124855, 7970: 0.316013018847749, 6038: 0.36051308161651363, 2494: 0.38681852007545375, 5364: 0.41938804039124855, 7748: 0.18207622489074235, 1654: 0.35544903790712484}, {11640: 0.3453086279660136, 7356: 0.4032665905190023, 3245: 0.8474302380865392}, {6128: 0.133870120494138, 895: 0.20631858431258818, 3634: 0.2687827583593407, 7346: 0.12446845370298674, 6570: 0.12501718888181176, 3942: 0.18022160882828012, 4789: 0.16360453659527255, 7711: 0.29236832503211957, 1701: 0.29236832503211957, 4651: 0.29236832503211957, 2185: 0.23760433514075527, 4649: 0.2524236031451986, 3636: 0.1585809589326904, 408: 0.23140051962472755, 7353: 0.20781495295194868, 1844: 0.13705825293361207, 3373: 0.2687827583593407, 3227: 0.16064381960639096, 1917: 0.2140187684679764, 10863: 0.2549860862975064, 7133: 0.1257106073147757, 1696: 0.13741455755129983, 11590: 0.1267418007771606}, {341: 0.3367171674428623, 5812: 0.09453214892276503, 1328: 0.5169552017546977, 5387: 0.36745095734674843, 10480: 0.16084534407072523, 6896: 0.25561043554193336, 7644: 0.3030479205352642, 10873: 0.32093203880545207, 4734: 0.33378171043915034, 1326: 0.2792288844829523}, {9450: 0.1794156951842585, 2674: 0.13790281221970876, 6916: 0.1590624660657661, 2587: 0.39616995640859487, 1177: 0.23896504826322013, 4785: 0.101445969591392, 4543: 0.27987010760997005, 3988: 0.1906390494260713, 5968: 0.09765348822052033, 1771: 0.34894156117623215, 11464: 0.10978524025002967, 6018: 0.1064428975386795, 9929: 0.1512663241635728, 10785: 0.09859334962747127, 11680: 0.14427354000240628, 7381: 0.17798267215508887, 4621: 0.2122752692784196, 8820: 0.1906390494260713, 628: 0.20746353795688816, 4704: 0.132179026527854, 8945: 0.1858273181045398, 4325: 0.06206549911278237, 4072: 0.2465678664017504, 2722: 0.1012969742388817, 8950: 0.11329153589703424, 6180: 0.11037606799157754, 11642: 0.10559275220872626, 6851: 0.13649440187780526, 9532: 0.19633011965942307}, {10801: 0.569709598266809, 7133: 0.24495998184245657, 10660: 0.33589545274196875, 8176: 0.4777918180998384, 10091: 0.5237507081833237}, {11640: 0.6804931692778877, 4367: 0.7327544244602936}, {5097: 0.24080452193858437, 11585: 0.1865408030377719, 9331: 0.21258091843165, 9727: 0.30488300162086657, 9729: 0.13321104115238308, 6058: 0.1320316743560909, 2171: 0.09000314313972807, 10448: 0.27041042923641784, 7927: 0.321379473454561, 3210: 0.30488300162086657, 6186: 0.09714604963605938, 3209: 0.19561946661575708, 1455: 0.15688669706448286, 4902: 0.321379473454561, 7925: 0.2558990001422522, 6180: 0.14386547254020524, 8067: 0.24848119188654294, 7967: 0.284099905009414, 10776: 0.18166665461209655, 6226: 0.13853000154372871}, {9373: 0.834432726562581, 7346: 0.31727944517419354, 4418: 0.45061710854259923}, {10737: 0.6691170142026384, 3809: 0.42840510501006535, 10237: 0.5071825370464693, 4680: 0.3339421528063924}, {4376: 0.2988633617554525, 2199: 0.4666316697201877, 6767: 0.40183172084004243, 11618: 0.32510170491499046, 10365: 0.17572641544771128, 4624: 0.23960236353831746, 1461: 0.5122925756892049, 9987: 0.2739397316873669}, {3315: 0.7487955829946451, 11259: 0.22584837933502278, 5097: 0.487831133871355, 4127: 0.3877092586787699}, {11240: 0.2550482584224001, 9378: 0.3210438515516361, 2585: 0.3210438515516361, 1555: 0.24196512818587895, 8558: 0.23377995874753982, 3915: 0.19883169974356063, 4191: 0.3384147473587045, 6649: 0.2739772132214947, 5584: 0.3384147473587045, 9223: 0.1378233473221341, 6371: 0.11838738186636814, 11259: 0.09023103490637287, 2888: 0.30871900483563147, 11336: 0.3681104898817776}, {11359: 0.5404966209036705, 11568: 0.7558683444830998, 7346: 0.3694948559859689}, {7356: 0.15115952587372145, 3: 0.2922756723952292, 8995: 0.24742947335201626, 11253: 0.35792839729458215, 6664: 0.3722178079329381, 1345: 0.323499193516165, 10674: 0.22773919315814453, 7151: 0.16884826791991653, 6180: 0.17563882060155667, 7235: 0.25953038721849153, 10064: 0.24233055513095314, 3902: 0.18982288477083117, 9151: 0.323499193516165, 10160: 0.15193869230647122, 11259: 0.10461374001728448, 3222: 0.21720030619646624}, {4325: 0.16469430589260128, 6474: 0.7116946847218862, 10860: 0.5505162106204047, 3117: 0.40410192178374926}, {6088: 0.9238783016638122, 3900: 0.38268640388023967}, {6742: 0.41001665396551235, 6018: 0.2914732909288807, 4771: 0.4479294820225766, 6911: 0.7391135523591755}, {8696: 0.3249993193780723, 8123: 0.34342926469315865, 810: 0.3007135058324508, 6606: 0.3872104539745908, 4127: 0.18682192785990379, 6923: 0.17507838373718487, 5833: 0.4439774479749127, 5682: 0.3249993193780723, 2856: 0.2891833338424568, 11655: 0.13490853608847383, 9515: 0.1957462597416103, 9223: 0.16622850936710076}, {4418: 0.19694655077298387, 5529: 0.31014706486740207, 11547: 0.13410586867092478, 6128: 0.13015130544549947, 7067: 0.12622873131950185, 4826: 0.15057788070497807, 213: 0.1613190963190081, 10854: 0.1655910373406136, 215: 0.21076983408463348, 10474: 0.3399801727691745, 9256: 0.17911058525480822, 8365: 0.4196198037692468, 6570: 0.1215443018653806, 9730: 0.20597423466746062, 4704: 0.1400852316685495, 11315: 0.12304983165395425, 9777: 0.21803845278166772, 10861: 0.16448264792995096, 4278: 0.253258591971738, 5771: 0.15358507709815913, 11132: 0.3399801727691745}, {7346: 0.1387081907042275, 6925: 0.27191680145962643, 10339: 0.48127054182955087, 11471: 0.3809108029989143, 7961: 0.3722765402902629, 8156: 0.44244609512959365, 3622: 0.44244609512959365}, {11655: 0.23901359482202628, 3615: 0.5299080684087956, 7970: 0.38205939206748296, 5914: 0.44441718887127557, 2171: 0.20251384198854755, 3232: 0.4611078154049056, 9084: 0.25487575418199987}, {3222: 0.49292702460396975, 11653: 0.766597131039059, 4862: 0.4115237381947257}, {2857: 0.573495843408808, 5601: 0.36913999195086905, 3583: 0.731326318366386}, {9521: 0.2524364098904302, 540: 0.42815538136227754, 10480: 0.13321618468523766, 4785: 0.1619462194059121, 1202: 0.24689402303961355, 4474: 0.37341139813551844, 9196: 0.34795689551644915, 3737: 0.35907616566871936, 5506: 0.35907616566871936, 8747: 0.26775840967062076, 5099: 0.24073432466206807}, {4145: 0.19095406447167565, 10312: 0.15577742575928785, 10218: 0.1457622625111571, 7559: 0.16171082563739966, 10279: 0.09964974707179675, 11259: 0.059138818214084767, 4947: 0.10458862424383242, 9645: 0.24843696567160375, 11635: 0.22629364861591975, 4583: 0.13839527196760767, 10014: 0.2885714043336264, 7659: 0.2104171141931662, 915: 0.1866256297175217, 3764: 0.13031737648286043, 8838: 0.15088171423036298, 9934: 0.14872070627669848, 3328: 0.13031737648286043, 9983: 0.16716257999603115, 11240: 0.16716257999603115, 4872: 0.12925765655520793, 9156: 0.1960735161908815, 5801: 0.19095406447167565, 2056: 0.1960735161908815, 11458: 0.16522531223264764, 8906: 0.17661046646939096, 1563: 0.20233921870841895, 10505: 0.22180226842990952, 2974: 0.16924347592584152, 5985: 0.15577742575928785, 2622: 0.12874242658924728, 10478: 0.22180226842990952, 6237: 0.20233921870841895}, {6018: 0.1265555799605841, 9205: 0.21559837376031146, 4276: 0.2990938400340722, 5712: 0.4707504923394647, 9377: 0.3144702229634488, 5541: 0.36182908457788243, 11481: 0.4148750425641367, 10873: 0.2575594419161818, 10861: 0.1845248646484353, 3790: 0.23556702838914254, 10480: 0.12908414257382786, 316: 0.2279559917769652}, {8189: 0.943317938527754, 4325: 0.3318904440500639}, {10430: 0.2941406332606638, 6222: 0.6235775300168737, 9063: 0.7243150916024536}, {10775: 0.5973903616428294, 10708: 0.8019505943736494}, {371: 0.24862433011332324, 6052: 0.2584180749352741, 6717: 0.4172550664445375, 11598: 0.38881185278082975, 7151: 0.18927841104975615, 9289: 0.45564258357173654, 7271: 0.41306186087201197, 6570: 0.15724163656832282, 4600: 0.32764016795071077}, {3141: 0.704374026233659, 6117: 0.3899630793723453, 1861: 0.5931155265998546}, {11640: 0.19463007433465748, 6592: 0.28294549702297095, 9693: 0.3126198521007049, 9084: 0.20794795989313436, 7346: 0.24064140878317583, 9596: 0.6417564241077972, 9729: 0.24454759684181537, 1969: 0.36623509634719353, 11590: 0.2782016993215585}, {7623: 0.5802790230065559, 7773: 0.5352147684059472, 2245: 0.3495738812318385, 3227: 0.29221203023437164, 11051: 0.41137773163546476}, {}, {9776: 0.6767890591450134, 9835: 0.7361769959877907}, {4448: 0.1543874227504981, 8011: 0.4172176387676331, 2939: 0.4172176387676331, 6538: 0.4172176387676331, 10669: 0.4172176387676331, 8803: 0.11207798887204766, 6828: 0.32272978628202875, 6226: 0.1653329641097867, 1384: 0.18766915572303527, 6825: 0.21922403834814116, 3329: 0.22924314763382403}, {11655: 0.30538392839955664, 9515: 0.4430984391544381, 8479: 0.8428549279052446}, {5830: 0.4018770335254044, 11560: 0.4791902972155406, 2363: 0.32456376983526813, 4689: 0.2974871295214247, 2171: 0.12337257920056187, 4680: 0.2391537445013032, 10348: 0.33614376136649277, 6026: 0.4791902972155406}, {8803: 0.3471965347283621, 10861: 0.4418430106575009, 7760: 0.7393782217557536, 10430: 0.3708762130401899}, {6742: 0.7477290245899928, 9729: 0.6640039953085358}, {5601: 0.5361756201282891, 4325: 0.2673874023573205, 11259: 0.21769402828903633, 4785: 0.33592179153470886, 10365: 0.24111363424513046, 8174: 0.6501151873638689}, {11640: 0.2494058614698343, 9713: 0.41852015898676886, 1841: 0.4371028375629171, 8834: 0.7560282416554464}, {11227: 0.16163784062009792, 5046: 0.5146616559255974, 1053: 0.25541366544921323, 6720: 0.1718396930715287, 7346: 0.11401115116008326, 2995: 0.2967775071082998, 7792: 0.3955801615956899, 9982: 0.29984493848574334, 4930: 0.19619791677092477, 1192: 0.26793319744909444, 9227: 0.3955801615956899}, {4991: 0.32725459696250286, 11655: 0.25421556337752943, 9729: 0.24503576676872096, 4710: 0.3517783307274148, 6130: 0.2716241502156103, 8725: 0.34450671196117405, 4947: 0.27875705012024443, 785: 0.5911632025425198, 11259: 0.1576209901616312}, {8892: 0.27975013708763496, 4325: 0.13773767433448159, 358: 0.4511594956354965, 5660: 0.5952067995218314, 408: 0.47108783989644454, 3822: 0.24407310840681676, 4537: 0.25190620084391874}, {11344: 0.23915316752823979, 4325: 0.07638839549397977, 9693: 0.09043158755809298, 7728: 0.15568938339696883, 6018: 0.056628859487396946, 7356: 0.06575039464803614, 7970: 0.090169699479188, 7151: 0.0955535358464335, 11347: 0.08940255965165418, 865: 0.07612474920686538, 7346: 0.0696103733727969, 7960: 0.2415244690309572, 1068: 0.16074903428567844, 6851: 0.12291661323907663, 1508: 0.11288885225155373, 1384: 0.08350338210478145, 917: 0.07724011953551856, 10480: 0.05776029610184778, 2130: 0.1449971280235924, 10809: 0.1706651761745918, 1025: 0.1469291062034887, 9800: 0.11076200506410004, 10212: 0.14359860338624025, 10430: 0.0693062502459767, 6923: 0.11728007082293752, 11054: 0.25209315916796526, 9457: 0.11609554096506683, 10400: 0.11885744512552292, 3855: 0.1706651761745918, 8803: 0.06488118966479048, 11547: 0.06731922487229582, 6076: 0.04232479445007773, 1041: 0.21703210571013576, 2722: 0.07011402664394027, 9879: 0.1407135906193459, 6593: 0.12713218498753767, 10365: 0.05039956927609095, 9357: 0.15086825495864079, 8626: 0.1706651761745918, 6371: 0.059703672906595916, 4745: 0.09647215885057381, 3018: 0.1469291062034887, 6570: 0.06101349829290313, 4710: 0.10155623782026577, 8747: 0.11609554096506683, 10638: 0.11293206016718048, 1665: 0.19115917440683752, 1198: 0.13589246218101783, 7429: 0.10200172787061988, 1632: 0.1469291062034887, 6186: 0.051588383967365724, 4412: 0.04073584150169579, 7896: 0.07545579478047004, 2742: 0.11986253341513717, 6299: 0.18564096895221474, 8293: 0.13022395054724828, 8617: 0.1706651761745918, 11655: 0.0564095124056938, 6742: 0.07966004504533152, 6758: 0.08369543227574848, 3899: 0.1706651761745918, 10846: 0.11885744512552292, 3775: 0.09986081917796846}, {7346: 0.575710027815265, 4418: 0.8176539389454115}, {3630: 0.7609807092465419, 7100: 0.6487745064000511}, {7133: 0.34420577488710324, 10480: 0.32405553326119163, 9080: 0.4398557256985159, 6058: 0.3023483691397198, 4376: 0.36962826440462027, 2538: 0.5958168731239369}, {2739: 0.1344543219935242, 2171: 0.0857130979244815, 213: 0.17882626510588945, 10854: 0.1835618188938655, 215: 0.2133276843989764, 10499: 0.2634939050537786, 8199: 0.19045198550121353, 7688: 0.12058358634433207, 7569: 0.17151158170637495, 1342: 0.2352449672712428, 8990: 0.25588760779580294, 10157: 0.11563292349236988, 3995: 0.156651108265793, 3587: 0.20795688080916636, 3912: 0.1545964291373115, 10042: 0.16819916716139843, 7273: 0.25588760779580294, 7272: 0.2025271561239481, 7740: 0.25588760779580294, 4798: 0.25588760779580294, 10761: 0.25588760779580294, 8747: 0.1600261538225298, 4947: 0.11092739340021504, 7366: 0.10493039375168503, 9398: 0.17729374879035315, 7906: 0.16250934444634194, 4032: 0.16383316394841588, 10586: 0.1517579933256813}, {5605: 0.9494795283336656, 11655: 0.31382897456302533}, {9373: 0.3629165133425206, 11618: 0.24048041152105923, 4437: 0.28395324246163894, 6291: 0.3209140334422701, 5968: 0.1743278933349452, 4451: 0.3242922235672883, 11640: 0.14520589064443418, 6300: 0.38910655909403496, 4862: 0.20342588953209292, 1910: 0.4401650928929851, 11232: 0.2892642184080719}, {7346: 0.28491980576412557, 11640: 0.2998123621953028, 7067: 0.33743142523632486, 6785: 0.7357767541836794, 11359: 0.4167803414559252}, {8874: 0.5655913358535909, 5037: 0.4347258231851458, 1384: 0.19554453270274103, 10483: 0.10073783677133949, 6101: 0.4347258231851458, 8646: 0.1876813855802183, 11655: 0.17186255629057057, 8866: 0.4347258231851458}, {4187: 0.33174007478743556, 7346: 0.12877564097387093, 11640: 0.1355066525124372, 8493: 0.20681721725656976, 10475: 0.36311562195900865, 268: 0.37471930261341585, 8803: 0.12002689227775047, 7490: 0.2936942743089381, 2822: 0.4468079512196657, 9084: 0.14477891989835978, 11618: 0.2244171735417205, 758: 0.24771489276714373, 11536: 0.3270712976558837}, {6570: 0.3106965945543739, 11511: 0.5149280451550344, 6130: 0.3993157749937858, 9697: 0.6919997444264283}, {10198: 0.3691221600001558, 10626: 0.4046280940412469, 2734: 0.20632878386867456, 11259: 0.10788540382028887, 5724: 0.35769180229711106, 356: 0.3087464595269867, 9929: 0.19079829970066878, 11315: 0.1464480073902419, 7896: 0.17889727190247098, 2713: 0.25516752049217667, 717: 0.28930739397867405, 6130: 0.18591610865598598, 4600: 0.3014162282889029, 5030: 0.21020706401475675, 10365: 0.11949175639570539}, {6018: 0.11032727313375396, 5664: 0.24001104524472597, 2261: 0.3033221096371525, 7067: 0.12345099927681712, 213: 0.1577691816659958, 215: 0.2061317289904268, 6485: 0.3033221096371525, 3609: 0.36167533783660166, 6355: 0.1678301096534782, 5961: 0.3033221096371525, 2084: 0.3033221096371525, 4179: 0.3324987237368771, 499: 0.3324987237368771, 3103: 0.31543149859052605}, {11640: 0.11487486946293436, 5771: 0.15730836737842502, 11352: 0.24055500165838523, 7896: 0.15395864439422888, 316: 0.20812248806760028, 6216: 0.3176658132633531, 11240: 0.262439665728679, 9752: 0.2565531376503072, 3291: 0.348222151069876, 1593: 0.1896401186921494, 5056: 0.28710947545683013, 5030: 0.180903790614796, 9742: 0.3787784888763989, 8195: 0.25388059352754355, 2770: 0.348222151069876}, {2373: 0.46713531861721164, 3932: 0.5570029971715406, 10700: 0.5570029971715406, 7067: 0.19012238161528863, 1856: 0.35374199127777917}, {4913: 1.0}, {10782: 0.5093738599336743, 8646: 0.39845227842767694, 9084: 0.299057701663686, 2739: 0.4849489963516984, 7985: 0.5071124280129375}, {1643: 0.41158020281650703, 1669: 0.47057845962569056, 1992: 0.3806120994177081, 9313: 0.359474566553424, 1429: 0.5514636027259235, 11655: 0.17594888642004097}, {7356: 0.28284505551066524, 6226: 0.3164614727026497, 9101: 0.5299518179046742, 9391: 0.7341674751496216}, {11315: 1.0}, {7259: 0.5102763165722564, 6527: 0.35002588311824795, 10002: 0.6651703573056388, 1638: 0.41790950893061757}, {6067: 0.45937926714423705, 9704: 0.3131682392891486, 5022: 0.3900905678013418, 11234: 0.307739449281156, 7748: 0.2320083431637317, 2960: 0.29949091733683725, 11613: 0.44699047788349233, 4152: 0.276229963230089, 11259: 0.15619206867720972}, {8558: 0.2543806146450372, 940: 0.23615955205177688, 3173: 0.2743068246461986, 4957: 0.303610751527173, 6359: 0.4370462616405393, 3838: 0.3098356008147716, 5762: 0.24521054676112006, 11432: 0.22743228866080392, 152: 0.40054833260765027, 7356: 0.14186637299520327, 5508: 0.3493341889665925, 2171: 0.10312537873875736}, {10824: 0.7761763497404119, 11019: 0.38261768760140763, 3236: 0.5011526506347155}, {10861: 0.39429959436871215, 8726: 0.5537649650420544, 3636: 0.48084982776810203, 6893: 0.5537649650420544}, {5812: 0.16624225902238032, 6052: 0.4910460734228225, 5594: 0.5921437649011103, 7133: 0.39089167015860987, 4785: 0.3438620262884399, 5968: 0.33100700273154793}, {10365: 0.08414064201156392, 4991: 0.1577256306530435, 8950: 0.13091363968911046, 865: 0.1653454242835176, 5615: 0.19842911128726606, 2000: 0.24529394801815385, 10019: 0.17597461211660834, 8403: 0.28492063917615656, 3615: 0.16048038633375963, 270: 0.25991898068898567, 6349: 0.2056672568601512, 7523: 0.30992229766332746, 5746: 0.19842911128726606, 6511: 0.1546756652238586, 11480: 0.1952906310438121, 6834: 0.13116676191183763, 3209: 0.17342745280009517, 10171: 0.2122435532451836, 9912: 0.13245972359416613, 8397: 0.22686858591601541, 7615: 0.18605917343263276, 2960: 0.14566501143381275, 9795: 0.23491732220181477, 2687: 0.2122435532451836, 10279: 0.12800712017249158, 5097: 0.1640903735359512, 10910: 0.13435145583068892}, {4046: 0.5317813912457701, 5905: 0.8468816634717706}, {3604: 0.3355626310598054, 9450: 0.3207450964204521, 5762: 0.38188843862280947, 1441: 0.5231635345155645, 4160: 0.30212804654823466, 5328: 0.5231635345155645}, {4325: 0.11022852322302813, 1699: 0.3192664806594015, 9729: 0.18151072930780382, 2674: 0.24491583176764384, 2276: 0.4763313076150276, 7022: 0.3871089101056397, 2960: 0.22387806844862637, 6758: 0.21475192099204266, 6180: 0.1960282467360426, 9223: 0.1783420387474323, 1574: 0.31609779009903, 8748: 0.3871089101056397}, {6836: 1.0}, {6511: 0.3127066898764824, 4: 0.3613076628697362, 4325: 0.1449949461172532, 4159: 0.36941845800011175, 9205: 0.325608829969712, 6130: 0.2646671722252896, 8783: 0.5760220253478144, 9084: 0.2030263665104569, 4826: 0.2551212769550286}, {6178: 0.48396045805464627, 4599: 0.23051983653276598, 10483: 0.11956652106257054, 6894: 0.3991248795326315, 8892: 0.24251289448328162, 5594: 0.3360816189786245, 1389: 0.377706059240048, 7346: 0.14871173419052558, 6223: 0.4500062798505673}, {2045: 0.34030845931819953, 1689: 0.527928225741637, 2643: 0.3297703561076596, 8961: 0.44275151335117324, 9084: 0.13148372883046636, 9735: 0.20188087711324104, 8892: 0.19071725922519117, 8595: 0.3211600988464242, 4085: 0.3211600988464242}, {7346: 0.2558001195271031, 11655: 0.26969098850115647, 11298: 0.5592602788795911, 4052: 0.6605780920027111, 4785: 0.3357046237464842}, {9223: 1.0}, {8871: 1.0}, {4710: 0.4079404034216339, 768: 0.3541317258469447, 6058: 0.19066846887346173, 2650: 0.3757370719437113, 6570: 0.1659204421277785, 9084: 0.2128234823090554, 10483: 0.11698359902997427, 3284: 0.3826573106309619, 7133: 0.282407814931377, 6188: 0.46685453222155143}, {2582: 0.6118212213747055, 3412: 0.7909960765234956}, {7346: 0.14966447578699119, 7133: 0.2232788087831941, 7067: 0.1772481110486934, 7114: 0.22063154906955798, 6094: 0.30982958411543743, 7214: 0.386493462312403, 8858: 0.5192851481152343, 4948: 0.45288930521381865, 9401: 0.2616653978408384, 7356: 0.18392062709082957, 11259: 0.12728688155829213, 5812: 0.09495821066967983}, {10747: 0.2262953075641401, 1923: 0.1718228032589957, 10110: 0.28314997186755914, 5991: 0.3376224761727035, 4818: 0.14487654213417134, 2739: 0.23080444738106462, 5294: 0.3278099147920007, 4108: 0.21274528139297416, 1856: 0.214417602109752, 10033: 0.25128559952867796, 5785: 0.31038622402013133, 3964: 0.23683636693401955, 11259: 0.08275783024411797, 3666: 0.2262953075641401, 9547: 0.25128559952867796, 981: 0.31038622402013133, 11227: 0.13795577557082644}, {7697: 0.6859978550796424, 11355: 0.7276035615815317}, {1869: 0.9077452308225981, 4367: 0.41952186584113593}, {4340: 0.6689640106995858, 8811: 0.6689640106995858, 9256: 0.3239973839052549}, {6451: 0.3294308429720939, 9381: 0.6316222438242145, 1566: 0.3059117551053322, 4270: 0.6316222438242145}, {9729: 0.35608301127903097, 9605: 0.9344543269087353}, {2123: 0.47890284155583934, 11275: 0.5244298285500424, 4115: 0.70400669263631}, {11655: 0.10689879313981399, 9515: 0.1551053738681734, 778: 0.25752288194716494, 2183: 0.2409217269359626, 9380: 0.19686082704447105, 47: 0.35179874265841665, 5029: 0.1689488957517528, 10865: 0.3234188807172904, 9256: 0.17038565671609987, 9909: 0.2784378637649619, 5044: 0.35179874265841665, 3675: 0.21685569180143105, 8100: 0.2859027438882911, 10279: 0.14530333657027486, 7896: 0.14299243254269917, 8696: 0.25752288194716494, 5360: 0.2784378637649619}, {7067: 0.27498453459170136, 2081: 0.5247408870825208, 1599: 0.8056242965298617}, {4325: 0.05031521816357683, 5801: 0.17208717624371797, 2203: 0.24671069182146527, 2945: 0.15454713970702935, 10288: 0.0964410991129507, 5061: 0.12894865579333248, 1091: 0.2174275129523371, 7402: 0.19988747641564844, 4965: 0.17208717624371797, 4818: 0.09329990881665748, 8509: 0.11946706663365206, 260: 0.2174275129523371, 8220: 0.16182691260847615, 570: 0.19988747641564844, 2792: 0.16182691260847615, 6580: 0.19988747641564844, 7869: 0.2010704645029171, 7589: 0.3202317287330221, 1209: 0.16312141390119506, 4376: 0.10039292866582986, 9386: 0.09292777087862127, 8646: 0.09386858271995521, 747: 0.08373644555453812, 4418: 0.0890006266490057, 2674: 0.11179496147444032, 10480: 0.06765035541291547, 6128: 0.07652097999069528, 10806: 0.19988747641564844, 6604: 0.15454713970702935, 374: 0.19988747641564844, 527: 0.15064636851130536, 6699: 0.18962721278040662, 4579: 0.13920891942857433, 9536: 0.19988747641564844, 6412: 0.2174275129523371, 10483: 0.05038388828367914, 8572: 0.17670080933244892}, {10430: 0.22686522952908425, 495: 0.35229770184455994, 4826: 0.24742754160470284, 1967: 0.480953814195029, 9431: 0.6076721923617653, 2518: 0.4032567963503029}, {7151: 0.24644060789158564, 2068: 0.4721596430019263, 4870: 0.38133868328663423, 3708: 0.3273046369018493, 10511: 0.48184020471652134, 6226: 0.24684448928873162, 6018: 0.1900162021414551, 3900: 0.22204754055483958, 11555: 0.2920125502061419}, {5812: 0.04281919831703809, 10480: 0.07285636436513963, 4448: 0.06659994311674357, 5030: 0.17916494719957954, 1772: 0.20421990725029612, 9704: 0.16999045534746018, 6451: 0.12212898636328803, 5968: 0.08525783141807777, 7697: 0.15118946746435732, 11263: 0.20421990725029612, 11454: 0.15889347459867506, 11259: 0.07467529822047514, 7366: 0.09602049919620882, 5867: 0.13502714362110524, 9523: 0.1164982821321185, 5317: 0.3016575656362905, 935: 0.19637992202381763, 10365: 0.06357185871329896, 10483: 0.054261162421438046, 2171: 0.08905097665270027, 6130: 0.09891086170718855, 6462: 0.22300816368559043, 8892: 0.14318635866633633, 9576: 0.12082062687481514, 7384: 0.21526974581106825, 2449: 0.19029875703923005, 6134: 0.17428024490227342, 6076: 0.06945767658627619, 2381: 0.177490098236567, 5788: 0.17140893325197942, 10288: 0.07983112596602031, 4367: 0.09948866967878996, 2742: 0.19670203219959242, 7938: 0.17428024490227342, 377: 0.20634371437910032, 7281: 0.1622393060251294, 10501: 0.2554961690991404, 11158: 0.2656962250443515, 404: 0.1388718535701944}, {4862: 0.4014219020800983, 5502: 0.8239967814999555, 4537: 0.39986217701616983}, {6130: 1.0}, {5812: 0.10568041722396464, 6876: 0.21897030638381498, 5096: 0.2719348111568444, 4583: 0.2548044132405659, 2318: 0.3436030248703463, 8723: 0.330610649902614, 9532: 0.4230477985099492, 8228: 0.2603988054044604, 10480: 0.13820895182817278, 9817: 0.31160012885535093, 11315: 0.14780159112859426, 1455: 0.19935157022417846, 5196: 0.38740636551739904}, {7818: 0.3984534260654853, 2363: 0.3320821623455415, 5304: 0.4507384550545635, 6742: 0.2103876516470924, 2463: 0.4111863574848895, 6434: 0.29814721431332986, 8491: 0.2783881330029806, 6128: 0.172551822248965, 4904: 0.3439303998416241}, {5577: 0.355623598594948, 6130: 0.2911343657973506, 6257: 0.689225597813517, 7669: 0.5601256220641525}, {10170: 0.7598251489416799, 2970: 0.6501274821415829}, {7346: 0.15196618763847358, 9879: 0.399665223276198, 11491: 0.385971895110335, 5023: 0.32733640371504524, 1070: 0.399665223276198, 4793: 0.4847359479711283, 11500: 0.40785945147093355}, {4624: 0.12777031941610958, 4537: 0.14608105905792024, 2171: 0.08886557654574852, 10160: 0.12287975451255477, 10483: 0.11814524956569554, 8803: 0.12063349557870702, 6210: 0.14932004227845438, 10480: 0.10739362919247365, 5812: 0.08211771661770811, 4412: 0.07574008708962533, 11296: 0.20721500406291052, 10564: 0.1016667174306953, 5771: 0.14334727818675883, 10907: 0.28947302971329564, 3178: 0.2731850598787747, 2518: 0.22905262169894858, 10674: 0.18418307886784543, 7053: 0.3451619664039063, 3202: 0.3451619664039063, 6076: 0.07869442484859257, 7896: 0.1402948425123803, 7748: 0.12567396153860944, 1967: 0.2731850598787747, 1890: 0.31731749805860093, 5710: 0.3010295282240801}, {316: 0.354919274713061, 2462: 0.5632839679241686, 6076: 0.14727089101524735, 11019: 0.31842013372682, 6128: 0.22733269916062382, 1384: 0.29055350758673887, 8892: 0.303597581410989, 11257: 0.36146055049260356, 6834: 0.273380074500864}, {11640: 0.1164040231621084, 8803: 0.06980230885247453, 1886: 0.25984365684154165, 1938: 0.23888187894627352, 1384: 0.1168805802254142, 2579: 0.29483945005087314, 4367: 0.1436352853614611, 10636: 0.19695832315573725, 6916: 0.15410418769535306, 10430: 0.09700870237606425, 7346: 0.07489019246730746, 10160: 0.0925059185894469, 5749: 0.11582859753014817, 8226: 0.2615024839699994, 8362: 0.24746909492951571, 2700: 0.31079248992967395, 6222: 0.26756754824668305, 1163: 0.1081138243797531, 1209: 0.14983776589427042, 6816: 0.20565824703382787, 1808: 0.18469646913855975, 3724: 0.12927654290147209, 3630: 0.2111719158277297, 5812: 0.04751587600202971, 6186: 0.09394583396593745, 6128: 0.09144882814457532, 1202: 0.14983776589427042, 6076: 0.08750831027479668, 9627: 0.25624868632129383, 4387: 0.226620024929096}, {10336: 0.6457059515411655, 7896: 0.3832409218231747, 5029: 0.45280809199168387, 6018: 0.287618682897529, 11227: 0.3852668491572858}, {8307: 0.6329161771148054, 2677: 0.774220325712507}, {8803: 0.0905885381274402, 4117: 0.22602671887536685, 8142: 0.1610563834621832, 10068: 0.3100178282729097, 2974: 0.3077654981532474, 1534: 0.21590770793325428, 7679: 0.2941045519299193, 8757: 0.2941045519299193, 7618: 0.24311163352399232, 1804: 0.27405619474328785, 7499: 0.2941045519299193, 7366: 0.13828263009431577, 1283: 0.18695918541524142, 1455: 0.15134032234861322, 9304: 0.19999443159026392, 6715: 0.19445752428234164, 7748: 0.12278291889045993, 8012: 0.1965794988147149, 11437: 0.27405619474328785}, {10910: 0.18177599026465707, 4019: 0.24722794201573237, 11480: 0.34376595240009594, 10029: 0.28716311133280625, 7063: 0.3069506140431874, 9331: 0.25499061508715054, 9897: 0.2060836056688264, 3822: 0.17194874636832455, 10143: 0.3854943812321124, 11259: 0.13372481939979802, 6234: 0.3492905525522547, 8824: 0.31784046261002413, 8339: 0.2905301005120104}, {9729: 0.19089657838098284, 917: 0.20843664791399905, 886: 0.5009622138934282, 10212: 0.38750861229825007, 6128: 0.17630758419168766, 8892: 0.23545471611715243, 10257: 0.4605492256019356, 6069: 0.2790200190128595, 5029: 0.24058361383294496, 10861: 0.222814040925045, 10430: 0.1870266717300617}, {9897: 0.20490353069091016, 4543: 0.2715596953391561, 9195: 0.30056826317290786, 9912: 0.1781902694121096, 11234: 0.20135151582968996, 1984: 0.28551875926916376, 2699: 0.44082306826361867, 2074: 0.20192575373566576, 2698: 0.2607322041721975, 8307: 0.2128978725409657, 3874: 0.22709894463718552, 11232: 0.2518854997341518, 3209: 0.2333017441028839, 4134: 0.3496537055761386}, {11655: 0.23495480841628508, 4616: 0.33745487166300764, 4891: 0.5183280025864628, 747: 0.22888563266621237, 6146: 0.4597215880174137, 2956: 0.5463734601876565}, {1442: 1.0}, {9987: 0.557560321782266, 1018: 0.8301364270853653}, {1063: 1.0}, {4710: 0.291847073630714, 2126: 0.49044877307660123, 7922: 0.2527331141242884, 7151: 0.21106109735344464, 3715: 0.5334854332686063, 4785: 0.2017864373234938, 6327: 0.49044877307660123}, {2123: 0.4984864531457599, 7133: 0.34273089277789304, 9693: 0.3882915826003882, 1222: 0.6951808671471867}, {5903: 0.5619627576099882, 3891: 0.6444304122527859, 10253: 0.3893374963326392, 7259: 0.34252535201429174}, {4814: 0.28455709240124183, 9254: 0.33773090930443844, 9513: 0.22657911112409543, 10211: 0.19357088726743404, 1625: 0.24498096323766919, 4289: 0.2595873349807568, 8673: 0.3095268498217269, 8293: 0.21712776771061573, 806: 0.19657460510244842, 4973: 0.19657460510244842, 3478: 0.21712776771061573, 11259: 0.07587104621683977, 11324: 0.2595873349807568, 5532: 0.3095268498217269, 2283: 0.23461757756027174, 237: 0.18467806271930168, 2854: 0.2515488685445805}, {11590: 0.4591818556758847, 852: 0.8883422895585075}, {11640: 0.2973342694614137, 4325: 0.22687680893795337, 6128: 0.3450414485067547, 274: 0.6043406200388751, 4704: 0.3713770759645656, 3724: 0.4877674927682004}, {747: 0.4704700201895295, 9080: 0.6712211496308499, 6058: 0.461384513298516, 6186: 0.3394767433557054}, {}, {10533: 0.6602718265939997, 11486: 0.6972147030067677, 11259: 0.2791644191465089}, {7399: 0.4428863832744908, 1908: 0.5890369965299947, 4865: 0.4428863832744908, 1508: 0.5106258118612932}, {7644: 0.33518569378865865, 11427: 0.49866999659387756, 1276: 0.3832403898276361, 8820: 0.40641857516803076, 954: 0.5717775183929733}, {7410: 0.4105292930047297, 9653: 0.48105425940827273, 7675: 0.3290977274437021, 11486: 0.3376697205624753, 8470: 0.5515792258118158, 9704: 0.2710843085641136}, {7346: 0.1914062159995974, 7334: 0.40123001544436526, 8587: 0.5137118689286542, 1696: 0.31213727495561605, 7501: 0.6641148789843074}, {3636: 0.5290385389816264, 4418: 0.399250213118814, 8906: 0.7139834274366014, 4325: 0.22571033858166764}, {7133: 0.37435631787443857, 6186: 0.2419477898104319, 10937: 0.6734726315742692, 5606: 0.58970605287191}, {9408: 0.47868308870895243, 1384: 0.21531700189439126, 8950: 0.20219953799061383, 6188: 0.34024719862528147, 6076: 0.10913627229315875, 1531: 0.41747862855283185, 10019: 0.2717973875950179, 4862: 0.3004179958788189, 10483: 0.11092393477400114, 9693: 0.2331816726312427, 4325: 0.11077275231900156, 7114: 0.20338072783052205, 2081: 0.31178874530284995}, {9334: 0.35215102605330045, 9406: 0.49196448888607325, 8014: 0.49196448888607325, 10533: 0.19308923202168443, 6570: 0.10946359526825669, 6989: 0.29047166331844027, 7748: 0.12126625869328082, 495: 0.19308923202168443, 8493: 0.15416415452395923, 6371: 0.10711365304284101, 9511: 0.22808634863449, 6825: 0.17500204389809926, 5749: 0.14846404414151404, 512: 0.25762855502397364}, {8068: 0.41074575733722724, 7561: 0.4981750286548822, 11247: 0.47260360606178026, 3942: 0.3340314895579214, 7153: 0.4981750286548822}, {2989: 0.7017967095938342, 6686: 0.7017967095938342, 2171: 0.12232234794400794}, {1166: 0.3069765485734455, 7346: 0.1151079542597654, 9729: 0.17278837524038182, 7037: 0.32457610839572293, 5962: 0.18205705782643894, 10480: 0.0955126254834921, 10365: 0.08334090213645207, 4730: 0.2181984556368537, 7489: 0.21311759292917315, 4053: 0.4521213966685089, 9007: 0.31610147220609097, 4412: 0.0673609284524062, 2995: 0.1770167522243148, 8219: 0.25744850306740014, 4418: 0.12565615463565882, 10873: 0.19057475244418795, 6787: 0.24947626840077225, 2819: 0.3069765485734455}, {2582: 0.38059125701850605, 472: 0.43929713133194753, 3902: 0.2125322626933542, 4947: 0.20714613482318617, 4191: 0.43929713133194753, 5919: 0.26140871232962004, 6434: 0.29057919175456676, 5676: 0.4778452578689118}, {9084: 0.41824186850360806, 7346: 0.2518487192677288, 6197: 0.5856944616173194, 6226: 0.34627705118710433, 6758: 0.3939624682395098, 10910: 0.37880567195385306}, {4475: 0.24406347897456557, 10600: 0.1734219491290518, 4785: 0.18895335085197212, 9506: 0.41895787973983084, 7365: 0.49955716342429735, 1130: 0.49955716342429735, 9735: 0.24853800109710505, 6649: 0.37181032119943147}, {6186: 0.08152618824189072, 8827: 0.3328836871626737, 8945: 0.20326528306280356, 8345: 0.23842021803327626, 6087: 0.24603914683806505, 2179: 0.20852854542416982, 749: 0.26970570134445115, 10480: 0.09127978840906197, 6076: 0.0870216634802869, 2674: 0.15084355974629413, 4412: 0.06437569132927151, 5566: 0.2933722558508372, 5601: 0.1361350712029431, 6226: 0.11625612184246052, 6128: 0.10324881251209543, 2152: 0.3328836871626737, 1384: 0.13196211869353047, 1861: 0.19108710902050408, 6371: 0.09435094689522518, 3764: 0.15846248855108294, 2150: 0.200909616619381, 11677: 0.22237259233167894, 7989: 0.21835105302304675, 8174: 0.21475366352689018, 1047: 0.13973246069909967, 1844: 0.13752888190501153, 4127: 0.12344854602110182}, {11259: 0.10510059844813446, 1974: 0.3047711946741215, 6896: 0.21200842249479832, 9709: 0.34845883826917035, 10552: 0.30077690323226663, 4107: 0.39418358115896995, 10627: 0.27230557196913246, 4645: 0.39418358115896995, 4325: 0.0992229890444546, 8765: 0.35959414844576654, 7726: 0.33936062738732486}, {10316: 0.8971063753300506, 10279: 0.44181461196092026}, {9084: 0.2001686540301036, 5407: 0.6679582496331296, 6768: 0.5387630379054431, 11227: 0.25241790972086553, 4160: 0.2991922771626829, 6742: 0.26508084769759926}, {7907: 0.26367899046817916, 9576: 0.159089971685023, 7151: 0.12198298516549565, 11138: 0.25057496101584026, 11232: 0.18627897309066793, 6876: 0.15199113848621834, 3995: 0.18875473220244154, 4944: 0.30832847222164955, 10709: 0.26890556771924834, 3460: 0.28345538879887844, 6482: 0.30832847222164955, 11347: 0.14848745288113066, 5919: 0.1686733258743179, 6527: 0.1124152091288681, 6851: 0.15691450113821073, 8636: 0.30832847222164955, 9492: 0.26890556771924834, 8501: 0.2026694168058827, 2755: 0.30832847222164955}, {11640: 0.21653147822091573, 8380: 0.9288992804296317, 4127: 0.30043342982560683}, {7848: 0.42956293852652194, 3536: 0.7570268972859917, 11019: 0.492327085005491}, {8189: 0.25754837706616773, 9365: 0.3414617941095376, 6186: 0.1415715598116032, 9341: 0.38615364503904637, 10910: 0.22084495046624614, 8803: 0.1368533990524034, 1112: 0.3210158719362459, 6167: 0.5094457224348766, 11259: 0.12487504710525019, 9929: 0.22084495046624614, 1742: 0.25267240158453536, 3764: 0.27517270412843786, 4325: 0.11789157829547947}, {7985: 0.24272319188807587, 7338: 0.21922995222501743, 7133: 0.14599294450513225, 5601: 0.23273269197973856, 4367: 0.14426200889789859, 5518: 0.27593977354292776, 2322: 0.3395394673783886, 9912: 0.14511800001918218, 1040: 0.3395394673783886, 10785: 0.1248166420098726, 2758: 0.25271237741939323, 11052: 0.3395394673783886, 2856: 0.22115797907177207, 10288: 0.115757891241504, 2743: 0.20136898643928794, 6451: 0.17709125043399898, 6774: 0.25271237741939323, 2280: 0.27593977354292776}, {2479: 0.24195674222945698, 7045: 0.37702150276142327, 6923: 0.20916608360097982, 4733: 0.5304197012663344, 5968: 0.1931265655679484, 11464: 0.21711919139704594, 5023: 0.32929096287681686, 7145: 0.5304197012663344}, {737: 0.6040663021885807, 3889: 0.7969340641233844}, {1400: 0.2837671097309657, 5526: 0.30068600758496306, 6354: 0.32960905450359346, 9523: 0.17837568618936955, 5150: 0.2913748661921106, 2475: 0.20460572543621938, 6307: 0.32960905450359346, 10048: 0.35853210142222386, 4418: 0.1467596315989586, 1236: 0.2515041098348656, 11324: 0.30068600758496306, 6371: 0.11530689281920792, 3270: 0.25484406281233535, 1247: 0.26684821187696833}, {4818: 0.4414744339646815, 5040: 0.8972738289706014}, {10634: 0.24505215019648435, 1590: 0.4971115805647001, 8299: 0.43355085188327075, 2895: 0.4971115805647001, 3902: 0.22110138646069633, 889: 0.45700922568090363}, {10483: 0.11303363166256958, 5158: 0.3237000985675081, 4127: 0.2052567452442882, 7133: 0.20973557642138432, 3892: 0.3860686254277259, 1804: 0.39641907119407194, 4710: 0.2668475848793415, 3938: 0.4877873115704759, 8237: 0.425418784710258}, {8957: 0.8245366615637092, 6720: 0.46304432488172703, 6018: 0.32516034034334335}, {10483: 0.19659685996621518, 6527: 0.2377516431869347, 6076: 0.19342848308588653, 2209: 0.39654211300346087, 4818: 0.27982003496533797, 11259: 0.15984160452365317, 10480: 0.2028932831462731, 6211: 0.6520968246207669, 8793: 0.3740575648588334}, {8413: 0.36260380548956395, 1691: 0.24849580839503488, 10785: 0.16841492601756705, 10163: 0.3995623060324623, 7836: 0.4581401434758387, 3508: 0.5198425452849048, 8721: 0.3723251627203732}, {11454: 1.0}, {10430: 0.4314242448202505, 6419: 0.6191898584710241, 8909: 0.6561074913060432}, {917: 0.19871213073644295, 7896: 0.19412167986539758, 9912: 0.20412034368279114, 11347: 0.23000188539976058, 6058: 0.18037915674796823, 3952: 0.3620074332314801, 3708: 0.2509462916743112, 4278: 0.32706717073937575, 6355: 0.22161862297065812, 10480: 0.1485972779351807, 8185: 0.24818952064317795, 4412: 0.10479924048307145, 3469: 0.2438785633793071, 923: 0.40053497190185267, 5517: 0.3620074332314801}, {5535: 0.4596480212771276, 9729: 0.19052303129314846, 699: 0.3126770608967106, 5688: 0.4596480212771276, 5133: 0.3228226023870811, 9725: 0.4596480212771276, 9897: 0.2457258164452879, 4745: 0.2598259230028012}, {7318: 0.4939094629055544, 9693: 0.32867913439538343, 5968: 0.24566776611515262, 7748: 0.24566776611515262, 5609: 0.37756421727458983, 8078: 0.6202930180835957}, {371: 0.3720349569066224, 5338: 0.6004000919335227, 5968: 0.2606621353849928, 6877: 0.6581526962812206}, {10480: 0.09310560995999521, 10157: 0.17592996906132619, 4163: 0.2751004824948791, 10024: 0.2751004824948791, 7615: 0.1796464044581543, 2582: 0.18319114736932043, 4325: 0.06924766394495074, 6210: 0.1294539882870639, 4606: 0.24318921213208536, 5812: 0.05472010046130431, 6570: 0.09834954731425181, 11547: 0.10851394325813842, 3173: 0.2049283059065374, 9377: 0.22682059422226794, 2528: 0.15607275861531492, 11590: 0.1297208599200874, 3109: 0.16745005234129998, 5720: 0.16825069858387212, 6949: 0.2751004824948791, 7978: 0.23147103564193164, 3708: 0.15723375204576354, 10288: 0.10201889349856097, 3796: 0.1425274555446839, 5867: 0.17255574962527548, 6753: 0.22682059422226794, 10483: 0.06934217303332327, 7896: 0.12162953225974099, 9974: 0.2227186141800889, 11655: 0.11830036755008574, 2734: 0.14027979972410887, 5058: 0.13920812816281533, 1384: 0.1346016874788755, 1455: 0.13429484339972755, 9401: 0.1507858747139355}, {9729: 0.2293912268202971, 10279: 0.19110745282584196, 1702: 0.22098290574824114, 1566: 0.22409649786206942, 7356: 0.16387807630094833, 5771: 0.1921599321034514, 10480: 0.14396333683491744, 4704: 0.22803078309830105, 10776: 0.2404498390262324, 6785: 0.34437575658857544, 10873: 0.2872476506584513, 10329: 0.4626966125053015, 10316: 0.3880444640393976, 9897: 0.22740122435058957}, {4704: 0.34913226200101016, 9960: 0.7490385564515417, 10430: 0.3440952747252708, 4418: 0.3772755832885235, 2171: 0.23729618780143374}, {6742: 0.48202807607256287, 4325: 0.25995039133661074, 10549: 0.6743787512924091, 9515: 0.49526561330968416}, {1598: 0.5343693684626024, 6744: 0.42557231293156184, 10483: 0.1175886422403501, 11547: 0.1840151050031112, 4173: 0.3222680696778171, 9991: 0.36582918731419833, 1752: 0.4425622404750562, 1384: 0.22825401889690897}, {4821: 0.21330594431450267, 4967: 0.2423580775079777, 7847: 0.34513722552928866, 7067: 0.18859526570619076, 4826: 0.22497473534096288, 4862: 0.1804386111542432, 2171: 0.14225439761391062, 11315: 0.14130784834978335, 1921: 0.27661779351508137, 5031: 0.4246858226490449, 2908: 0.27661779351508137, 7133: 0.18260361370309666, 9223: 0.15900557915788704, 2397: 0.24756566032160635, 11555: 0.199086877456911, 6186: 0.11801735041554866, 8803: 0.11408418168888936, 5577: 0.21912752660642298, 3708: 0.223148142406746}, {3535: 0.3817780535955626, 8225: 0.40243512819263594, 2699: 0.3557532174130726, 6355: 0.20313079982355062, 7748: 0.1593848972454919, 2528: 0.22831352402990332, 11500: 0.33861109604988404, 9996: 0.26983689986144355, 6851: 0.2227789975996717, 7151: 0.1731850591389811, 1068: 0.2566156830003345, 10122: 0.27583754678560407, 3902: 0.1946983995154781}, {9450: 0.6261550640147296, 6692: 0.7796985544482623}, {1763: 0.47135110826194637, 6018: 0.3477559205980015, 6128: 0.3083821698325924, 8919: 0.6521679995940908, 11359: 0.3694202079800933}, {5876: 0.2861978047831012, 2020: 0.2861978047831012, 6697: 0.23369778184243842, 9365: 0.23604046179306984, 1696: 0.16551779113426374, 3900: 0.12553406141506016, 8439: 0.3237526594808136, 575: 0.2439980899315792, 9256: 0.17056150023424774, 6130: 0.14875594528432237, 6268: 0.3521618103589575, 7450: 0.3521618103589575, 1392: 0.3521618103589575, 9223: 0.13185204126696304, 8438: 0.27872522066162597}, {7354: 0.8111002575844645, 6186: 0.2988304713430037, 10638: 0.502808832006095}, {8189: 0.943317938527754, 4325: 0.3318904440500639}, {7133: 0.4074419019809008, 8406: 0.8264379450461132, 7366: 0.38857614375839317}, {7356: 0.21269949436604285, 9929: 0.38454540053040204, 4947: 0.26033434919558013, 5628: 0.5520939740748998, 10638: 0.28080060227869896, 10157: 0.27137770897003755, 1742: 0.38751553527729576, 11232: 0.3628207562961142}, {6570: 0.1826114548922038, 306: 0.3356805056250224, 11315: 0.1848734036701851, 6076: 0.16481026478074798, 4745: 0.28873809529364775, 7346: 0.1601359142968361, 7133: 0.23890075450164397, 10365: 0.19625296389863375, 5149: 0.34812831862999594, 2711: 0.3501099893126947, 7615: 0.3335601655535328, 4032: 0.35573654332924276, 10480: 0.17287472446715582, 11259: 0.13619264724743652, 4947: 0.2408604371526398}, {7356: 0.1506882735085341, 6210: 0.23946243748215731, 8683: 0.32249065793628995, 10549: 0.2554189903621696, 7423: 0.3911343936936685, 10453: 0.21725674983712093, 1844: 0.1994480486364829, 6720: 0.1848178460442556, 10480: 0.13237638106236252, 4862: 0.18076595179498933, 11257: 0.23807837086474973, 9141: 0.37105738802992927, 6186: 0.11823145024073055, 7224: 0.3568125258149792, 7569: 0.2851669019759117, 6076: 0.09700094176508939}, {5218: 1.0}, {4325: 0.14556396574079722, 3748: 0.453481083632675, 7137: 0.4134696190016507, 6478: 0.5485992133622843, 5334: 0.35716279308817644, 6371: 0.20230016631688688, 4862: 0.2672580095600116, 11642: 0.2476496601929681}, {8803: 0.12023584531929636, 9114: 0.36374776457472835, 337: 0.31814328865966746, 2394: 0.3542503619938693, 1050: 0.3753716459051662, 239: 0.3392645725709643, 9827: 0.32764069124052647, 5781: 0.39035743532807116, 5029: 0.21494995928084099, 8491: 0.2541402694510246}, {1112: 0.4819206251787058, 6834: 0.3236815179783923, 5215: 0.39889056990042404, 9080: 0.4202237848119049, 6058: 0.28885375041971106, 1053: 0.49380621779996775}, {371: 0.49973916648412936, 1400: 0.7611131536935937, 7133: 0.4134822036754903}, {4367: 0.40097979104474685, 3900: 0.33641867398224823, 5096: 0.5777566995286854, 5664: 0.6262865790670092}, {251: 0.2994068114334352, 8803: 0.1295564917580376, 4000: 0.5262280480771097, 2143: 0.3170122887197517, 340: 0.42061782589433455, 8195: 0.3232553405363162, 10610: 0.2506280536933919, 4475: 0.2356237792159955, 10600: 0.16742502903127782, 9996: 0.2972884614421085}, {10279: 0.6610768183080745, 1844: 0.7503182260185826}, {6826: 0.30119934493082323, 5526: 0.4556254459282391, 10359: 0.4043514911389483, 7346: 0.1565797609949814, 1400: 0.4299884685335937, 6018: 0.16572452657587675, 2130: 0.3261527636018281, 4135: 0.44151639914121643}, {7024: 0.5596614378577025, 4785: 0.25241177965554773, 5578: 0.4966797595548956, 5870: 0.6134953829611592}, {6527: 0.39869067562142013, 10433: 0.9170854623057415}, {5847: 0.2802993848105662, 3192: 0.2677773503244697, 4058: 0.4348347884033948, 10892: 0.283228290884447, 6512: 0.39975634790405123, 2034: 0.33422349204291535, 2145: 0.2914896481188454, 7655: 0.33422349204291535, 2207: 0.2446574886394857, 1984: 0.2288856983315903}, {4947: 0.6068983447541964, 7114: 0.457194294850089, 11669: 0.6501136638243704}, {3: 0.26940657406392937, 4688: 0.39339288832506925, 4325: 0.09103562254522267, 924: 0.24788776408920304, 9700: 0.3616575962233388, 8319: 0.3430936403943942, 7569: 0.2636760611030208, 2760: 0.24983632830932495, 9195: 0.2836068154016526, 11618: 0.19758851615852804, 4019: 0.23194076900129038, 10453: 0.2008837759507425, 1729: 0.3299223041216084}, {10483: 0.05975851323597133, 11432: 0.14642653830475397, 11655: 0.07836120775146589, 10634: 0.1653920188201393, 4872: 0.13816052759825373, 6346: 0.23707932857097044, 9248: 0.2578829334780705, 7563: 0.2578829334780705, 2171: 0.0663946720538161, 11640: 0.10175356525198717, 7615: 0.20142258969917567, 464: 0.1520453665491457, 4325: 0.07764165302656632, 4826: 0.13661191783207866, 8945: 0.17867622593848134, 4448: 0.07334737046997789, 3896: 0.2041063949133982, 774: 0.22490999982049828, 949: 0.2578829334780705, 747: 0.09931677884465685, 6834: 0.10914241921821835, 4412: 0.05658814626667415, 4704: 0.09768602797092224, 7819: 0.13760981017207827, 2258: 0.17660552511176603, 4660: 0.19193706616292605, 6841: 0.2578829334780705, 9494: 0.2926146560912489, 3117: 0.19050531849572583, 10480: 0.0802376473327967, 2259: 0.2041063949133982, 4862: 0.10956814632264904, 1638: 0.11225769912837005, 6186: 0.07166394285693378, 9260: 0.14716764404803806, 6434: 0.1568194162092357}, {10420: 0.25636007542947925, 8062: 0.23386459362034825, 1257: 0.16957293413361382, 6051: 0.25636007542947925, 5968: 0.10153170391381801, 5853: 0.20754656703235516, 2437: 0.2788555572386103, 7225: 0.2788555572386103, 2470: 0.1644106294386333, 11280: 0.2266226660179614, 10656: 0.21136911181121723, 5198: 0.2788555572386103, 8246: 0.2432010621354827, 11058: 0.2266226660179614, 306: 0.16847268910570282, 11518: 0.19320727384855804, 8252: 0.23386459362034825, 6435: 0.2788555572386103, 6371: 0.08968225640885345, 4325: 0.0645303715618647, 11507: 0.21136911181121723, 11259: 0.0683529163407268, 2171: 0.07179429450230015}, {11298: 0.24601382306410768, 4325: 0.11754539570838152, 4383: 0.4669730822996971, 9913: 0.4430032617743063, 4656: 0.3446790690958159, 2739: 0.266898586176501, 8803: 0.13645153604893792, 5601: 0.23570659979314887, 9652: 0.4259964034598737, 6592: 0.22395116313503344, 11259: 0.12450835791933969}, {9162: 0.65341475613503, 9861: 0.7570001033454351}, {5006: 1.0}, {9223: 0.33933365345314104, 4799: 0.9063216053957244, 6186: 0.2518607139413778}, {6040: 0.3727384464370203, 5771: 0.15479991126769513, 6041: 0.2633831382942161, 2711: 0.1805283725057764, 10480: 0.08913996631946015, 2171: 0.07376112121185847, 1638: 0.12471262371264512, 5265: 0.2633831382942161, 1859: 0.20364012493603822, 7114: 0.12172466663157569, 11669: 0.17308805007639383, 2283: 0.21715963343369243, 9667: 0.20971931100430988, 7482: 0.2864948907244779, 4123: 0.23283106343457172, 7151: 0.11334503671821593, 11019: 0.14122823071751525, 7819: 0.15287753631520654, 3187: 0.22675187736630004, 7404: 0.22675187736630004, 10360: 0.2864948907244779, 646: 0.2864948907244779}, {11259: 0.24403192797989848, 925: 0.7034815040298531, 7588: 0.5800211301752125, 8646: 0.3303599251794604}, {3159: 0.5839203057424631, 1376: 0.4897097492386744, 8747: 0.36517016776134276, 7336: 0.3128347286447036, 3157: 0.3128347286447036, 9450: 0.30023499417450517}, {6527: 0.24187391297130267, 8509: 0.28017151683628083, 4127: 0.21456417849010515, 4312: 0.46877167944268755, 7133: 0.21924610370998412, 5006: 0.4402404272492543, 11259: 0.1249879152756852, 11432: 0.2895259350101911, 1158: 0.44470953657078666, 4448: 0.14502812305554427, 2722: 0.19258451407067104}, {11259: 0.13589869612836092, 1844: 0.2599036728658016, 6371: 0.1783055115170077, 2983: 0.7213148186345629, 4367: 0.2355587588938, 3203: 0.4058439590207371, 6094: 0.33079164158796465, 9084: 0.17964783353174651}, {11464: 0.19621838295605587, 9735: 0.1833080032708583, 8228: 0.21598928917694543, 10580: 0.25528070195738073, 10613: 0.3090003623216366, 2171: 0.09486028932558685, 3107: 0.2850035083130345, 4412: 0.08084937783719058, 1068: 0.21598928917694543, 5029: 0.17694361314541357, 395: 0.18878961908600053, 11227: 0.15055055225534966, 10561: 0.21598928917694543, 5919: 0.20156104159312493, 7193: 0.2697089495412012, 8803: 0.09897636162188216, 2659: 0.2198319432546754, 751: 0.261890828831729, 8892: 0.17317142900191762, 3915: 0.19901290914855901, 3487: 0.2850035083130345, 11655: 0.11195727924043919, 6825: 0.19359731481732964}, {9521: 0.3845485047152251, 6423: 0.6522296516580373, 4785: 0.24670045237871271, 6230: 0.45190246222066666, 11185: 0.4020472365958474}, {7130: 0.5851928808601923, 3329: 0.34975316185336713, 11162: 0.5173113269415822, 11559: 0.5173113269415822}, {11019: 0.5002726428975699, 3192: 0.42309269580704834, 4517: 0.5341909085350504, 1177: 0.5341909085350504}, {9223: 0.20695607811281602, 6226: 0.2849821682037745, 11259: 0.13549127539789882, 7151: 0.21868510929267757, 4785: 0.2090754272255663, 10564: 0.162813140246036, 5058: 0.2571448994877672, 7133: 0.2376704511965298, 11575: 0.3668140131092073, 6355: 0.2564983456481298, 9675: 0.39289821828479793, 5334: 0.3138562484922997, 7318: 0.40462724946465944}, {5353: 0.4012615683889185, 4366: 0.6926945868976949, 6069: 0.3225624372574233, 3812: 0.5050909196246233}, {11050: 0.37279634961447367, 5762: 0.2808224526994118, 8102: 0.4587199313947907, 9223: 0.17174820649233402, 3708: 0.24103112257714476, 6570: 0.1507647148635657, 2904: 0.42171465893630605, 6016: 0.4587199313947907, 464: 0.2704569828319054}, {11640: 0.0711109429565032, 5812: 0.06333437394821183, 4785: 0.08868825722688152, 5745: 0.23447509138916633, 1202: 0.13520908795180855, 4160: 0.11356263563653991, 7151: 0.0927646136229731, 9328: 0.16057517293572426, 758: 0.1691278965810678, 6076: 0.0695512683644905, 10924: 0.23447509138916633, 2086: 0.2044950863983141, 8892: 0.11020445165016819, 1292: 0.23447509138916633, 8057: 0.2044950863983141, 10114: 0.17451508140746186, 11303: 0.13059516794487203, 4412: 0.06694018201741297, 10600: 0.10590169113867849, 2999: 0.21555981422821685, 10033: 0.17451508140746186, 8854: 0.23447509138916633, 11259: 0.057474401673817004, 7922: 0.11108010891372229, 10580: 0.1624579177883416, 2683: 0.15012370634234037, 9613: 0.23447509138916633, 7346: 0.06757865455085006, 2102: 0.17163990076562705, 1881: 0.12229355088037283, 4367: 0.12961215629125672, 3195: 0.17772925990631794, 5365: 0.16448024850314186, 9773: 0.11292051244433521, 3490: 0.1555998042465124, 3254: 0.16057517293572426, 4475: 0.1145550714257574, 4826: 0.09547186157141291, 5022: 0.14354264062739214, 10789: 0.21555981422821685, 4695: 0.1338093464437281, 8803: 0.0629875015857696, 4152: 0.10164505787960255}, {6180: 0.24051100588030738, 10077: 0.4154051722609722, 7789: 0.5096964275090365, 6758: 0.2634834591061404, 8903: 0.474951722034753, 6018: 0.1782745769663508, 9848: 0.42780609441072076}, {7259: 0.7491243150825666, 8892: 0.6624294381683797}, {9401: 0.3565643222094321, 3148: 0.5438888825400479, 3254: 0.3724704865419288, 5106: 0.5000129684843025, 4418: 0.22263259472642283, 11248: 0.3724704865419288}, {9888: 0.44121425409866033, 11458: 0.30215599761089346, 8003: 0.3848005623007598, 3902: 0.19623981239105137, 9693: 0.2149294182022512, 11359: 0.18601456384643728, 8922: 0.29572887582835444, 6602: 0.44121425409866033, 6444: 0.40562117743455484}, {9870: 0.721883597420651, 10218: 0.4361314226013084, 10104: 0.5372834019349362}, {4325: 0.2437059888090216, 2722: 0.39775204619541105, 11298: 0.5100586173470254, 4865: 0.7226623745816724}, {4649: 0.6539439168730861, 4325: 0.22804105085650878, 11381: 0.7213561067246506}, {11655: 0.08307031883268715, 11585: 0.14587946170707472, 3192: 0.14828187811808508, 10673: 0.2733803896116088, 7557: 0.19708679031061832, 155: 0.2733803896116088, 2360: 0.2733803896116088, 4285: 0.26036116644525065, 566: 0.3556760871110089, 5152: 0.2733803896116088, 7667: 0.19708679031061832, 8580: 0.2733803896116088, 10279: 0.11291422608072847, 7242: 0.2733803896116088, 5029: 0.13128902791421118, 8171: 0.23842594990375057, 5269: 0.2733803896116088, 9735: 0.13601129269666346, 3762: 0.22217306100806525}, {1791: 0.4051093043967645, 4448: 0.13211374440504722, 4049: 0.314614024285849, 5708: 0.3774940363538902, 2096: 0.42702877578093557, 3352: 0.4645003596126311, 797: 0.3520856081175445, 1384: 0.20893745187532423, 6076: 0.10590271292783647}, {1763: 0.16543727885661685, 5919: 0.12931757387979492, 4105: 0.2682243761014525, 5812: 0.043226631284786765, 11359: 0.12966103800674178, 3294: 0.17303992246052094, 1647: 0.23638764326896095, 11347: 0.1138415754752238, 7151: 0.0935212702687256, 8950: 0.0998520177192548, 7490: 0.15538151717301382, 9701: 0.1658218707906097, 3222: 0.12030238028786831, 4437: 0.1823956184460741, 2472: 0.17917895090809757, 6094: 0.14103982266752202, 11259: 0.05794320530796886, 8397: 0.17303992246052094, 3878: 0.12515697313726004, 1704: 0.14471348033380235, 2553: 0.21731807914867315, 10533: 0.13704563828145067, 6186: 0.06569054543794627, 6076: 0.053894668123998235, 4325: 0.05470280959732469, 11224: 0.23638764326896095, 11640: 0.09327211056476171, 1163: 0.12796205057904436, 10878: 0.19824851502838536, 1047: 0.11259083438901339, 3241: 0.15134822607731824, 3229: 0.20616309923320716, 3189: 0.18285260857437793, 4862: 0.10043532364565012, 1384: 0.10632980323334669, 9223: 0.08850531880083627, 7887: 0.23638764326896095, 7592: 0.1335585004183364, 1764: 0.23638764326896095, 758: 0.13105572437554952, 1015: 0.20616309923320716}, {3902: 0.0889832180382311, 4821: 0.10048603672323311, 1861: 0.1924859431333396, 6599: 0.2392924072220527, 11259: 0.07243766035295847, 3804: 0.1303115384186052, 9615: 0.12087061349329653, 8133: 0.18764975012479013, 2702: 0.1370099812539131, 10785: 0.07354493249041823, 2856: 0.1303115384186052, 6906: 0.18392535761631668, 11078: 0.20006472537693323, 3900: 0.07131647096218496, 7520: 0.14034196788986186, 6049: 0.1442316862410409, 7918: 0.201341927206436, 3101: 0.20006472537693323, 2708: 0.174484432691008, 9304: 0.11865139355511566, 11655: 0.07909269082505607, 1637: 0.15475579185526853, 9987: 0.08467232721983828, 6128: 0.07041035717852422, 2659: 0.11936788657385046, 7336: 0.107184479558541, 7346: 0.05766115660285074, 1420: 0.10396776140469324, 6593: 0.1370099812539131, 10365: 0.0543154673413299, 5919: 0.14239364562320495, 10430: 0.0746911417271016, 237: 0.11936788657385046, 10634: 0.0986223074433201, 6876: 0.0986223074433201, 8803: 0.0537437777815848, 5194: 0.14034196788986186, 2663: 0.174484432691008, 7122: 0.11865139355511566, 2692: 0.18392535761631668, 2005: 0.11536649325640921, 2889: 0.15834506493039144, 9713: 0.13246687199886653, 9405: 0.1442316862410409, 7429: 0.10992696164854178, 6228: 0.174484432691008, 1818: 0.12606632940915835, 11359: 0.08434666896261656, 9256: 0.0968967636481102, 11019: 0.0986223074433201, 7819: 0.10675723480394597, 9084: 0.06482685630541549, 6996: 0.20006472537693323, 1192: 0.135507254334467, 377: 0.135507254334467, 1702: 0.09555048201484731, 2171: 0.05150876660113618}, {5707: 0.8004672420807492, 7346: 0.21143842223080456, 2742: 0.47367526675474686, 4418: 0.30029606994598956}, {2931: 0.8439190634811862, 3658: 0.5364705157723373}, {4160: 0.37121071750000556, 8509: 0.32368915552286015, 7970: 0.372279012035206, 3254: 0.40343737755686737, 8510: 0.5415836369280325, 11248: 0.40343737755686737}, {4143: 0.41262494361968194, 1284: 0.44883259762992445, 5720: 0.2523602808236973, 8824: 0.34020963559919687, 4325: 0.10386500660394829, 11680: 0.24143803561290902, 1213: 0.3647610285335813, 3614: 0.35523716977436154, 8751: 0.3471848619281113}, {5150: 0.5243328068826506, 4947: 0.27968717031817997, 2792: 0.48019677803237903, 11419: 0.6451831125760288}, {1455: 0.5968372054134653, 11454: 0.693623274989603, 11640: 0.4033262979610215}, {1112: 0.25434417471204435, 4251: 0.32803278429100635, 1763: 0.320723405633605, 6371: 0.1298136712810735, 3928: 0.34518753610362, 161: 0.4580012703863378, 2722: 0.19834059053748898, 4599: 0.18033046463839555, 1393: 0.32803278429100635, 1083: 0.3710772218941216, 6250: 0.24881110059849645}, {9420: 0.2862415776041671, 9515: 0.27349810658112944, 6742: 0.26618800615651944, 7805: 0.399870738481059, 8072: 0.3423638205228159, 1036: 0.3773709479736744, 851: 0.43833438440647526, 9151: 0.3614070925556427, 5968: 0.17360283912479396}, {9223: 0.3506369639635549, 6149: 0.9365114625579447}, {1704: 0.26876004905024203, 2323: 0.4036003935278111, 3192: 0.2381229280550958, 4517: 0.30065067192459666, 5133: 0.2834589149347033, 6923: 0.17312194089134494, 4675: 0.43901614730909955, 2760: 0.27881079085898053, 3476: 0.3828835056309838, 7621: 0.30065067192459666}, {6570: 0.10350478020999987, 9010: 0.33298115654842486, 3821: 0.3149258480005134, 6278: 0.21330465378692645, 10861: 0.14007024649501493, 5058: 0.14650506385045384, 8969: 0.20163265635057562, 5771: 0.13079008562810068, 10480: 0.09798596900944558, 1518: 0.19150739045257006, 2957: 0.28952054944711664, 10434: 0.3149258480005134, 4537: 0.13328438784820865, 7259: 0.16738826240010196, 5674: 0.24925410392035738, 776: 0.2387099523403232, 3307: 0.28952054944711664, 5105: 0.2641152508937199, 92: 0.28952054944711664, 5812: 0.05758838882317535}, {7340: 0.3700889318177146, 117: 0.48825182840359554, 188: 0.48825182840359554, 8521: 0.35740894642295956, 4485: 0.3382891330966497, 1566: 0.23647358087961834, 8372: 0.29690745975578414}, {6128: 0.19270433091324052, 10580: 0.37937579539638083, 6300: 0.4449892204514128, 5805: 0.40753198563013454, 6570: 0.179960648778072, 8366: 0.26295808285797434, 7: 0.5475521015374459, 6130: 0.23129035591223432}, {10288: 0.32804491196511415, 6371: 0.3513411538171953, 4448: 0.21035246235411498, 1702: 0.3532222178009344, 10480: 0.2301130440233485, 3050: 0.7395808426209137}, {4325: 0.15668501487123443, 5215: 0.3531420572407382, 7122: 0.4015548930925613, 9223: 0.2535053920369491, 11598: 0.5502583502272493, 7934: 0.5678423421772811}, {251: 0.3358311471837192, 4120: 0.39598829474651615, 4416: 0.4536761822696153, 6702: 0.4026215884874804, 4475: 0.2642885901594241, 10600: 0.18779312099698814, 8228: 0.3171167677238132, 3820: 0.39598829474651615}, {7424: 0.5178000223598558, 4407: 0.5486725420659265, 1992: 0.43003384920775445, 9656: 0.4958956210359284}, {6774: 0.22661463646097899, 9912: 0.13013158735677993, 2488: 0.1876844312471514, 3316: 0.18166392297840164, 5432: 0.20205241167893248, 2489: 0.3044750468886341, 11315: 0.10130951272085993, 10856: 0.25535059732454113, 5374: 0.24098261689276002, 2207: 0.22288110231221522, 6779: 0.22661463646097899, 2491: 0.27991282210658763, 311: 0.27991282210658763, 3692: 0.27991282210658763, 5247: 0.2307883725424946, 7748: 0.11085979643357212, 6058: 0.11499601445982417, 2687: 0.20851312188043414, 6527: 0.11101026714750607, 5235: 0.2307883725424946, 6923: 0.12006690731862983, 10529: 0.24098261689276002}, {10160: 0.5083782610546482, 6355: 0.6626444322444514, 747: 0.549958089405439}, {404: 0.17282714662249507, 9304: 0.224853301820883, 213: 0.1653865642788935, 10854: 0.1697662171810413, 215: 0.18856645081547377, 7539: 0.2914134810447633, 189: 0.2914134810447633, 11298: 0.14113943946623936, 7358: 0.2914134810447633, 7356: 0.1032129464341637, 4325: 0.06743641904136043, 1384: 0.13108103989927514, 4624: 0.10787397563866821, 10664: 0.18657876021446151, 3025: 0.2914134810447633, 5133: 0.18815651688577695, 11655: 0.08854991690122223, 9515: 0.1284819740574263, 11640: 0.08837905683097076, 768: 0.2044215506876775, 8995: 0.16894684494213985, 7896: 0.11844818493506212, 5360: 0.23064479003325924, 9256: 0.14113943946623936, 3005: 0.22088786491545692, 7985: 0.16011917390395292, 7819: 0.15550216242427103, 7281: 0.20190813047774336, 3636: 0.15806305031475296}, {7271: 0.2106628162276238, 6226: 0.16366644670847935, 6254: 0.24555678730602193, 4991: 0.16155671083434386, 6076: 0.09416378212567846, 4120: 0.232379115432771, 9185: 0.22268583518650478, 11013: 0.31745016214169425, 11577: 0.26623228849785924, 9202: 0.31745016214169425, 7210: 0.31745016214169425, 8105: 0.23627175304954134, 10186: 0.29184122531977674, 7637: 0.2512520207737003, 10430: 0.11851522054747206, 1793: 0.3463761931499816, 7133: 0.13649513048522693}, {5924: 0.6540898614323675, 464: 0.5920735444678734, 2734: 0.47076041795443624}, {7562: 0.4506241081082754, 739: 0.464960987780364, 11282: 0.464960987780364, 1209: 0.2916450420781895, 10337: 0.464960987780364, 3724: 0.25162456587097576}, {6117: 0.30728995029258765, 10627: 0.45570195699366345, 6018: 0.21888489776798983, 1612: 0.5252588161086377, 2171: 0.18474063745989652, 274: 0.4423116232654185, 5097: 0.37991119392009853}, {8164: 0.4343027988760135, 2145: 0.4785692406094922, 4537: 0.23223602652275294, 527: 0.3801918359814777, 4309: 0.36414214868809186, 11294: 0.3955931626200522, 4624: 0.20312606904756306, 11359: 0.2313428234876558}, {3822: 0.18352200398716523, 8803: 0.1202247169010053, 4947: 0.19401068466450117, 11490: 0.3392331719677176, 11528: 0.21258869630625174, 9708: 0.3064910374519269, 5386: 0.2739809152320754, 10116: 0.3100846466792197, 9705: 0.44754436630915045, 5385: 0.29699451390234444, 9669: 0.44754436630915045}, {8430: 0.40408560345872324, 9375: 0.48484776172449867, 5322: 0.5484694490252594, 10269: 0.5484694490252594}, {1384: 0.16584455041797935, 5968: 0.09088185636069411, 1888: 0.24960588389073254, 4063: 0.24960588389073254, 4108: 0.15728358670799067, 6371: 0.08027531924383585, 9515: 0.16255619608100302, 6766: 0.21769125449254992, 11590: 0.18383579760970675, 6076: 0.07403944578073131, 9668: 0.2028517968988691, 3604: 0.13426911191885407, 8803: 0.06705211591661357, 6130: 0.10543550752663348, 4710: 0.13654870823406387, 10621: 0.24960588389073254, 5805: 0.18577662509436732, 1841: 0.13266962827783896, 1956: 0.15851994127594485, 2608: 0.2511993536588752, 7067: 0.0851982221845753, 8259: 0.16258001837582958, 10871: 0.2294699946292128, 11152: 0.19755536523103015, 5765: 0.20933410536769304, 10401: 0.18919821610617327, 5962: 0.14803252251186927, 6015: 0.19755536523103015, 9223: 0.09345432791162542, 8084: 0.24960588389073254, 1286: 0.24960588389073254}, {11547: 0.1571535070644677, 6133: 0.3779592342910518, 3915: 0.2340810348014949, 4152: 0.24441939242635394, 3792: 0.29352886844545056, 31: 0.43336995310076226, 8731: 0.43336995310076226, 8736: 0.3521947172805897, 11550: 0.36344941077310644}, {6188: 0.7309540996767293, 3121: 0.6824266291446883}, {2171: 0.3931505218829275, 8950: 0.6450305770407903, 4818: 0.6552619490139855}, {5030: 0.7390729267362314, 1638: 0.6736254218521903}, {5094: 0.5395275622124528, 10332: 0.26550878209840656, 5061: 0.2459397048224396, 10480: 0.1290273895375174, 2171: 0.1067669790811756, 2722: 0.15662367471038896, 5758: 0.1648746593103008, 8251: 0.2808784046589159, 5092: 0.3143319632235106, 6076: 0.0945469138692804, 576: 0.25386950949144804, 5812: 0.07583207629257187, 9570: 0.41469263891729463, 1956: 0.26336339409976883}, {917: 0.2037031421608578, 5385: 0.32489344557958133, 2479: 0.2233298114569126, 8142: 0.2338250185179487, 2171: 0.12604895391815898, 11642: 0.1927513093152711, 865: 0.2007616081240654, 6851: 0.24915985354890227, 6180: 0.20148287810884205, 11547: 0.17753905246112547, 1747: 0.36438867245774353, 895: 0.2655518537458653, 7133: 0.21050878385800711, 1696: 0.23010764176866919, 3822: 0.2007616081240654, 7151: 0.19369314179906383, 1136: 0.27143124694640924, 4872: 0.26229499202021994}, {2171: 1.0}, {7067: 0.32354503790662315, 6311: 0.7285707309400411, 4108: 0.4590926141103147, 11259: 0.23234684237328612, 9929: 0.3158357404918213}, {1962: 0.42768891128099196, 3209: 0.4455273555482718, 11547: 0.16993738127115787, 7151: 0.24121070442724551, 11259: 0.11486861542477146, 9912: 0.2002878767574477, 8793: 0.34973355912518966, 3886: 0.3330965348484295, 6720: 0.2035694606803302, 9576: 0.24179806153035402, 353: 0.38084449447028285}, {11259: 0.1608100456585084, 6834: 0.27765557910855615, 1047: 0.31247386337353245, 4238: 0.3803433937112313, 6784: 0.5721654027350147, 5882: 0.5721654027350147}, {10862: 0.6159003242185398, 9729: 0.23469487569389627, 11590: 0.26699306835690595, 7259: 0.3273611414787164, 11655: 0.18714962099028662, 7346: 0.17751017816660222, 325: 0.5662151944800613}, {3192: 0.27363891695869286, 9636: 0.38776609524232286, 7773: 0.29994817432603726, 3042: 0.27201125428155865, 3528: 0.35648472065505804, 3323: 0.32520334606779316, 9846: 0.27562354037596676, 1005: 0.29994817432603726, 3989: 0.35648472065505804, 4376: 0.17904345871455393, 3728: 0.2299705132288752}, {2960: 0.3327497458321107, 6460: 0.5441617343560223, 2582: 0.33312882752310097, 5933: 0.45636588393331345, 10910: 0.23589435729307662, 5123: 0.43068724382520923, 11315: 0.18106166896859283}, {6058: 0.30442379688531257, 11568: 0.3217228863567137, 8128: 0.45763216364463655, 5645: 0.5456716214622945, 3822: 0.2237604962286811, 10561: 0.31988197355971665, 2713: 0.31635289654412146, 747: 0.21015096664077818}, {11234: 0.5598475951924408, 4187: 0.6615422902022055, 6570: 0.38099561453049646, 6186: 0.3221409786720574}, {4325: 0.059812247194286565, 6252: 0.1628672799461234, 2650: 0.2502815393713156, 3329: 0.14201655049646578, 2171: 0.09829513722973064, 4087: 0.16414752609097705, 4194: 0.25846709258548584, 10638: 0.12085409552172119, 5929: 0.2376163631358282, 1444: 0.2376163631358282, 5030: 0.12344332682830242, 9084: 0.1089624867929705, 3095: 0.18131022517775863, 4973: 0.16414752609097705, 1136: 0.14329679664131947, 3126: 0.13480689292297937, 5615: 0.1654846904060308, 2995: 0.14904397583125922, 11545: 0.12518261144679327, 10365: 0.07017109537768655, 10784: 0.1439579494966513, 6052: 0.13960876627844343, 1638: 0.11251198643788253, 7346: 0.07449344942830957, 316: 0.18476779207663047, 11655: 0.07853871237559401, 11534: 0.15932421971431707, 3613: 0.1892024686277729, 5099: 0.14532551426297963, 5945: 0.2210381076749646, 2397: 0.15067038510539144, 5023: 0.20876261703137586, 9929: 0.11204560122004568, 10822: 0.1770055737870409, 11547: 0.09372825635449929, 1844: 0.15764036763115158, 8892: 0.12148081077286149, 6058: 0.09761944638839556, 10610: 0.1343177562274595, 6186: 0.07182627676685392, 3900: 0.09213498715640368, 630: 0.17506417478685535}, {999: 0.4155333825554228, 3365: 0.48266187532665616, 9773: 0.25284131225650064, 1638: 0.2285416949159906, 8646: 0.22666145464590015, 5527: 0.48266187532665616, 7408: 0.4403085119248786}, {7346: 0.1086979799269231, 11259: 0.09244563096105457, 11227: 0.15410516055999593, 6180: 0.1552094551737053, 5215: 0.19670503988481228, 7748: 0.13731912160229673, 11511: 0.20543327931582472, 2104: 0.2858715026826593, 10004: 0.2858715026826593, 11359: 0.1590032713521401, 8803: 0.10131326568310464, 7403: 0.2917326489393973, 6796: 0.3162960861671674, 1420: 0.195991310407668, 6308: 0.2414689035614981, 9708: 0.2582797340452527, 6248: 0.3771452531361837, 0: 0.3771452531361837}, {11026: 0.5729552849681868, 10181: 0.36320647113881804, 9084: 0.17558759040826943, 8335: 0.39667142901342445, 3900: 0.19316511714953066, 5615: 0.34694604726509726, 2534: 0.4403859149118072}, {10430: 0.2788607088072098, 3465: 0.7469452178113909, 8892: 0.3510679435240989, 5738: 0.49097947584598106}, {6180: 0.7366980771897756, 9897: 0.6762218149874251}, {9629: 0.27201271247818254, 8431: 0.36515613651545664, 1992: 0.2610851095273646, 2171: 0.10226290954390233, 4108: 0.250285737585807, 4348: 0.2662267646786376, 2476: 0.3971984239610135, 7544: 0.2823280250313639, 10160: 0.14140504916462496, 4624: 0.14703291335896057, 10533: 0.2302756217853071, 2068: 0.30107156162434295, 8300: 0.3971984239610135}, {2446: 0.3729056798246599, 7346: 0.12815237327594875, 10430: 0.1660016489253892, 5609: 0.24881631197247595, 10757: 0.40877555139074795, 3636: 0.24117625447209537, 4475: 0.21723581760510155, 1763: 0.23918565191088534, 6186: 0.1235639898794836, 3902: 0.19776590074099812, 4865: 0.2345203970250253, 10105: 0.444645422956836, 4271: 0.3439459624622046}, {4368: 0.5324344414308902, 9084: 0.20173845188065107, 10064: 0.35350978387770843, 11366: 0.5059740981568136, 4979: 0.5429882202296006}, {6117: 0.1354047484752023, 9084: 0.07874708328070754, 11528: 0.1154394135133405, 2874: 0.21195135681996652, 7940: 0.1683814350155919, 9313: 0.1508725403863375, 8493: 0.11249049683904841, 7442: 0.21195135681996652, 7133: 0.10449406935851811, 5149: 0.11703780392436891, 4704: 0.0920576505594001, 2055: 0.3893398098424775, 8803: 0.08493662803341265, 9063: 0.22341952516423472, 6231: 0.24302448843988053, 7318: 0.17789821013527776, 6495: 0.30105882138701073, 8185: 0.1262926884387601, 7738: 0.3589766372919204, 6916: 0.1441293269819744, 7063: 0.17789821013527776, 940: 0.18641803904621423, 1765: 0.22341952516423472, 7356: 0.08607451315432606, 3550: 0.24302448843988053, 11715: 0.1923463935443207}, {6756: 0.3193810960828954, 8138: 0.3193810960828954, 5020: 0.2595572997962935, 6923: 0.16385818298919808, 2766: 0.29361638940729784, 2084: 0.2678516827317003, 2235: 0.2785450021599932, 9827: 0.23379259312069595, 8542: 0.24208697605610274, 2793: 0.2595572997962935, 8056: 0.29361638940729784, 1691: 0.17323272100777373, 9166: 0.3193810960828954, 9375: 0.2595572997962935}, {9729: 0.17139191504555668, 9515: 0.19830319829171072, 2594: 0.5379668973018831, 6058: 0.22101184445733235, 4418: 0.23953171971978823, 1691: 0.24395957984852729, 9331: 0.27351074202974934, 3736: 0.31551077572172803, 4862: 0.19109920013697232, 5022: 0.2753476499801654, 10061: 0.2363320757857515, 2070: 0.36552846042336584}, {11640: 0.11575498340330043, 5767: 0.38168050069090176, 10365: 0.10362223890813561, 4376: 0.17623355369658175, 3257: 0.350890055468233, 9103: 0.38168050069090176, 306: 0.2305951545931893, 316: 0.20971701874178802, 9724: 0.38168050069090176, 5655: 0.38168050069090176, 268: 0.32009961024556427, 10610: 0.1983481453509184}, {11693: 0.4570527724370275, 1136: 0.2533947263116226, 7885: 0.3986139663465225, 10388: 0.34644042785994994, 6681: 0.28800162176944494, 1070: 0.34644042785994994, 8139: 0.34644042785994994, 6911: 0.35354340244076404}, {4821: 0.8898984787615669, 2171: 0.45615863194490697}, {4710: 0.47993541578661, 3474: 0.8773038223293763}, {10494: 0.2108888784190845, 5230: 0.28183860888562035, 1573: 0.3467978896953115, 11381: 0.2538621694115796, 10255: 0.2682580946362164, 9385: 0.258114674755212, 1395: 0.28183860888562035, 7430: 0.3467978896953115, 6570: 0.11397997204137553, 5749: 0.1545895469533011, 9897: 0.17044054913631113, 2582: 0.2123052156881348, 341: 0.22588572993753875, 10660: 0.20446879344063973, 8344: 0.2108888784190845, 699: 0.21687932807592922, 8945: 0.24028165516217562}, {11640: 0.1957303936662545, 4865: 0.3403966994004719, 7133: 0.2774981873541558, 5030: 0.3082342580086813, 8646: 0.2786277117594706, 1508: 0.3924603409595995, 8779: 0.49922340179971564, 3900: 0.23005827963705094, 8995: 0.3741614094438789}, {7346: 0.202508922908447, 4418: 0.28761392104988087, 9372: 0.5892729540413968, 11575: 0.4662766515870218, 6186: 0.1952582684277192, 2538: 0.5229589343261125}, {7354: 0.8145927229540876, 11359: 0.45531313930457495, 11315: 0.35934473822063673}, {7346: 0.17274195783788204, 4785: 0.22670151236990693, 1326: 0.32373707760007076, 3768: 0.4260214673458944, 651: 0.522722475424413, 10841: 0.5993562112176057}, {4930: 0.5638281601111896, 6226: 0.7217095304091168, 10365: 0.40153849078548215}, {3490: 0.6202541699095571, 11464: 0.3825920368263022, 11227: 0.38191427943448525, 9331: 0.5683745079003332}, {7922: 0.20508645593340652, 10114: 0.32220601782615715, 5812: 0.11693382747660626, 3822: 0.1775208813427975, 10786: 0.4060456528053432, 5385: 0.28728286917359036, 5771: 0.17978932989399585, 10480: 0.1346954673408137, 5147: 0.432909779869015, 11555: 0.2029421555847521, 895: 0.23481082643075746, 9912: 0.1850241502950962, 6133: 0.37755789884758606, 2722: 0.16350403691316392, 3900: 0.15431804725749076}, {6214: 0.35088536449728275, 4325: 0.10564219723642534, 9623: 0.21783387619057618, 7133: 0.15087137864190292, 3331: 0.2942730063839762, 6076: 0.10408151248278037, 8163: 0.2942730063839762, 11347: 0.16898236368522518, 7843: 0.32257918544062947, 4274: 0.35088536449728275, 1489: 0.35088536449728275, 11618: 0.176238362640335, 1136: 0.194534430738615, 6241: 0.2942730063839762, 2620: 0.2777149530971355}, {1455: 0.3120769925373523, 11275: 0.4762164188443923, 6925: 0.39288861444137, 9983: 0.48180005917258945, 5499: 0.53789693508654}, {10480: 0.3046142749451541, 7067: 0.33417224864877887, 8803: 0.20214594140249706, 865: 0.3085740538246019, 4325: 0.17413746566017327, 917: 0.3130952423659646, 2074: 0.3644568416106773, 622: 0.6310917891066452}, {6787: 0.40945355379778975, 6018: 0.1536896398184419, 2995: 0.29052919038997516, 9331: 0.30637818241633313, 4418: 0.20623348025024357, 6675: 0.37498755505415204, 5114: 0.4631820187313052, 11533: 0.29535137905086734, 4689: 0.31278129239552643, 11590: 0.21840881275250174}, {251: 0.2994068114334352, 8803: 0.1295564917580376, 4000: 0.5262280480771097, 2143: 0.3170122887197517, 340: 0.42061782589433455, 8195: 0.3232553405363162, 10610: 0.2506280536933919, 4475: 0.2356237792159955, 10600: 0.16742502903127782, 9996: 0.2972884614421085}, {6349: 0.29160632479109355, 9865: 0.4394248436632151, 6875: 0.3270550484062649, 4159: 0.25908077950127745, 11352: 0.2790703461268461, 10483: 0.10182672805098938, 7223: 0.30093055874477653, 4448: 0.12498173635293604, 9623: 0.2728002552250576, 10480: 0.1367225630621913, 1047: 0.20929693750113007, 4325: 0.10168794452547987, 1177: 0.30093055874477653, 9834: 0.3399076949415042}, {4471: 0.44590555601772475, 251: 0.2618132966458528, 7838: 0.21390879564099444, 11359: 0.1777988499372751, 2150: 0.28881065958479174, 1455: 0.18926512923402997, 3708: 0.22159351504149585, 6018: 0.12864577338691086, 3724: 0.2098162612345875, 8867: 0.31966401681290546, 3213: 0.2581759306884526, 10861: 0.2770672962004216, 7133: 0.18133145175814552, 6180: 0.17355657147556183, 6570: 0.13860651169706772, 11242: 0.3138829922914531}, {5888: 0.34994287731757645, 1109: 0.31356537906940785, 10042: 0.24322209985104856, 9571: 0.3007131966375614, 7521: 0.29286161769196156, 5175: 0.3401727599768643, 10179: 0.3227116250349594, 7350: 0.3401727599768643, 11044: 0.3227116250349594, 7042: 0.3227116250349594}, {11464: 0.2217821536603167, 10365: 0.1470960560380761, 6834: 0.2293078932992329, 4596: 0.366978020310274, 7704: 0.45439464252640327, 6058: 0.20463462169238852, 4325: 0.09637082627193702, 2199: 0.3002277830640733, 9260: 0.23765691560296903, 1047: 0.19835309777665786, 4583: 0.2388840082588809, 6835: 0.4164479424365761, 10211: 0.2604368498712607}, {6697: 0.3945477192528366, 924: 0.2879575573366063, 2074: 0.22132916461280952, 3745: 0.546585733048661, 1163: 0.19013803700556983, 2992: 0.38325256252477236, 7489: 0.24385220626501025, 11259: 0.11201537581251723, 9576: 0.2357920014348833, 6527: 0.16661394097609328, 4017: 0.2743457011499402}, {94: 0.4542176226634154, 1600: 0.41757555474411473, 8456: 0.3691373757068203, 7399: 0.31168656583992094, 9080: 0.24957304738524352, 2171: 0.11694310162780104, 1795: 0.3442914189055134, 8015: 0.3076493509862127, 11275: 0.31106107210585104}, {6570: 0.1671629567303058, 5126: 0.3375205939893568, 6128: 0.1790003867452031, 3291: 0.467583342349699, 8621: 0.32564207122186145, 1755: 0.467583342349699, 3214: 0.4435821970554365, 7640: 0.2932899041823601}, {4947: 0.23222916209400074, 4475: 0.2011673517243532, 10600: 0.1429416411818111, 5749: 0.1835454277428667, 1209: 0.23743736365228013, 2777: 0.35910880644034016, 11020: 0.44927512489181476, 161: 0.35910880644034016, 11642: 0.16210955415374034, 2089: 0.4117559580123358, 10586: 0.244198062055862, 8725: 0.22059784892879047, 4818: 0.17668782030215205}, {10776: 0.298716205231847, 7179: 0.5013220146445965, 11315: 0.2825174519582288, 11715: 0.4549509989840154, 3558: 0.3744061402150437, 9256: 0.2784000842678917, 2551: 0.3936518505938314}, {1638: 0.18212119978222777, 4562: 0.5004092906558184, 1891: 0.4125879083582092, 9040: 0.43081249180421466, 3248: 0.41837619702751666, 5591: 0.41837619702751666, 11259: 0.10255213658841765}, {8803: 0.36921284291500955, 8185: 0.7142451525409731, 11259: 0.3368967922617736, 3900: 0.4899349855461088}, {11533: 0.47273096070093357, 10170: 0.5351406976300359, 7621: 0.5522520448081681, 7489: 0.43031099397994443}, {6018: 0.38924508880061404, 2528: 0.6655274834415523, 6511: 0.6368370510802245}, {5771: 0.20649887733040576, 1696: 0.30404713247791454, 11315: 0.1654434960240127, 3343: 0.4571116238497719, 4730: 0.3534253133793988, 4246: 0.36397597593642395, 7114: 0.21125786710148559, 10803: 0.40408731621368005, 6579: 0.40408731621368005}, {7522: 0.2924407813414104, 7520: 0.23521705697024856, 5968: 0.12208832709349277, 4046: 0.16286905173938085, 6052: 0.18111699491337366, 4745: 0.22670821228708338, 6443: 0.3353140661648275, 320: 0.30826403513639, 1029: 0.3353140661648275, 7748: 0.12208832709349277, 5464: 0.211290688256098, 4694: 0.30826403513639, 2722: 0.12664348555097424, 5099: 0.18853343618161914, 486: 0.3353140661648275, 7190: 0.30826403513639}, {3900: 0.5554817420172409, 7489: 0.8315287332891699}, {1643: 0.7275054435661621, 3295: 0.6861019090350949}, {7067: 0.16595113389801805, 1696: 0.2972995445752693, 2081: 0.3166772463887654, 9987: 0.20576688568568505, 8259: 0.3166772463887654, 6087: 0.407745974722759, 6690: 0.3848030631050188, 10537: 0.37608057292678165, 6128: 0.17110808681360012, 2537: 0.3951195793227759}, {7312: 0.7689734971002629, 11590: 0.46239248486202245, 9084: 0.34562540103623085, 2171: 0.27461979691759486}, {5225: 0.5497571727022705, 8072: 0.36178606435380495, 2038: 0.3703671806561912, 9754: 0.65551967643586}, {4865: 0.7787543177810047, 4862: 0.6273290305234103}, {11418: 0.24063195293514872, 6180: 0.14014546435318845, 5464: 0.21458437919186885, 7259: 0.18100318296066517, 3613: 0.19160363758293456, 5099: 0.14716984069155112, 6205: 0.12825759927084904, 8239: 0.20246902822308296, 6834: 0.11077791405455507, 9115: 0.19840126072709172, 1518: 0.15916934896026577, 1209: 0.15093549326396224, 1566: 0.12677130425039226, 3454: 0.20246902822308296, 7069: 0.2617472990391772, 4704: 0.09914984923742043, 5030: 0.16264168997967438, 4325: 0.07880511012710677, 11634: 0.2617472990391772, 7748: 0.09530256283738736, 5598: 0.2617472990391772, 5915: 0.2617472990391772, 8667: 0.18135368211905445, 7688: 0.1233448868663972, 10750: 0.2617472990391772, 7083: 0.24063195293514872, 11642: 0.10305069573981866, 10520: 0.19160363758293456, 9246: 0.1534407128708688}, {9151: 0.34406128992059326, 8109: 0.4539140175350371, 9380: 0.33046534768239266, 3625: 0.4539140175350371, 4418: 0.1858027599949688, 11259: 0.11126314703460279, 1502: 0.39587653291142744, 1299: 0.39587653291142744}, {10861: 0.3256511939154504, 11259: 0.17947040146249088, 1136: 0.4059254806948462, 5946: 0.4907488460948464, 10260: 0.6150522688687032, 9729: 0.2790026086599006}, {3615: 0.20402128200610947, 4039: 0.2932530815961731, 9677: 0.33043915751549113, 8688: 0.3940091741806178, 7317: 0.33043915751549113, 4032: 0.252266103023735, 1881: 0.205500638487937, 6407: 0.3118461195558321, 5917: 0.26146807326360977, 10856: 0.33043915751549113, 5678: 0.34363112788839545}, {8803: 0.11138649727462781, 585: 0.25224442665513885, 10130: 0.24157377598966198, 6226: 0.16431290337699683, 2005: 0.18377936160795097, 3796: 0.15179792529944686, 2767: 0.27795451474119826, 3850: 0.27795451474119826, 6537: 0.23720498923455638, 1835: 0.26728386407572136, 2186: 0.3187040402478401, 11259: 0.07812055393922766, 10822: 0.2182575388921481, 586: 0.29299395216178076, 5762: 0.28819599019774184, 6193: 0.2332969763127306, 9281: 0.3477443245194879}, {2022: 0.6974576210640473, 11259: 0.2160038833331714, 2171: 0.22687907471801802, 9693: 0.4292696087047662, 10279: 0.36396960559401226, 3900: 0.3141254588631136}, {6128: 0.14183547203069077, 5914: 0.22770134533233155, 9047: 0.3189722070460201, 6934: 0.40301279382208366, 2743: 0.2390127970707423, 11567: 0.35148354396197834, 8631: 0.35148354396197834, 5853: 0.29995429410187313, 4847: 0.40301279382208366, 2613: 0.3275238072345715}, {6018: 0.25403644966016536, 5762: 0.3918582134673026, 2618: 0.5066158194213911, 8950: 0.27038139905362435, 6128: 0.22527383984737293, 5914: 0.3616525236389922, 1500: 0.5201981185721821}, {4010: 0.47945681100517945, 7748: 0.38471915727680095, 2969: 0.47469824609726885, 4127: 0.3010039768238666, 5499: 0.5533267727964785}, {9912: 0.33812717776725765, 11234: 0.49709375608476447, 2171: 0.20368529979750272, 8399: 0.6634897688430611, 7346: 0.22801419533628167, 4418: 0.32383786271654685}, {11259: 0.24310411810682575, 10229: 0.603338822510315, 4785: 0.2883345386852845, 9084: 0.24700838745778828, 2171: 0.19626275440759527, 5334: 0.43283707618537015, 2659: 0.4548249114215936}, {9450: 0.39359051651803323, 6117: 0.32781848652728895, 2694: 0.5921248432049027, 982: 0.6221010490728938}, {7217: 0.7828210193361995, 2038: 0.6222469378674623}, {1047: 0.2476273584863232, 4437: 0.23699291879605136, 5514: 0.2580136454179865, 3121: 0.34501080038752807, 8186: 0.39960696452490674, 10480: 0.12433363564946977, 8049: 0.2499048942793931, 7428: 0.34851318428642486, 5758: 0.15887685468949794, 9693: 0.1946611914666304, 6018: 0.15859312128588926, 4152: 0.17322979936837396, 10453: 0.20405695759208803, 4412: 0.08768714180785454, 10600: 0.1387241015591101, 7334: 0.24142556298876847, 9294: 0.2132356639117764, 11315: 0.1329632338333717, 4325: 0.09247363098966667, 11547: 0.14490999081303169}, {4785: 0.4092349735654678, 6570: 0.35559482725204244, 11259: 0.34503916111032523, 6186: 0.3006640005629906, 5594: 0.7047185192033781}, {8803: 0.23572252407768504, 4367: 0.37282508611351567, 1081: 0.6530999713169043, 7613: 0.6155459156494776}, {11264: 0.2607242167234546, 10634: 0.2192439459502878, 1881: 0.23196892732386346, 1274: 0.2637699359292983, 2125: 0.4447571850040957, 9729: 0.16947909936716568, 4785: 0.16822571384215473, 8501: 0.29234627167421545, 1691: 0.24123687435157368, 6205: 0.2179334381538285, 5099: 0.25006884236722726, 4264: 0.38789049376135903, 11454: 0.23196892732386346, 9704: 0.2185845447647279}, {4325: 0.131517254096546, 11019: 0.28015766430508915, 6747: 0.4307845207366697, 8576: 0.5224791422151585, 6069: 0.31653975751269475, 8623: 0.42299391639317746, 8072: 0.40808561143218147}, {5029: 0.18104974511207803, 1420: 0.1959137934177325, 8467: 0.34658353733390124, 890: 0.3769960875189388, 11019: 0.18584097710464426, 8725: 0.20197528255014688, 9515: 0.16621469042099393, 279: 0.28059058432976924, 4306: 0.3769960875189388, 9095: 0.2983807857393164, 8995: 0.21856332560013556, 685: 0.2526859187299366, 5766: 0.3769960875189388}, {1170: 0.2274035629643986, 9403: 0.3324727688132058, 6916: 0.23511115184730966, 5809: 0.24980406068961566, 6681: 0.24980406068961566, 11347: 0.1909180696707157, 1742: 0.2558103463941567, 3775: 0.21325155462527481, 9223: 0.14842788805856294, 2674: 0.2038349450118141, 10480: 0.12334640393229171, 6186: 0.14332974051719236, 7639: 0.2538186163547722, 3900: 0.14131564013882242, 11259: 0.09717368072035315, 3817: 0.34574592611380706, 9168: 0.258216064545095, 3189: 0.30665305638600276, 6076: 0.09038407946152299, 9386: 0.16943453280266943}, {4325: 0.09631006729663101, 940: 0.31924545426663037, 2179: 0.29582392701341154, 2954: 0.41618538424101453, 3381: 0.338229018164065, 4818: 0.1785885230128363, 10365: 0.11298995164235971, 4826: 0.16945933641350228, 4967: 0.2375070798982748, 5720: 0.23400408302870193, 4437: 0.24682499988156884, 2115: 0.3490374253971544, 11524: 0.38261140481908446}, {860: 0.3817469945127636, 4325: 0.11869276254852934, 10638: 0.23982557311685765, 10061: 0.2695037974279989, 11262: 0.3967489313740705, 3235: 0.512907885691148, 10430: 0.19148640776592663, 3902: 0.2281271430533035, 9041: 0.4301545769417572, 11259: 0.1257236906028275}, {7346: 0.17689289059358623, 4508: 0.5147338432891158, 6128: 0.3190654095252408, 7366: 0.37176264318390134, 6371: 0.2915687142328404, 3372: 0.37080685626747, 7613: 0.4305410753831908, 747: 0.2363728411666641}, {11590: 0.5988396628272384, 495: 0.8008689394807114}, {10430: 0.2943354534824482, 4418: 0.32271753799508773, 7985: 0.43318928267274176, 4549: 0.7883952542165295}, {1063: 0.7951312399145525, 11227: 0.6064373927388932}, {4421: 0.2455427184185356, 11135: 0.4302662920885154, 1276: 0.2883908796784211, 6774: 0.4166399244340358, 5334: 0.24430621619674173, 1351: 0.349672408096665, 1018: 0.27112271795986403, 5996: 0.4302662920885154, 4325: 0.09956855073632126, 8189: 0.21751951258721272}, {7133: 0.16353424060519287, 6593: 0.26046464325934104, 3495: 0.2549243329709147, 2157: 0.31897178287098105, 4745: 0.19764929267861128, 1049: 0.28307619161253744, 7688: 0.17922807317289743, 11315: 0.12655100960383509, 7498: 0.380335701437027, 9993: 0.20295225810615405, 10679: 0.2576078643049351, 1136: 0.21086199840403105, 4893: 0.380335701437027, 10483: 0.08813416127193913, 6318: 0.349653742154004}, {9468: 0.2592207978667663, 11482: 0.3080807340239705, 377: 0.2269788947102306, 1274: 0.19874480002742256, 8081: 0.2723438007120128, 5786: 0.21637300651616945, 4712: 0.2810467875860572, 5520: 0.21455864494010612, 1305: 0.2723438007120128, 6592: 0.1477494985029629, 1136: 0.18579100239534294, 5968: 0.12201573047624623, 10810: 0.3351146804618838, 11232: 0.20246206292350263, 7953: 0.3351146804618838, 10480: 0.10426751853755388, 8534: 0.2810467875860572}, {11227: 0.5932901100195447, 3658: 0.8049887237427593}, {10483: 0.12245919252615975, 6570: 0.17368659824993968, 7772: 0.4294753574344522, 3636: 0.2866387685206563, 10480: 0.16442573569013172, 9084: 0.17123740927715098, 395: 0.2707811965167913, 428: 0.528462541818935, 4234: 0.528462541818935}, {8646: 1.0}, {8131: 0.4904478722195633, 1592: 0.5050890136593809, 495: 0.40002471846744386, 1140: 0.5461103297349842, 10480: 0.21468513056959876}, {10365: 0.1698050706746797, 11248: 0.4283308362930053, 10784: 0.34835981477686834, 4911: 0.6254572867550464, 1708: 0.5245450929589784}, {4325: 0.2258149954806529, 6371: 0.31383049307634886, 5968: 0.35529597467855156, 7080: 0.8510480478849278}, {5078: 0.29429992367040597, 9353: 0.29429992367040597, 3230: 0.29429992367040597, 11348: 0.20918793027462357, 4325: 0.06810437494286446, 7133: 0.1265411433787652, 1593: 0.14734488386999792, 5867: 0.16970682910500837, 10279: 0.1215546153989761, 3874: 0.2085641269530475, 11640: 0.11612271538472323, 10160: 0.1047725586641397, 6018: 0.08977473624894178, 5812: 0.053816663645006806, 11257: 0.16468542762568833, 9912: 0.1257827747053998, 6358: 0.25667071057074164, 8493: 0.1362247271701088, 6834: 0.12455498784628388, 160: 0.3211165069740459, 51: 0.22764941518176246, 8546: 0.1735164834925286, 3203: 0.21543273272588656, 6186: 0.08178398092601746, 11480: 0.1854465401265645, 9839: 0.29429992367040597, 4818: 0.1262864835749527, 7151: 0.11643291637849887}, {1185: 0.33772252894705324, 11212: 0.36735752961451107, 3868: 0.36735752961451107, 3768: 0.2611171635128284, 2087: 0.23520227061971224, 7124: 0.36735752961451107, 7346: 0.10587703556651541, 4418: 0.1503721856354459, 7041: 0.2734168000809769, 8273: 0.36735752961451107, 4070: 0.33772252894705324}, {7334: 0.8190316415700684, 6834: 0.5737483508534374}, {10430: 0.3613612120792374, 11234: 0.4674604779809046, 6527: 0.35290222799746446, 1455: 0.43439221192634253, 4017: 0.5810870843708831}, {6226: 0.4060657992967131, 900: 0.8593791125637786, 11640: 0.31077018442993165}, {11245: 0.49532362953748815, 4325: 0.13142795338459218, 10279: 0.23457632993095243, 11486: 0.3476859175504441, 3117: 0.32247798884712164, 9401: 0.28618263467648836, 7067: 0.19385570973430824, 7114: 0.24130404144566092, 6920: 0.3348524426374427, 11586: 0.4094426145726081}, {9713: 0.4335291362028749, 10861: 0.37888414020834, 3469: 0.43499793918619145, 7322: 0.6922976883289028}, {11640: 0.2831078349859097, 4537: 0.3950782660606468, 1163: 0.38840183716693266, 8462: 0.7828838549500527}, {1707: 0.9368296183192283, 6186: 0.3497860292233654}, {5968: 0.17506423883263936, 10735: 0.5198915484269667, 3: 0.3292734800436739, 11259: 0.11785630310447172, 10453: 0.2455237041688466, 4862: 0.20428514237457168, 4325: 0.11126534809909532, 9392: 0.4808117463848151, 2382: 0.4808117463848151}, {7519: 0.41198303530603325, 2171: 0.10606936314847631, 1068: 0.2415114534519048, 6876: 0.20308786315445287, 8900: 0.3593068496120845, 11232: 0.24890265953906168, 6861: 0.3787480624470658, 6187: 0.41198303530603325, 7571: 0.3593068496120845, 11486: 0.2522107249480253, 5968: 0.15000360750356428}, {10564: 0.08166629061271224, 8335: 0.20295908852730227, 953: 0.2101595749841059, 1540: 0.27725983654397063, 1561: 0.1898753762602682, 2862: 0.2548930826906824, 5495: 0.27725983654397063, 9508: 0.2253258423805905, 8747: 0.17339184821721035, 11264: 0.2400830550616269, 11347: 0.13352515457948705, 2770: 0.2548930826906824, 8129: 0.19210166617020275, 4854: 0.27725983654397063, 2007: 0.2194426165703601, 574: 0.2548930826906824, 6834: 0.11734320260875276, 6234: 0.17751682720433945, 2413: 0.19449301221937748, 4081: 0.24180937042364833, 4448: 0.07885857227215641, 7346: 0.07990975332868924, 6570: 0.09112531926086101, 11019: 0.13667579224576695, 7133: 0.11921435891562084, 7356: 0.0982000028103095}, {10160: 0.24494419783622567, 3219: 0.559157038994725, 8871: 0.47670962951622986, 4762: 0.6325295841425274}, {6570: 0.17396828740503653, 7852: 0.30280771013857527, 1209: 0.23460639609145997, 7589: 0.374025976539768, 7640: 0.23460639609145997, 2767: 0.354827149306856, 10785: 0.14955912306373398, 1495: 0.4068465884751368, 2207: 0.2978188755956397, 11329: 0.2726935567622194, 4412: 0.08927575759365322, 9401: 0.20500812476522157, 10564: 0.11983579065528233, 1384: 0.18300414141994173, 4616: 0.2310084532080095}, {8527: 0.9009639715227079, 11347: 0.4338939064080402}, {7356: 0.10114858919718557, 1784: 0.2625465883141234, 2479: 0.13027268795106028, 11711: 0.23209155207958296, 2412: 0.24907002009213688, 8259: 0.1860148609912746, 9171: 0.2855849338582776, 10365: 0.07753330387918339, 10784: 0.15906172454730153, 4904: 0.20033292496643063, 2796: 0.2625465883141234, 3608: 0.2125551063259962, 2596: 0.2260316745479827, 10972: 0.2855849338582776, 7653: 0.1860148609912746, 3653: 0.2855849338582776, 11642: 0.1124356439777462, 395: 0.1463320935314441, 8986: 0.23950824276996924, 9172: 0.2625465883141234, 11585: 0.15239197106304433, 8241: 0.23209155207958296}, {3658: 0.27029833316825147, 307: 0.3568894287945228, 5765: 0.4088813626461551, 3877: 0.396219808451869, 11021: 0.4875421219608475, 4410: 0.4875421219608475}, {9777: 0.37932086649746394, 2701: 0.5092020647087505, 9773: 0.3098362818720299, 2038: 0.3634987394480272, 1691: 0.34896097832007883, 7067: 0.219599759263265, 3881: 0.4457591992688198}, {4634: 0.289726521826639, 11479: 0.33653119635475603, 7443: 0.33653119635475603, 7819: 0.1953354361449414, 6923: 0.14435301555127714, 11309: 0.2974940492912609, 2171: 0.09424642187293739, 7067: 0.12494818629948823, 4747: 0.4153629119792492, 4418: 0.19494859894248615, 11591: 0.33653119635475603, 10358: 0.31925698359265614, 10359: 0.27245230906453916, 11547: 0.13274541292930375}, {5914: 0.3840423527343832, 8936: 0.6248898134267695, 9593: 0.6797236147006347}, {6018: 0.40333289829096886, 11640: 0.521707033743469, 3902: 0.5880811299555649, 7356: 0.46830004130790304}, {6058: 0.20824937341090918, 6527: 0.20103147647540762, 5812: 0.13117967507434336, 10480: 0.1715569059001972, 9729: 0.21010955126252895, 5749: 0.2457855229829058, 4325: 0.1275961241467281, 4348: 0.36957009230434335, 6186: 0.15322538371733652, 6076: 0.12571110726153095, 11227: 0.22529998195909054, 4412: 0.12099167419344664, 10365: 0.1496944224188664, 7356: 0.19528871956688454, 10157: 0.24916375782515335, 4914: 0.4808822910171151, 5387: 0.3919215049504619}, {6117: 0.8155568375557968, 11655: 0.5786769778693356}, {10312: 0.43804956028478886, 4421: 0.3871714796931181, 2074: 0.3285885092917622, 3914: 0.5214663791319566, 5464: 0.3285904209429905, 9035: 0.4127245695466613}, {7276: 0.9238035602727229, 5022: 0.38286679410395646}, {3900: 0.13570772677066542, 11509: 0.3807020835663241, 5246: 0.3320254830040795, 4187: 0.21725760587177276, 383: 0.2706024518092131, 11536: 0.27868063706663926, 4704: 0.1442099090555396, 3902: 0.16932568420428248, 3077: 0.3807020835663241, 7923: 0.3807020835663241, 10555: 0.2578560211765912, 4366: 0.3499905679688909}, {11315: 1.0}, {11124: 0.4577156950176919, 11359: 0.2636159462161709, 10480: 0.19454937860694604, 3209: 0.34989624840756667, 8566: 0.46538299906476105, 1844: 0.29312248616553316, 9794: 0.5081574582298394}, {7356: 0.07972624735486204, 5514: 0.14534042021785906, 3894: 0.09925970671857699, 4325: 0.05209087436184278, 6069: 0.0848771976850646, 1638: 0.06633674728084124, 4732: 0.212924470661307, 9084: 0.049379316846774046, 10483: 0.056573962066742374, 6767: 0.09460646242169918, 6324: 0.12384676830933068, 1856: 0.09678091463689714, 3232: 0.11622681746859079, 8623: 0.1134218922242131, 7537: 0.15239146284847918, 3915: 0.08231293162632235, 3925: 0.16228072677715863, 11454: 0.07948176074985853, 3095: 0.10689999321707602, 10811: 0.07423470149649695, 3222: 0.10090144759320467, 6864: 0.15239146284847918, 4656: 0.07948176074985853, 11642: 0.0599969754377255, 6076: 0.04520317900821416, 9993: 0.08131813917111269, 2171: 0.057954533412706566, 3815: 0.12384676830933068, 6455: 0.12780440125772546, 4091: 0.2589085242964529, 5259: 0.12384676830933068, 5680: 0.11787915866061124, 10296: 0.15239146284847918, 10533: 0.08834888747104011, 4412: 0.04350616605175297, 10600: 0.052902903717075074, 3024: 0.12780440125772546, 6058: 0.05755614801395288, 5594: 0.09925970671857699, 4360: 0.10986283371026119, 5058: 0.07089326308599511, 4087: 0.09678091463689714, 11547: 0.07189740699593691, 11640: 0.04621685210771594, 7067: 0.05201592810400587, 4826: 0.06204964697758655, 1442: 0.12061314674096928, 5679: 0.15239146284847918, 3900: 0.05432252644559148, 10336: 0.10436198299719765, 616: 0.0848771976850646, 5812: 0.036255555881569985, 9814: 0.15239146284847918, 9002: 0.12384676830933068, 10365: 0.053827095477361385, 11232: 0.0920684522018208, 7748: 0.05548594777218611, 9339: 0.11787915866061124, 8778: 0.10112836142883626, 4448: 0.04334336100205627, 2173: 0.1400979320531023, 1384: 0.08918220107426439, 9937: 0.24414026564949007, 3018: 0.12061314674096928, 5649: 0.10436198299719765, 8462: 0.12780440125772546, 1163: 0.06340591127618458, 6355: 0.07071501217942851, 1174: 0.1134218922242131, 3126: 0.07948176074985853, 6742: 0.06539241237293159, 1593: 0.07629666401590778, 3117: 0.0865282322918738, 8913: 0.15239146284847918, 6923: 0.060094158233831255, 10785: 0.05601996966827456, 3942: 0.09393710691208007, 1742: 0.07558237355467816, 11655: 0.04630620149524142, 5056: 0.11551087046234862, 5936: 0.09678091463689714, 6451: 0.07948176074985853, 10140: 0.13290667753634614, 9912: 0.06513158685003106, 10288: 0.05195423824732887, 3167: 0.15239146284847918, 4159: 0.08984858173923271, 7896: 0.061941170701120096, 7658: 0.1400979320531023}, {4862: 0.46736238570211874, 6675: 0.8187066054984232, 11640: 0.3336043982684779}, {7988: 1.0}, {404: 0.6105508594743342, 11655: 0.3128225451092713, 5867: 0.5936475712737529, 11227: 0.42065694382392427}, {8592: 0.9256650121107959, 3900: 0.378344136143168}, {5771: 0.19037698058529698, 2093: 0.3997919833235488, 998: 0.45840349260938523, 11640: 0.13902331552951458, 10811: 0.22330283995399658, 9401: 0.23098741163813707, 6076: 0.10451267147223371, 3117: 0.2602825850641605, 10019: 0.2602825850641605, 4421: 0.2615999481658301, 3840: 0.372539183496322, 5812: 0.08382518849404776, 4412: 0.10058906783430102, 10600: 0.15913539629970985, 3469: 0.23408106053073596, 10480: 0.1426275763193995}, {6018: 0.2235543011191872, 7845: 0.41205544029175484, 7133: 0.3151088831095973, 1274: 0.4346316640497995, 6923: 0.28899522014222867, 6541: 0.6391535493938441}, {912: 0.3209543828594364, 573: 0.3209543828594364, 5099: 0.13870515820709442, 5561: 0.226791664275193, 11454: 0.16739795650768505, 7839: 0.226791664275193, 6094: 0.147188196385458, 11259: 0.06046913361609146, 6925: 0.13938073954376984, 5693: 0.24669253124762677, 7441: 0.24669253124762677, 4325: 0.07427252225282432, 11655: 0.07496085309373186, 8965: 0.21515040336447908, 987: 0.20689079730275925, 10940: 0.20048414911171683, 8950: 0.10420488423465858, 5030: 0.11781982168902246, 2672: 0.226791664275193, 9976: 0.20689079730275925, 4019: 0.14544761001670387, 6902: 0.19082373439935124, 7644: 0.1446153522634413, 2836: 0.24669253124762677, 3739: 0.1730505726773906, 11528: 0.11718177582911511, 11200: 0.1869899303303255}, {3818: 0.4452863220559566, 11219: 0.3353805515911979, 4803: 0.4424616719044791, 10289: 0.3238897445709115, 3896: 0.35019477838946494, 8142: 0.2113187416917957, 10506: 0.4424616719044791, 10480: 0.1376674412289876, 5812: 0.08091001409622575}, {5812: 0.07920255112672375, 6570: 0.1094151198167967, 9777: 0.19627990016511204, 10638: 0.15566155159220543, 10483: 0.07714404771539475, 4448: 0.09468630896139359, 6392: 0.35199507758542675, 2757: 0.2791967845673253, 6355: 0.15448140930077903, 11259: 0.08160241002154779, 7133: 0.14314192224989755, 4960: 0.3777449901169753, 10480: 0.10358117294468316, 4965: 0.2634870349943448, 11655: 0.10115882110293581, 8803: 0.08942993369126985, 917: 0.13851421685528034, 6876: 0.16410804709300184, 10061: 0.17492454504504595, 8415: 0.20521154156906993, 1360: 0.21314630308683843, 8957: 0.2575144653991704, 9655: 0.332908773125953, 815: 0.24777728542136432, 2171: 0.08571086313245896}, {2635: 0.5687994667235525, 8547: 0.5229139976175008, 2885: 0.41006182406207226, 10160: 0.20249606167810727, 11483: 0.3895299589980338, 10480: 0.1769761589047855, 5812: 0.10401256378306976}, {9912: 0.2122172120811671, 11305: 0.45647886066562143, 9270: 0.4965346762568487, 10674: 0.26495759770535443, 7125: 0.33280801845173125, 7905: 0.2693210528038342, 8170: 0.4965346762568487}, {4: 0.2490126588219022, 9966: 0.3728308988995483, 5805: 0.4181535766561237, 11574: 0.3340324396644379, 7978: 0.3340324396644379, 10786: 0.3113166974523833, 3900: 0.15393294492031437, 7896: 0.17552178519136422, 4325: 0.09993028313217082, 5859: 0.4318294488839041, 2622: 0.23043001600057583}, {404: 0.47006052223663164, 9651: 0.7925952361286437, 6205: 0.38837597389746537}, {2171: 0.2368849076715734, 9741: 0.6454220572484749, 526: 0.6231875766006126, 11640: 0.27904001552495555, 8803: 0.24716355443524543}, {5611: 0.4048833782530281, 4761: 0.43923948701812077, 927: 0.40069641874387907, 1306: 0.43923948701812077, 4325: 0.11056435856650876, 10600: 0.1658628642830316, 2472: 0.36215335046963737, 6742: 0.2050203685710138, 495: 0.27699423854944766}, {614: 0.4364614826942219, 9328: 0.3427215748690239, 4680: 0.24976347582249914, 7114: 0.2126284986803968, 264: 0.4364614826942219, 1797: 0.460077343589974, 2479: 0.2282852449148556, 8258: 0.36078620113768295}, {8199: 0.4239760643123358, 1742: 0.28253026503693157, 7062: 0.45085729268626795, 10325: 0.45085729268626795, 3228: 0.4106721457141543, 11585: 0.3039708113387573, 7922: 0.26986382506773937}, {404: 0.33705953333503647, 9329: 0.4956675155251396, 3588: 0.5683348585453165, 5149: 0.2737035438890168, 9084: 0.18415721281630681, 9735: 0.2827560488341678, 8493: 0.2630692528935352, 7133: 0.2443688803089044}, {3900: 0.2224810206455597, 6925: 0.3526309356918949, 2433: 0.5234302506768067, 6876: 0.30766513432842746, 10365: 0.1694441753483172, 5506: 0.5234302506768067, 6180: 0.25685213185358813, 1702: 0.29808217477713406}, {1578: 0.5526688826481224, 6117: 0.19788057524787092, 5220: 0.35742341999430777, 11259: 0.11326203529758429, 4241: 0.32843776646906425, 8803: 0.16149221619456808, 865: 0.18947794453503689, 6742: 0.19827745580798076, 2462: 0.30970685235949663, 3261: 0.40298861787681467, 4325: 0.10692800853109048, 7366: 0.18947794453503689}, {5158: 1.0}, {7346: 0.1326254641959222, 10160: 0.1259169457428763, 6720: 0.15364405657696992, 747: 0.1362155863251529, 9546: 0.3084700073072035, 3773: 0.2874423936668295, 6018: 0.1078923788582447, 11655: 0.10747446688447745, 3675: 0.2180235078682377, 3372: 0.2136864501461328, 9196: 0.2874423936668295, 8950: 0.1494028405183011, 70: 0.35369325702668636, 7322: 0.2874423936668295, 135: 0.35369325702668636, 2074: 0.17130322132237574, 2228: 0.23956248182183107, 4013: 0.32516056322547254}, {6371: 0.23383072638917468, 9084: 0.23559105409348471, 9675: 0.5167980063152844, 1669: 0.7687510217563241, 11259: 0.1782182197324017}, {11640: 0.11070501865870916, 7133: 0.15695284433536721, 5814: 0.3650291824018954, 11315: 0.12145799459078893, 10288: 0.12444800223876194, 7346: 0.10520597677292001, 4872: 0.1955640249209965, 7366: 0.1947452813016775, 10564: 0.10751856332219353, 11667: 0.3650291824018954, 6371: 0.11739640786455238, 6258: 0.28236052140853674, 9967: 0.33558201120746445, 2674: 0.18768748641581565, 5924: 0.23776062590172636, 11259: 0.08947574656111597, 6130: 0.15419122541768285, 4044: 0.24724049762417152, 624: 0.3650291824018954, 4818: 0.15663698200412038, 6058: 0.13786647400730934}, {9934: 0.5136729841964992, 11259: 0.2042621635844317, 3133: 0.8333168868050557}, {10430: 0.146987862331974, 3107: 0.23408413374567147, 7067: 0.10329309786015572, 415: 0.36327522171502863, 10417: 0.19218729437251966, 2528: 0.15783467853955982, 5294: 0.19891620978837793, 10663: 0.34337530767925656, 9328: 0.2072417606594481, 4680: 0.15103053403527228, 5055: 0.24593459782282623, 10616: 0.22938119305921367, 5786: 0.19539129658229337, 8995: 0.17544299625174295, 7653: 0.19710967338988486, 11561: 0.18786927268870818, 10723: 0.2181653360281787, 417: 0.19068835589583558, 1430: 0.2639257425452476, 8803: 0.12007964754787634, 4624: 0.11202182266331805, 9523: 0.15055777877364862, 7336: 0.1621276060020207, 11315: 0.100691801064976}, {11359: 0.250414807742087, 1605: 0.49813581150611813, 11259: 0.14559294728244482, 2171: 0.15292314496939224, 11547: 0.21539115885776458, 4745: 0.30866739971759266, 4097: 0.5460515943646551, 7896: 0.24142451299951873, 4862: 0.2523622002462018, 11298: 0.2876744836304825}, {1881: 0.13622801724534958, 7055: 0.2611918336120104, 6527: 0.09522939735886317, 4325: 0.07863787434541823, 8546: 0.15399626313215742, 9256: 0.1265022773036905, 8252: 0.21905076100871346, 10335: 0.1645842147988683, 9713: 0.13292572683050896, 6158: 0.15989828721188717, 6155: 0.27616646708843406, 324: 0.17887155349067207, 5817: 0.31240501039852975, 10323: 0.21226756339886876, 8747: 0.1633432931857271, 11642: 0.1028320073371529, 9912: 0.11163249093772508, 8950: 0.11032950469528895, 2341: 0.1769096884054165, 9195: 0.1882997541084427, 4204: 0.1769096884054165, 11123: 0.26895579977247436, 6003: 0.2849916106453582, 2568: 0.2849916106453582, 5508: 0.22779582370381374, 7067: 0.08915286581390923, 9352: 0.1856547511005168}, {4865: 0.5025892892338465, 5594: 0.8075066241264177, 9084: 0.3087669968428837}, {878: 0.43467914923001855, 853: 0.5734645679298784, 3604: 0.3084806217411967, 9929: 0.24859714886468737, 1643: 0.4076177273679177, 10279: 0.23685791056379385, 9380: 0.3209014002446015}, {5936: 0.2671706598295511, 10171: 0.2880987430537433, 7793: 0.33296124674789873, 4818: 0.1805204468987478, 4376: 0.19424430932556383, 2537: 0.341887891131241, 1403: 0.27917209867040105, 2825: 0.3867503948253965, 8091: 0.3528132208840422, 9084: 0.13631514760303218, 253: 0.4206875687667508}, {5771: 0.23606880864020546, 7780: 0.3260606939146944, 11585: 0.30331849024276464, 4785: 0.21500150437154866, 8415: 0.35038752407245255, 6592: 0.250613610685236, 3454: 0.43969191622532144, 9919: 0.35038752407245255, 1520: 0.3702409412058169, 6720: 0.24692273369432902}, {4152: 0.5160396655225661, 11655: 0.3617196072274156, 4437: 0.70598561559645, 10365: 0.323181527822219}, {10483: 0.28379355440183696, 10157: 0.553423500325396, 9929: 0.5309026571620848, 8892: 0.5756092566887678}, {11640: 0.20316187799962504, 4826: 0.20964951520577468, 8366: 0.24727275075358962, 9274: 0.5148908634278444, 6058: 0.19446715841261292, 6186: 0.143084728084141, 8382: 0.5148908634278444, 7743: 0.5148908634278444}, {1174: 0.45926982682224815, 6570: 0.20280738423838957, 8498: 0.5381679460520176, 7324: 0.4677285525131173, 10402: 0.432861244062591, 4624: 0.22842241016101583}, {3412: 0.3730198802886086, 9521: 0.277874411787364, 11124: 0.34500032896602445, 6226: 0.18676468480800082, 11342: 0.33977242270927926, 7151: 0.18645910463460852, 1283: 0.2612938527683451, 5615: 0.30175226064963095, 9734: 0.32275995988978895, 2763: 0.4647800692376792}, {1763: 0.4491047127205307, 3928: 0.5487833183388813, 10634: 0.5354491841617681, 9080: 0.4587330353328314}, {10042: 0.25482936430982284, 6018: 0.11826027170336481, 10662: 0.504385035989798, 3822: 0.15897430402175, 4448: 0.11026478401489588, 2659: 0.2313086460129951, 5771: 0.16100575534684528, 10480: 0.12062309522923628, 1384: 0.17438339294223484, 10786: 0.2794892078108956, 8307: 0.19796720090460598, 8142: 0.18515576722613358, 4713: 0.18996595779569983, 5812: 0.07089269799889966, 9145: 0.3876813276179577, 5353: 0.2686080678211575, 10533: 0.2247580891220784, 10453: 0.19796720090460598}, {4821: 0.34834602550608534, 347: 0.6375977141171887, 4865: 0.3657989538471113, 5620: 0.5816488002489203}, {3414: 0.24692234578626648, 10785: 0.10823244102111627, 8259: 0.1917730483755647, 2532: 0.20163081551948359, 4336: 0.21552455796457162, 2171: 0.0758029051689488, 3598: 0.23302860334117845, 2200: 0.27067385537527344, 5398: 0.27067385537527344, 4749: 0.22774644761328336, 648: 0.2944253649642804, 8733: 0.23927606755357858, 8279: 0.23302860334117845, 3053: 0.2944253649642804, 10919: 0.2944253649642804, 10721: 0.2944253649642804, 7427: 0.2944253649642804}, {11640: 0.3367007298247418, 4905: 0.5708369360380839, 5968: 0.40422829028620855, 8491: 0.6303786959617428}, {1219: 0.6599733330864085, 4325: 0.152725392410044, 1384: 0.2968633794723743, 5758: 0.26239404376841774, 1283: 0.36589595818159315, 3580: 0.5002517382653329}, {5079: 0.45089019796775437, 579: 0.5116161757345181, 4967: 0.25731228997715205, 895: 0.24456342854247914, 9526: 0.41451655583294844, 4865: 0.23781409361848985, 4624: 0.16690826401341555, 7346: 0.12995219555434248, 4946: 0.3124028325526392, 4152: 0.1954611041976777}, {9919: 0.192768167978576, 3796: 0.14894881215138875, 3488: 0.26226718173372177, 7253: 0.24751002128635774, 8072: 0.17259367626294442, 7911: 0.21936928362787325, 5812: 0.05718543092885474, 3764: 0.16891421880176566, 904: 0.2874947141467756, 7346: 0.11726243656168908, 10333: 0.22544932326991377, 1455: 0.14034529225265663, 9084: 0.10133120720329719, 306: 0.18893350501180653, 2023: 0.20555751752612825, 2532: 0.21416103742486037, 3087: 0.27273755369941155, 10480: 0.09730034087238747, 4251: 0.33065121837107475, 10873: 0.19414175121481944, 7114: 0.13286802816801616, 10057: 0.2327528608389938, 6919: 0.16687280699536253, 9993: 0.16687280699536253, 5594: 0.20369066545917053}, {4821: 0.4138424110683991, 10213: 0.6333043789268311, 2171: 0.1630508695631394, 5571: 0.6333043789268311}, {11655: 0.15322201658117643, 2148: 0.2837114648754918, 7346: 0.11170385785247486, 9515: 0.27375791335389066, 9987: 0.16403114611362898, 7285: 0.3875746336259668, 6349: 0.25719805360059067, 1557: 0.31497739411409464, 10388: 0.2937768459101583, 2171: 0.09978516355818572, 8373: 0.2754874497553745, 11259: 0.09500207480303172, 7338: 0.2502447478918837, 8409: 0.3875746336259668, 7100: 0.26853414404666753}, {9713: 0.39358580940633947, 10875: 0.6485973267057666, 6570: 0.254180413733428, 7905: 0.4194794619086047, 6826: 0.428766855919695}, {7461: 0.5184744969005797, 2171: 0.15305653024523228, 3157: 0.3184949984224519, 213: 0.25932507519447257, 10854: 0.2661923429385435, 215: 0.3388185547769138, 7463: 0.5944854577430947}, {9604: 0.8407919116308352, 9987: 0.5413584407175767}, {11315: 1.0}, {8940: 0.6693946547345303, 847: 0.6153940982176485, 11655: 0.20340459486759277, 3636: 0.3630805294676057}, {7195: 0.6103504804443131, 6769: 0.3167969997099496, 4130: 0.37980191033772964, 8099: 0.4147104079347793, 1706: 0.40388237795847487, 6117: 0.21853320851796898}, {7818: 0.3984534260654853, 2363: 0.3320821623455415, 5304: 0.4507384550545635, 6742: 0.2103876516470924, 2463: 0.4111863574848895, 6434: 0.29814721431332986, 8491: 0.2783881330029806, 6128: 0.172551822248965, 4904: 0.3439303998416241}, {5346: 0.535394404088392, 3863: 0.535394404088392, 10480: 0.16658251401056873, 8495: 0.535394404088392, 10561: 0.31385729416104047, 4412: 0.11748345049552929}, {11533: 0.28631292672263364, 7570: 0.3865596932185186, 7564: 0.4259599047314405, 10677: 0.3865596932185186, 9798: 0.34715948170559663, 8598: 0.3865596932185186, 2568: 0.4096073394740599}, {}, {3902: 0.24146867554702817, 9929: 0.23534922268535646, 8793: 0.31142220214093397, 11050: 0.4412119382804658, 213: 0.23682442506144283, 10854: 0.2430958461109228, 215: 0.30942055786551986, 5125: 0.5429042163848486, 3227: 0.2983025161528538}, {8803: 0.14094024669806463, 2479: 0.23932906308522842, 747: 0.20205862004222558, 4826: 0.277935067161683, 213: 0.29776108987568745, 10854: 0.30564619364519097, 215: 0.368209317780522, 748: 0.5246592799039748, 6993: 0.44000998401798647, 2171: 0.1350790467573843}, {6117: 0.1258110570159049, 561: 0.29378002500626693, 9615: 0.17748941892520942, 11324: 0.2463811259213217, 2535: 0.22268167637884906, 8840: 0.29378002500626693, 11041: 0.29378002500626693, 4704: 0.11128384245133005, 8118: 0.20118886846768205, 9379: 0.2700805754637943, 9718: 0.29378002500626693, 9719: 0.29378002500626693, 9897: 0.14438389123803338, 2459: 0.29378002500626693, 10763: 0.2700805754637943, 5061: 0.17423066110218566, 6592: 0.12952536518256427, 11359: 0.12385674921127192, 9242: 0.2060816406212205}, {8803: 0.4971907097827553, 4771: 0.8676412842331328}, {9450: 0.45726745941478225, 9181: 0.8893292250681691}, {6570: 0.15872364747079248, 6876: 0.23806424429753392, 7114: 0.20518766415744238, 9993: 0.33527690473686617, 1036: 0.38222892161149524, 10784: 0.39731602035367003, 8366: 0.2319266257737022, 9515: 0.21292276484287598, 10365: 0.17058068057851497, 642: 0.42118771794243026, 8596: 0.39247642545640254}, {6018: 0.5406472026505751, 7896: 0.7203917709293816, 11259: 0.43443791114880537}, {1844: 0.3549043509760294, 2722: 0.28593514213656607, 7151: 0.29951741435824675, 450: 0.6349238248266273, 11555: 0.3549043509760294, 11464: 0.3098953202823311, 10785: 0.2783035095694915}, {4421: 0.5954439701923201, 9897: 0.5127995609594819, 11227: 0.4263435551871578, 2171: 0.3495015481369943, 8803: 0.2802907944289094}, {10365: 0.12976282879630094, 11642: 0.18817677681865597, 10480: 0.14871435454594928, 6370: 0.43940843851496825, 10279: 0.1974143008959649, 5029: 0.22954000182815237, 417: 0.3011798355208289, 5812: 0.08740251445953569, 8142: 0.22827569099570375, 5030: 0.22827569099570375, 2171: 0.12305745654306292, 10170: 0.31718280729639275, 10586: 0.2834651200104728, 11259: 0.1171588367919838, 8725: 0.25606999168710504, 10564: 0.1407839587431584, 4182: 0.3782956229056805}, {11655: 0.36006540398874637, 10365: 0.3217035655571946, 6825: 0.6226276294293847, 10610: 0.6157877522566956}, {10160: 0.16649350781645814, 8259: 0.30461567848254223, 8022: 0.3922157911725013, 6951: 0.3800703085228481, 9777: 0.2757341079476333, 10345: 0.4676704212128072, 1638: 0.20357921607173665, 2782: 0.30196007813512665, 10461: 0.3922157911725013}, {4325: 0.12356080446182978, 60: 0.32481985336754915, 9060: 0.32481985336754915, 3052: 0.33352822813482913, 108: 0.3579272133711778, 1518: 0.24956621177833002, 6595: 0.377293777471176, 7106: 0.32481985336754915, 9562: 0.3579272133711778, 6018: 0.12519083733887892, 11019: 0.20230806344732596, 2074: 0.19876838330140065}, {9222: 0.7262867070199752, 10785: 0.26698712996924334, 9221: 0.6334236273118228}, {213: 0.22805924090144086, 215: 0.2979684951322179, 10253: 0.41094330645886074, 11000: 0.5228106156651587, 4818: 0.22434227438655693, 2820: 0.38911739670487666, 3900: 0.18636472781775637, 11280: 0.42488210282698013}, {5097: 0.4656468555189294, 1702: 0.5464822537715106, 7809: 0.6960820011019847}, {11640: 0.10860012530608759, 10100: 0.2665181886316023, 11359: 0.15096908271495077, 2739: 0.18815515237133584, 6592: 0.15787856289184082, 10638: 0.16743518155583992, 4248: 0.2834161621198768, 139: 0.3580886885664772, 205: 0.3580886885664772, 1084: 0.2511930633340599, 7172: 0.3123034385990397, 5319: 0.3292014120873143, 6117: 0.1533511899354853, 3469: 0.18285589297446217, 4238: 0.2076017688882481, 1455: 0.20908216281018835, 1745: 0.23763091215243942}, {6186: 0.11710655627331322, 4067: 0.3084783248800238, 9838: 0.30380384885983025, 3900: 0.15021815892246723, 8712: 0.3874130089096741, 9782: 0.35341769087917346, 5351: 0.35341769087917346, 10336: 0.2885923386320911, 11040: 0.3874130089096741, 4789: 0.23581321279882894, 1908: 0.2956107018692722}, {5345: 0.43301494798296514, 8926: 0.5328179512408853, 2171: 0.13717958245590156, 11655: 0.16190392131725886, 2949: 0.4468523233838679, 4544: 0.5328179512408853}, {4448: 0.13252210955577154, 10564: 0.13724049015547035, 4412: 0.10224198183327836, 10207: 0.4063615020609425, 6592: 0.20542767715830296, 582: 0.39076131784940654, 4502: 0.465936135732814, 5133: 0.30084030456105865, 4325: 0.1078229613233429, 7133: 0.20034015171252553, 7845: 0.2619769033718726, 4081: 0.4063615020609425}, {1300: 0.4075901760857503, 9205: 0.32223839949379773, 6173: 0.5700595267595214, 4325: 0.14349407961651178, 9987: 0.2624339971939046, 6592: 0.2733894070409556, 10144: 0.49077579050968667}, {11640: 0.41877407412535916, 1638: 0.6010818274311744, 6876: 0.6806826805292229}, {1442: 1.0}, {7879: 0.7424065199373063, 2171: 0.19114036263296946, 6834: 0.314204753825737, 7354: 0.5599761544926919}, {7137: 0.6789462409158896, 1974: 0.7341879881516604}, {8934: 0.7138912638964603, 5097: 0.47756043352587296, 3117: 0.5121477283586199}, {4870: 0.31049166260061517, 11642: 0.19967994331522543, 4826: 0.2065115766389697, 5454: 0.46626928985746063, 5888: 0.32472691720222413, 1248: 0.2811881081211722, 10714: 0.4014206737688967, 3157: 0.2717234415917688, 3558: 0.3303528630643042, 3915: 0.2739511683937175}, {6018: 0.13496243954222747, 398: 0.2235302010577076, 3014: 0.31263148159049153, 7845: 0.19120455442272916, 9155: 0.3400647540893335, 8803: 0.09135225882870016, 4173: 0.28098168693768294, 7505: 0.3400647540893335, 7067: 0.11607463744215732, 8493: 0.1574082241280851, 9987: 0.14392379306215136, 3874: 0.1852353092909677, 7748: 0.12381805930508775, 2960: 0.15983211490794566, 7114: 0.14448519036411628, 6058: 0.12843775468022878, 10782: 0.18768420450223136, 2722: 0.16710137141470954, 6186: 0.09450172128892588, 5924: 0.2215001229418877, 4475: 0.16614192135841577, 341: 0.2215001229418877, 6210: 0.14711494428853386, 11140: 0.29658404590660403}, {11547: 0.18341314212268275, 10274: 0.36463246169030605, 9070: 0.4649821246825592, 9629: 0.3463752525035755, 617: 0.2886389303802102, 2127: 0.3912384775888615, 6851: 0.25740360217500924, 1343: 0.44111450040480094}, {8374: 0.7944815922515521, 11618: 0.43405818256699036, 9704: 0.42472637511706274}, {7356: 0.4185376360480118, 5577: 0.6097322315855114, 8803: 0.3174443839486778, 11618: 0.5935334161414493}, {7067: 0.26408526447616215, 4826: 0.40985896739597183, 7760: 0.5758438135627791, 1681: 0.6123538676980372, 6018: 0.23601110784959528}, {4991: 0.3926689727266986, 940: 0.454912347592112, 6050: 0.6470864546030914, 1508: 0.4691960721651386}, {2739: 0.22873921668322642, 11618: 0.2186504180144152, 3822: 0.17851189695619424, 7101: 0.435326511623596, 7819: 0.2322960958271021, 4626: 0.435326511623596, 2688: 0.28614733370283457, 11219: 0.32997218710983145, 4325: 0.1007395435256238, 7970: 0.21144718727753975, 4689: 0.2702559611494545, 11259: 0.10670698810727557, 5572: 0.32400474252817973}, {2515: 0.36893708508143647, 392: 0.4399133483770801, 10079: 0.4399133483770801, 11526: 0.36893708508143647, 2600: 0.38366599049462685, 10225: 0.4399133483770801}, {10483: 0.048225469171705004, 2924: 0.2094416151254922, 4412: 0.06745564391995622, 6186: 0.07524275449098446, 6076: 0.061731612288982475, 10279: 0.0859568681281024, 6180: 0.08564633984667396, 8558: 0.1321686224380898, 7975: 0.20811303335554607, 10274: 0.15003391363997642, 9249: 0.1913244027022691, 819: 0.17453577204899215, 7133: 0.11642016048191581, 4828: 0.18150368333164435, 5601: 0.09657178567193025, 5289: 0.18150368333164435, 4537: 0.11459286115728902, 4418: 0.08518788690063031, 3809: 0.13324528298669944, 7040: 0.1647150526783674, 3610: 0.13437203843256013, 10480: 0.08424461951874983, 1202: 0.1772656535508896, 6128: 0.07324286170119422, 6234: 0.13324528298669944, 9898: 0.16098138845646182, 5215: 0.1085440746936036, 2103: 0.20811303335554607, 3952: 0.15774714139571516, 10088: 0.20811303335554607, 10579: 0.15489433330774266, 10288: 0.07095118004135785, 9011: 0.18150368333164435, 11528: 0.09885607277387472, 4131: 0.1913244027022691, 5812: 0.049512312366258894, 1702: 0.09939433657397322, 9223: 0.07791909132401005, 11590: 0.09021709383115105, 10365: 0.056500498252409805, 1095: 0.08429622028930879, 10627: 0.17195534227754236, 4624: 0.077038235190077, 11640: 0.06311593251024046, 1012: 0.15774714139571516, 1742: 0.10321888598389459, 3469: 0.10627170242711469, 10657: 0.1381057026544657, 11655: 0.08227447278795802, 4599: 0.09297692340115647, 224: 0.20811303335554607, 5524: 0.16913104016701513, 6018: 0.093773346526205, 10782: 0.1148590926939204, 4127: 0.08757219151917944, 4325: 0.048159740843209085, 867: 0.14792642202509043}, {10157: 0.36483444464903864, 10480: 0.19307758470431005, 2528: 0.3236555915860663, 7845: 0.3489095718436833, 9826: 0.4249696915038853, 4412: 0.13616927922558567, 10600: 0.21542452554690186, 1257: 0.37735795007592476, 8215: 0.4410854647468115, 4325: 0.14360221372967322}, {9304: 0.16750140362173055, 2722: 0.10667120777966321, 3804: 0.18396215113212505, 8646: 0.12193317770426894, 8501: 0.18564818873875183, 9897: 0.13880739641201123, 4596: 0.1912969972106277, 6153: 0.20361336827351384, 10266: 0.28243344904555095, 1098: 0.23686522312808933, 11547: 0.13325046072616553, 4957: 0.21408111016935852, 4656: 0.14730685961795575, 11642: 0.11119489496617248, 6018: 0.0861549268505398, 10365: 0.07667770892060262, 11259: 0.11091049649779049, 6186: 0.10211311768384271, 6117: 0.12095189507969603, 6527: 0.1339722591483864, 10483: 0.06544753766927668, 4448: 0.08033005728942215, 5812: 0.05164672059652327, 4537: 0.11953280304839607, 9912: 0.120711084282683, 8803: 0.07587064883623275, 5746: 0.18082925531478303, 940: 0.16652012099715763, 1638: 0.12294465833443532, 9420: 0.169556480689969, 6056: 0.20075325847630607, 9496: 0.1766273050536224, 7133: 0.12143887475369435, 8375: 0.19341841239780339, 10674: 0.15071029627754953, 3874: 0.15384319209489158, 6058: 0.10667120777966321, 2857: 0.20361336827351384, 9223: 0.1057452162140087, 11315: 0.09397550108381215, 2908: 0.18396215113212505, 10910: 0.12243502755575522, 8366: 0.13563669664941358, 6592: 0.12452274665922494}, {4862: 0.41952186584113593, 9102: 0.9077452308225981}, {10214: 0.4823556831751719, 9826: 0.37875896485609256, 7545: 0.5084547667548354, 6592: 0.24384496822713622, 8658: 0.5530714448281953}, {2171: 0.2076108338170796, 7991: 0.6382243864541116, 9968: 0.7413281150860594}, {3563: 0.43548280914509385, 7922: 0.28183119224675135, 5952: 0.36176513185982406, 1487: 0.546915915356306, 5746: 0.38089216473334103, 2518: 0.3947860582151297}, {4568: 0.3573815361673203, 6923: 0.14093009006444462, 1896: 0.3116867027653714, 9218: 0.3116867027653714, 11279: 0.2904398151998481, 9795: 0.270890846233979, 3506: 0.2659918693634225, 4291: 0.2997210762117595, 9216: 0.2904398151998481, 6983: 0.3573815361673203, 8350: 0.3573815361673203}, {2973: 0.5669438290767366, 8466: 0.5669438290767366, 4683: 0.5976197698734673}, {10562: 0.747938018157634, 8797: 0.663768574877141}, {8544: 0.8405072005140743, 3444: 0.5418003745698167}, {4914: 0.6337170799039403, 4325: 0.1681489310623154, 10600: 0.2522482262251713, 6834: 0.3075248555236418, 1745: 0.48219375495731964, 8012: 0.42357653134020773}, {4325: 0.35146169969539603, 2488: 0.9362022610778202}, {6720: 0.24914225470909454, 6643: 0.4198362619920783, 8974: 0.5735329923867712, 1384: 0.2579815483795443, 2528: 0.29913363406951976, 5746: 0.3672070155367197, 758: 0.31797255021914417, 6128: 0.20184799080166257}, {4862: 0.37622168053447486, 6117: 0.37920901572615395, 4756: 0.8140539601017044, 2171: 0.22797789263555834}, {5030: 0.3739522595112759, 11655: 0.23792075040889016, 6130: 0.2542134745405105, 796: 0.6018198280459721, 6570: 0.19779649534456986, 1844: 0.2821248649412553, 1745: 0.39937311413688903, 3126: 0.3138870032032659}, {4826: 0.28069629858392886, 2908: 0.4490248314304688, 4970: 0.6012348862357897, 5823: 0.5602502375004275, 6018: 0.21029163481668142}, {26: 0.5, 9597: 0.5, 29: 0.5, 4476: 0.5}, {11640: 0.08010235314564053, 11608: 0.2641225920151252, 4785: 0.12997573056273362, 8315: 0.2641225920151252, 10480: 0.10691741363745416, 6128: 0.12093691481252088, 2610: 0.18773792963769217, 8185: 0.13725675316566197, 1455: 0.11853445917158342, 8161: 0.242815629288997, 11259: 0.06474158024492144, 5470: 0.22150866656286877, 4789: 0.14779868601097454, 314: 0.22150866656286877, 7435: 0.23035185508994857, 11611: 0.34363141474419784, 2649: 0.23035185508994857, 10483: 0.06120441239744582, 5746: 0.16910564873710274, 8449: 0.242815629288997, 8155: 0.19041261146323096, 778: 0.1933423939184821, 10601: 0.2641225920151252, 4159: 0.1557242108860419, 10181: 0.17703117361217013, 11655: 0.08025721216062129, 8366: 0.1268430351025262, 8803: 0.07095176756228762, 4325: 0.061120994572952794, 10600: 0.09169051726193805, 4448: 0.07512206156779894}, {11640: 0.09755004859838337, 5812: 0.054231517835878144, 1145: 0.2612139105954998, 2171: 0.051691557728574006, 4624: 0.07432176888759685, 8025: 0.1751036322618095, 4376: 0.09270381716333452, 3973: 0.20077470270944, 6186: 0.08241442556779428, 9084: 0.08464099113547435, 5786: 0.1296339091971615, 2448: 0.15530497964479198, 8493: 0.09293403395470694, 4348: 0.13457152976240402, 7133: 0.08632778468144398, 10564: 0.08735363984430713, 1703: 0.1589069901103705, 1841: 0.1067150531904749, 4537: 0.08497280714154071, 6094: 0.11979149195224488, 6076: 0.045775175980361814, 2719: 0.16838141840656193, 7845: 0.11288743425856405, 1617: 0.16316726425517683, 6371: 0.06457080700526281, 9162: 0.18323707082350846, 11359: 0.11012697617692785, 11642: 0.07904560191112521, 11234: 0.09696409130814086, 10300: 0.14943256181417902, 4152: 0.08703592418603463, 8312: 0.15218477625512292, 11227: 0.082038465392673, 10561: 0.11769754119052882, 6920: 0.17485406251520763, 4448: 0.07429477094105202, 9107: 0.14697062210373782, 6570: 0.06598741134654268, 5005: 0.1751036322618095, 10811: 0.09780370793147707, 1651: 0.14697062210373782, 10365: 0.05450821871488764, 458: 0.16316726425517683, 6896: 0.0992738036703502, 3321: 0.13197254088894206, 3912: 0.12129955165610731, 5599: 0.18457806055800097, 2526: 0.2401415933274421, 11511: 0.10936318364947464, 2722: 0.07582982859145931, 8123: 0.15530497964479198, 6696: 0.20077470270944, 7849: 0.16838141840656193, 11315: 0.06680477597761454, 1326: 0.1084467204888588, 10660: 0.11837488761096499, 5378: 0.18457806055800097, 9223: 0.07517156491223147}, {5968: 0.15163178462864654, 6186: 0.11573000521847128, 3724: 0.15925320318129124, 5502: 0.27916870350167494, 6052: 0.22494446290726605, 6592: 0.14112798088595355, 6896: 0.1582727822624157, 10365: 0.11306309612296524, 6205: 0.1568488951330097, 6609: 0.21921095334148308, 1057: 0.23088679755628255, 6697: 0.21241877436606907, 5812: 0.08646161320362326, 11315: 0.10650722921330313, 7565: 0.27916870350167494, 3117: 0.18175139784536568, 10270: 0.25334630603285907, 1702: 0.15287727261195205, 10564: 0.09428366001780149, 10483: 0.07417503302026741, 11467: 0.24760360053505695, 9910: 0.32009623516846497, 1095: 0.1686850904357516, 10480: 0.1295756690792233, 2743: 0.18983788522903366, 10279: 0.13220925874292222, 7133: 0.13763287154236112, 8596: 0.26013848500827313}, {8803: 0.1510025320023909, 11227: 0.29882876979084044, 6742: 0.31381998092046925, 4325: 0.1300802679921663, 10217: 0.490244493761116, 1138: 0.5621167946000142, 9006: 0.411479403941056, 747: 0.21648438933592476}, {3604: 0.3156918078266047, 9450: 0.30175171478627727, 6018: 0.1790218343279848, 6371: 0.18874229741878415, 9773: 0.2826299234583422, 1665: 0.4644896320854533, 5968: 0.21368025097599974, 9256: 0.28423710026559507, 1527: 0.3974971669524132, 6761: 0.4019052681726519}, {11298: 0.3753269240865737, 8603: 0.9268924964933697}, {3840: 1.0}, {4309: 0.33702810080212053, 10483: 0.11768766946500447, 747: 0.15033721301254183, 6451: 0.2035979086111231, 9241: 0.5078714277242776, 4721: 0.340449448169795, 9990: 0.2704647306494441, 1455: 0.17518847937349025, 8242: 0.3089587371466422, 10564: 0.11498000894035207, 9300: 0.3717708390253345, 5812: 0.07138271990419531, 7133: 0.16784487150200916, 5867: 0.22510007546623378}, {4862: 0.23756538040409955, 10413: 0.41615746607932513, 4905: 0.2874942562872698, 9729: 0.21306622668645717, 5777: 0.33567554622585627, 5985: 0.3610198374068773, 9968: 0.5140348063574816, 1271: 0.3675327354468268}, {233: 0.5737411865534601, 7356: 0.23299918457762972, 11618: 0.33041903539749684, 10432: 0.5517153101980841, 8478: 0.4505173785531554}, {8524: 0.3589013101955459, 6585: 0.32327531551177807, 10733: 0.38515644253543097, 5176: 0.37037031879255855, 9385: 0.3286905769107677, 1263: 0.3097904624339963, 2136: 0.38515644253543097, 804: 0.3589013101955459}, {11655: 0.1513666566008828, 9515: 0.21962625745033926, 6021: 0.3216335359234481, 11680: 0.2679618681275586, 6371: 0.16020605336329682, 6285: 0.43444806028142313, 2731: 0.3853257998545902, 9226: 0.49814032421256527, 8542: 0.37758429105311403}, {10365: 0.07382012975449666, 1276: 0.1822493565655359, 4348: 0.1822493565655359, 4325: 0.06292260117966299, 3846: 0.24997287934678863, 5968: 0.09900204752945001, 4246: 0.19904136751461599, 11642: 0.10705094987827618, 6180: 0.11190032151989972, 11227: 0.11110416562728047, 7644: 0.15939702245483592, 10480: 0.08460136890335071, 5749: 0.12120638100734071, 6186: 0.09830762686014556, 8959: 0.27190788756014306, 2171: 0.07000554391063178, 10684: 0.19327170566225693, 1170: 0.15597254647851802, 434: 0.21520671387561133, 2363: 0.1841678547067254, 4624: 0.10065349322023652, 2475: 0.15517135108790717, 2722: 0.10269584877030422, 10681: 0.27190788756014306, 7999: 0.27190788756014306, 1555: 0.17872956266302384, 4680: 0.13570347698463087, 9308: 0.23714172208896572, 7756: 0.20610286292007982, 9257: 0.21520671387561133, 3379: 0.24997287934678863, 11547: 0.0986023092344661, 6896: 0.13444587331866154, 1420: 0.14130254258788244}, {10365: 0.2488403898143038, 9210: 0.5468709537134816, 2203: 0.799381398571153}, {7125: 0.2719310136725031, 8683: 0.3075224049666727, 7012: 0.2969860736465619, 1921: 0.26425723920358685, 2598: 0.40570890829559775, 9740: 0.3297149080895369, 3236: 0.26195347854447704, 1049: 0.30196095772674497, 1163: 0.16880435795098153, 10767: 0.21735814799934358, 4615: 0.40570890829559775}, {4947: 0.3130153775046764, 11243: 0.6638152216451574, 4325: 0.16709404859325364, 7688: 0.34026326269346624, 2171: 0.18590314984913517, 6018: 0.2202622688162399, 7896: 0.29349106982064166, 10674: 0.3853034644164728}, {7688: 0.369549742201257, 810: 0.5311607615792914, 6923: 0.23769402798804384, 11309: 0.4898585499476104, 2713: 0.34945165301328, 11464: 0.24673185187672145, 5097: 0.31913683425030237}, {5801: 0.6740210640177121, 11640: 0.25827310434154094, 3416: 0.692091474325623}, {4785: 0.21538928301291882, 1962: 0.3070325458649387, 5165: 0.3557061831981947, 3531: 0.38172757863109413, 6018: 0.13351540039122833, 4512: 0.40238192215157337, 4325: 0.10128665667896151, 4787: 0.338566328198823, 413: 0.40238192215157337, 10811: 0.21321301064734216, 9084: 0.14182468032294218, 10767: 0.23449239119900245}, {7905: 0.23755371850068507, 11547: 0.15882043938428064, 3808: 0.3466371641955879, 8787: 0.3034487003445655, 10494: 0.2663289298144805, 6742: 0.18793507173428148, 1420: 0.22759844139714208, 2119: 0.33197343451144457, 5029: 0.21033046772259129, 6579: 0.35593045800994905, 9306: 0.33877979623258175, 6158: 0.2681176044565492, 4680: 0.2185799299022917, 6180: 0.18023977702969035}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ14GUcpiaFJ",
        "outputId": "f719a917-7af3-499d-87ce-ccf9c4e096f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "know bother bottle nasal spray find house police look arrive mr. marsh dead prior death think get p.m. a.m read time purchase dristan item pick mark yes little confused charge receipt rosen drug store account date day murder signature \n"
          ]
        }
      ],
      "source": [
        "# print(vocabularies)\n",
        "voca_search = {}\n",
        "string = \"\"\n",
        "for key,val in vocabularies.items():\n",
        "    voca_search[val]=key\n",
        "for i in result[0][0].keys():\n",
        "  string += voca_search[i] + \" \"\n",
        "print(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b41a166-897d-4433-c246-713bda433fe3",
        "id": "56QHIu2a9YXf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop key oh matter hello mrs. dreyfuss \n"
          ]
        }
      ],
      "source": [
        "# print(vocabularies)\n",
        "voca_search = {}\n",
        "string = \"\"\n",
        "for key,val in vocabularies.items():\n",
        "    voca_search[val]=key\n",
        "for i in result[0][0].keys():\n",
        "  string += voca_search[i] + \" \"\n",
        "print(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnhZna4llfOE"
      },
      "outputs": [],
      "source": [
        "def back_to_sentence(cluster_index, index):\n",
        "  string = \"\"\n",
        "  for i in result[cluster_index][0].keys():\n",
        "    string += voca_search[i] + \" \"\n",
        "  return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz38FiI9kYW_",
        "outputId": "7200f63d-304c-4c38-dbfd-b1ed42273b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of five clusters:  1222 1210 743 781 1548\n"
          ]
        }
      ],
      "source": [
        "print(\"length of five clusters: \", len(result[0]), len(result[1]), len(result[2]), len(result[3]), len(result[4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXaQiuUufAHK"
      },
      "source": [
        "### Questions and Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1i-_8q4fFIk"
      },
      "source": [
        "(a) When using k=5 clusters, give a few examples of the documents assigned to each cluster, and the top 5 tokens with the highest magnitude in the corresponding centroid. [2 marks] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JXLcTWifzsq",
        "outputId": "e702d48f-8189-4764-b955-5e867b61badc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================Four conversations of cluster  1 ================================\n",
            "I don't know.\n",
            "You see that's what bothers me.  No other bottle of nasal spray was found in the house.  The police looked. There was only the one bottle.  But you say you didn't arrive until after Mr. Marsh was dead -- yet we know he was using the nasal spray prior to his death.  How do you think it got there?\n",
            "P.M.\n",
            "A.M -- or P.M.?\n",
            "Three fifteen.\n",
            "Would you read for us the time of the purchase?\n",
            "Dristan nasal spray.\n",
            "There's an item you picked up that's marked.  Will you read it?\n",
            "Yes.\n",
            "Well - I'm a little confused.  This is a charge receipt from Rosen's Drug Store where Mr. Marsh had an account.  It's dated the day of the murder.  Is this your signature?\n",
            "Yes.\n",
            "\n",
            "Gulp!\n",
            "Yes.\n",
            "Are they following us?\n",
            "\n",
            "They are with me!\n",
            "Who are they? Lieutenant, get those people away from there.\n",
            "\n",
            "Really? Maybe I can get you on my screen and see you at last!\n",
            "...Not far, now.\n",
            "\n",
            "================Four conversations of cluster  2 ================================\n",
            "I don't think so.\n",
            "That you come with me.\n",
            "What do you suggest?\n",
            "Don't go home.  And don't go to work.  Either one could be bad.\n",
            "I'm going to find him.  Because he'd find me.\n",
            "No idea.  Honest.  What are you going to do?\n",
            "Where do you think Jerry is?\n",
            "Jonas builds assassins for a living.  Several of whom may be in place already.  We'd like to kill a few birds with one stone.\n",
            "He's shown himself.  Why haven't you arrested him or killed him or done whatever it is you do?\n",
            "He's why I watch Jerry.  Jerry's the bait for Jonas.\n",
            "And Jonas?\n",
            "I'm, it really doesn't matter. Think C.I.A. and exponentiate. I'm a government employee and I've been watching Jerry for awhile.\n",
            "\n",
            "I'm the artist.  Like my mother.\n",
            "You the architect?\n",
            "\n",
            "Please.\n",
            "Ice?\n",
            "Fine.\n",
            "Jack Daniel's okay?  It's gonna have to be.\n",
            "\n",
            "I need the money to smooth my way, you understand? Now, have you got that sort of cash here or do we need to meet in the morning?\n",
            "Excuse me?\n",
            "Your brother got mixed up with child procurers  and tried to make this world a better place, Mrs De Moraes. And having rescued one little life he unwisely set out to repeat the exercise.  You dont mess around with child procurers. Right now my guess is hes either on the run, held captive, or dead.  I understand your misgivings, Mrs De Moraes. But Ive seen the boy and made telephone contact with the man Leon bought him from. If anyone knows what happened to your brother it will be that man. Which leads me to why Im here at such a late hour. I need 20,000, in cash, by 11 this morning.\n",
            "What... What are you talking about?\n",
            "You seem surprised. Could it be you dont think that badly of him after all?  You neednt worry. It seems his motives were pure. From what I can make out he bought the boy to rescue him from further abuse.\n",
            "\n",
            "================Four conversations of cluster  3 ================================\n",
            "Who are you?\n",
            "No. Don't ever want to go out without telling us.\n",
            "\n",
            "Which is it?\n",
            "NOOO!\n",
            "Then he dies. Right now.\n",
            "No...\n",
            "I wanna play a game.\n",
            "\n",
            "No.\n",
            "Did you ever tie him up?\n",
            "Exactly what do you have in mind, Mr. Corrigan.\n",
            "Did you ever engage in sado- masochistic activity with him?\n",
            "\n",
            "I'll request the scientists from Pacific-Tech to monitor the drop.  We'll clear the area all around.  After that we'll hit them all over the world. I'll have long-range bombers alerted, loaded and standing by.\n",
            "Then our first target will be the initial landing place outside Los Angeles.\n",
            "There's only one thing that will stop the Martians!  We've held back pre- viously because of the danger of radiation to civilians.  Now there's no choice.  The United Nations has voted authority to the United States.  The White House will confirm an order to use the Atom bomb.\n",
            "\n",
            "================Four conversations of cluster  4 ================================\n",
            "Every time I said it, it was. I never really thought I was going to make it.\n",
            "It wasn't a lie.\n",
            "Every day for the last five years, I told myself someday I would be out here again. No more bars. No more guards. No more fights just to stay alive.  Every day for the last five years, I told myself that lie.\n",
            "\n",
            "I say you should get a shrink.\n",
            "We should get ourselves a lawyer.\n",
            "Not my head, buddy. Not me. I'm gettin' a headache just listenin' to you.\n",
            "Maybe that's the only way to get through. Besides, six heads'll be better than one.\n",
            "Come on, Professor. The army's not gonna give you any answers. You'll be buttin' your head against a stone wall.\n",
            "\n",
            "I heard stories.  I don't think you wanna know.\n",
            "I wish they'd tell us what they're going to do with us.\n",
            "I sure wish Superman was around. He wouldn't let any of this go on. Not for one minute.\n",
            "\n",
            "I heard stories.  I don't think you wanna know.\n",
            "I wish they'd tell us what they're going to do with us.\n",
            "I sure wish Superman was around. He wouldn't let any of this go on. Not for one minute.\n",
            "\n",
            "================Four conversations of cluster  5 ================================\n",
            "We talked.\n",
            "What?... What'd you do?\n",
            "He came by.\n",
            "He called you?\n",
            "He told me last night.\n",
            "How'd you find that out?\n",
            "It's a lot of money. About a half-a- million dollars. All of it in Cabo in safe deposit boxes and more comin' in.\n",
            "Well, if the A.T.F. guy is the one who wants you, that'll only interest him up to a point.\n",
            "I want to talk to them first. I know more now about Ordell's money.\n",
            "What I meant was have a lawyer do the negotiating for you.\n",
            "I don't know yet. I'm going to talk with Dargus and Nicolet today. Do what you suggested. Offer to help and see what happens.\n",
            "Are you?\n",
            "I called in sick this morning. As far as the airline knows, I'm still available.\n",
            "\n",
            "It's not strictly legal.\n",
            "...Well, if the pay's right and it's legal I'll do it.\n",
            "Got a job for you.\n",
            "...That's the test, ain't it? Test of true love--\n",
            "\n",
            "Sure, Lana, I'd love to.\n",
            "I can't  Clark already asked me.  Didn't you?  He used to love this song.  Didn't you?  So he just said --  \"Would you dance with me?\"\n",
            "\n",
            "Where you been, Harry?\n",
            "Jesus, if I have a heart attack, I hope you know what to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# the reuslt of clustering is a big-list contains five list of data\n",
        "for i in range(5):\n",
        "  print(\"================Four conversations of cluster \", i+1,\"================================\")\n",
        "  print(cluster_texts[search_index[i][0]])\n",
        "  print(cluster_texts[search_index[i][1]])\n",
        "  print(cluster_texts[search_index[i][2]])\n",
        "  print(cluster_texts[search_index[i][3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfIFplmivjdR",
        "outputId": "3662a24b-4a3c-4b76-d77f-5466a1bf9052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=======top 5 tokens estimated by TF in centroid  1 ========================\n",
            "lieutenant,people,away,\n",
            "=======top 5 tokens estimated by TF in centroid  2 ========================\n",
            "force,get,drug,trouble,got,\n",
            "=======top 5 tokens estimated by TF in centroid  3 ========================\n",
            "think,come,oughta,kind,warning,\n",
            "=======top 5 tokens estimated by TF in centroid  4 ========================\n",
            "try,store,sense,think,close,\n",
            "=======top 5 tokens estimated by TF in centroid  5 ========================\n",
            "know,oh,goodbye,lion,right,\n"
          ]
        }
      ],
      "source": [
        "count = 1\n",
        "for token_index in centroids_index:\n",
        "  # print(conva_token_all[token_index])\n",
        "  term = make_tf_sparse(conva_token_all[token_index], vocabularies)\n",
        "  list = sorted(term.items(), key=lambda d: d[1], reverse=True)\n",
        "  tokens5 = \"\"\n",
        "  \n",
        "  print(\"=======top 5 tokens estimated by TF in centroid \", count,\"========================\")\n",
        "  if len(list) < 5:\n",
        "    for i in range(len(list)):\n",
        "      tokens5 += voca_search[list[i][0]] + \",\" \n",
        "    print(tokens5)\n",
        "  else:\n",
        "    for i in range(5):\n",
        "      tokens5 += voca_search[list[i][0]] + \",\" \n",
        "    print(tokens5)\n",
        "  count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bqjjSaffHcz"
      },
      "source": [
        "(b) Based on (a), do the clusters make sense? Are there certain topics that appear in some but not others? [2 marks] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyCjpO5p5OK5"
      },
      "source": [
        "Actually the result of clustering doesn't make sense. It can be seen in the conversations of one cluster that there is no pattern. There neither any obvious topic among converstions in one cluster, but it seems that in the second cluster, conversations are a litte bit related to jobs or carrers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfXxe1nwfKMn"
      },
      "source": [
        "(c) Construct a confusion matrix between the k=5 clusters and your target labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBNYJvnG-psR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "dj8p6DWYq18-"
      },
      "outputs": [],
      "source": [
        "def setClusterLabel(label_indexes):\n",
        "  all_labels = []\n",
        "  for i in label_indexes:\n",
        "    all_labels.append(cluster_labels[i])\n",
        "  count = Counter(all_labels).most_common(1)\n",
        "\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setClusterLabel(label_indexes):\n",
        "  all_labels = []\n",
        "  for i in label_indexes:\n",
        "    all_labels.append(cluster_labels[i])\n",
        "  count = Counter(all_labels).most_common(3)\n",
        "\n",
        "  return count"
      ],
      "metadata": {
        "id": "G9fjdsYV_EXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2D6W6Xgpobg",
        "outputId": "71abfa33-50f0-45b6-9707-c95bd86515ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('action', 332)], [('action', 318)], [('action', 200)], [('action', 197)], [('action', 408)]]\n"
          ]
        }
      ],
      "source": [
        "search_index = []\n",
        "cluster_result_labels = []\n",
        "centroids_index = []\n",
        "for i in range(5):\n",
        "  list = []\n",
        "  centroids_index.append(nor_vec.index(centroids[i]))\n",
        "  for item in result[i]:\n",
        "    index = nor_vec.index(item)\n",
        "    list.append(index)\n",
        "  search_index.append(list)\n",
        "for x in search_index:\n",
        "  cluster_result_labels.append(setClusterLabel(x))\n",
        "\n",
        "print(cluster_result_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_index = []\n",
        "cluster_result_labels = []\n",
        "centroids_index = []\n",
        "for i in range(5):\n",
        "  list = []\n",
        "  centroids_index.append(nor_vec.index(centroids[i]))\n",
        "  for item in result[i]:\n",
        "    index = nor_vec.index(item)\n",
        "    list.append(index)\n",
        "  search_index.append(list)\n",
        "for x in search_index:\n",
        "  cluster_result_labels.append(setClusterLabel(x))\n",
        "\n",
        "print(cluster_result_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "Z5z353-K_IEt",
        "outputId": "2d6e902b-f953-488a-c61c-eb5772c64c42"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-cec0fa50e46f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msearch_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mcluster_result_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetClusterLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_result_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-1dfed802c511>\u001b[0m in \u001b[0;36msetClusterLabel\u001b[0;34m(label_indexes)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_indexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrc3Awf6sk2"
      },
      "source": [
        "Based on the evaluation of the most common label occurred in a cluster, it shows that all clusters have the same label: action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "jDCnYJpX7lEH",
        "outputId": "660cfb73-ab5e-4ddd-adb8-61b2d5356075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5517 5517\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Predicted', ylabel='Actual'>"
            ]
          },
          "execution_count": 198,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "\n",
        "all_labels = ['drama', 'thriller', 'comedy', 'action', 'crime', 'romance', 'adventure', 'sci-fi', 'mystery', 'horror']\n",
        "labels_cluster_predict = ['action' for i in range(len(cluster_labels))]\n",
        "\n",
        "print(len(labels_cluster_predict), len(cluster_labels))\n",
        "\n",
        "def plotConfusionMatrix(labels_val, labels_predicted):\n",
        "  x = confusion_matrix(labels_val, labels_predicted)\n",
        "  plot = sn.heatmap(np.array(x), \n",
        "                    annot=True, \n",
        "                    annot_kws={\"size\": 12}, \n",
        "                    fmt='g', \n",
        "                    cbar=False,\n",
        "                    xticklabels=all_labels, \n",
        "                    yticklabels=all_labels)\n",
        "  plot.set(xlabel='Predicted', ylabel='Actual')\n",
        "  return plot\n",
        "\n",
        "plotConfusionMatrix(cluster_labels, labels_cluster_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4LQmrwKfWI7"
      },
      "source": [
        "(d) What trends, if any, do you notice from the confusion matrix? Does it look like some clusters are able to pick up on a single label? When a cluster includes multiple labels, are they related? [2 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vse8FVqB4EaI"
      },
      "source": [
        "## **Part 3 Comparing Classifiers**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Preparation"
      ],
      "metadata": {
        "id": "4RZwg0RUyabs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Models"
      ],
      "metadata": {
        "id": "0T4MTomVtws8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omAqFgAfIABU",
        "outputId": "cfe971b0-2feb-40aa-a19d-9aea0b593000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf_train.shape=(3291, 10384)\n",
            "tfidf_val.shape=(1097, 10384)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_train = vectorizer.fit_transform(texts_train)\n",
        "tfidf_val = vectorizer.transform(texts_val)\n",
        "\n",
        "print(f\"{tfidf_train.shape=}\")\n",
        "print(f\"{tfidf_val.shape=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpjGzFECboVE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "def text_pipeline_simple_spacy(conv_text):\n",
        "  tokens = []\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  nlp.remove_pipe('parser')\n",
        "  doc = nlp(conv_text)\n",
        "  for token in doc:\n",
        "    if not token.is_space and not token.is_punct:\n",
        "      tokens.append(token.lemma_.lower())\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "mEuFRzH-yif9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "train_tokens = []\n",
        "for conva in tqdm(texts_train):\n",
        "  #这时候每一个conva都是一长段的字符串\n",
        "  one_list = text_pipeline_simple_spacy(conva)\n",
        "  #之后数据处理用的都是token，这里提前处理了\n",
        "  train_tokens.append(one_list)\n",
        "\n",
        "val_tokens = []\n",
        "for conva in tqdm(texts_val):\n",
        "  #这时候每一个conva都是一长段的字符串\n",
        "  one_list = text_pipeline_simple_spacy(conva)\n",
        "  #之后数据处理用的都是token，这里提前处理了\n",
        "  val_tokens.append(one_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2r3RbdT6nDj",
        "outputId": "7e183dbc-96e4-41dd-ef06-1e7d9d2fe1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3283/3283 [33:22<00:00,  1.64it/s]\n",
            "100%|██████████| 1095/1095 [10:57<00:00,  1.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make train and val sets have the same features, exclusive tokens in vocabulary list of training set will be ignored:"
      ],
      "metadata": {
        "id": "ERs1rqpDKlUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "vocab_train = make_vocabulary(train_tokens)\n",
        "vocab_val = make_vocabulary(val_tokens)"
      ],
      "metadata": {
        "id": "tw0RPw7xKjy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_onehot_dense(tokens, vocab):\n",
        "  vector = [ 0 for _ in vocab ]\n",
        "  for t in tokens:\n",
        "    id = vocab[t]\n",
        "    vector[id] = 1\n",
        "  return vector"
      ],
      "metadata": {
        "id": "uETiFm-BUzWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_onehot_ignorenewtokens(tokens, vocab):\n",
        "  sparse_vector = { vocab[t]:1 for t in sorted(set(tokens)) if t in vocab }\n",
        "  return sparse_vector"
      ],
      "metadata": {
        "id": "CZxQoqWk7cXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_dense(sv, vector_length):\n",
        "  dense_vector = []\n",
        "  for index in range(vector_length):\n",
        "    dense_vector.append(sv.get(index,0))\n",
        "  return dense_vector"
      ],
      "metadata": {
        "id": "BC7Al7nd7kHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_val = [make_onehot_dense(tokens, vocab_val) for tokens in val_tokens]"
      ],
      "metadata": {
        "id": "nMvZe2d9U438"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_train = [make_onehot_ignorenewtokens(tokens, vocab_val) for tokens in train_tokens]\n",
        "onehot_train = []\n",
        "for sv in sparse_train:\n",
        "  vec = sparse_to_dense(sv, len(vocab_val))\n",
        "  onehot_train.append(vec)"
      ],
      "metadata": {
        "id": "g4asMqHUUpE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All vectors for classification are prepared:\n",
        "\n",
        "Sparse tfidf vectors : tfidf_val and tfidf_train\n",
        "\n",
        "Dense onehot vectors : onehot_val and onehot_train"
      ],
      "metadata": {
        "id": "lwPvIPI8EnN2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyHYuC5oYiv_"
      },
      "source": [
        "####Dummy Classifier with strategy=\"most_frequent\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "dummy_clf1 = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf1.fit(onehot_train, labels_train)\n",
        "labels_predicted_dummyt = dummy_clf1.predict(onehot_train)\n",
        "labels_predicted_dummyv = dummy_clf1.predict(onehot_val)"
      ],
      "metadata": {
        "id": "8JHfq4YAFseW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcSJvO7DGEw",
        "outputId": "15dc5cc7-11be-4994-a9db-f46a9b96fd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====on validation set====on training set \n",
            "accuracy_dt1=     0.253 accuracy_dv1=     0.238\n",
            "precision_dt1=    0.0253 precision_dv1=     0.024\n",
            "recall_dt1=       0.1 recall_dv1=     0.100\n",
            "f1_dt1=    0.0404 f1_dv1=     0.038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(\"=============on training set====on validation set \")\n",
        "accuracy_dt1 = accuracy_score(labels_train, labels_predicted_dummyt)\n",
        "accuracy_dv1 = accuracy_score(labels_val, labels_predicted_dummyv)\n",
        "print(f\"{accuracy_dt1=:10.3}\",f\"{accuracy_dv1=:10.3f}\")\n",
        "\n",
        "precision_dt1 = precision_score(labels_train, labels_predicted_dummyt, average=\"macro\")\n",
        "precision_dv1 = precision_score(labels_val, labels_predicted_dummyv, average=\"macro\")\n",
        "print(f\"{precision_dt1=:10.3}\",f\"{precision_dv1=:10.3f}\")\n",
        "\n",
        "recall_dt1 = recall_score(labels_train, labels_predicted_dummyt, average=\"macro\")\n",
        "recall_dv1 = recall_score(labels_val, labels_predicted_dummyv, average=\"macro\")\n",
        "print(f\"{recall_dt1=:10.3}\",f\"{recall_dv1=:10.3f}\")\n",
        "\n",
        "f1_dt1 = f1_score(labels_train, labels_predicted_dummyt, average=\"macro\")\n",
        "f1_dv1 = f1_score(labels_val, labels_predicted_dummyv, average=\"macro\")\n",
        "print(f\"{f1_dt1=:10.3}\",f\"{f1_dv1=:10.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYgjToe7Yquz"
      },
      "source": [
        "#### Dummy Classifier with strategy=\"stratified\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo6tbgR0EYB9"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "dummy_clf2 = DummyClassifier(strategy=\"stratified\", random_state=123)\n",
        "dummy_clf2.fit(onehot_train, labels_train)\n",
        "labels_predicted2_dummyt = dummy_clf2.predict(onehot_train)\n",
        "labels_predicted2_dummyv = dummy_clf2.predict(onehot_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=============on training set====on validation set \")\n",
        "accuracy_dt2 = accuracy_score(labels_train, labels_predicted2_dummyt)\n",
        "accuracy_dv2 = accuracy_score(labels_val, labels_predicted2_dummyv)\n",
        "print(f\"{accuracy_dt2=:10.3}\",f\"{accuracy_dv2=:10.3f}\")\n",
        "\n",
        "precision_dt2 = precision_score(labels_train, labels_predicted2_dummyt, average=\"macro\")\n",
        "precision_dv2 = precision_score(labels_val, labels_predicted2_dummyv, average=\"macro\")\n",
        "print(f\"{precision_dt2=:10.3}\",f\"{precision_dv2=:10.3f}\")\n",
        "\n",
        "recall_dt2 = recall_score(labels_train, labels_predicted2_dummyt, average=\"macro\")\n",
        "recall_dv2 = recall_score(labels_val, labels_predicted2_dummyv, average=\"macro\")\n",
        "print(f\"{recall_dt2=:10.3}\",f\"{recall_dv2=:10.3f}\")\n",
        "\n",
        "f1_dt2 = f1_score(labels_train, labels_predicted2_dummyt, average=\"macro\")\n",
        "f1_dv2 = f1_score(labels_val, labels_predicted2_dummyv, average=\"macro\")\n",
        "print(f\"{f1_dt2=:10.3}\",f\"{f1_dv2=:10.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcVd9yf7Pf5W",
        "outputId": "f7ffc36c-d5ff-438c-e3ee-010c1106ed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============on validation set====on training set \n",
            "accuracy_dt2=     0.191 accuracy_dv2=     0.190\n",
            "precision_dt2=    0.0962 precision_dv2=     0.091\n",
            "recall_dt2=    0.0962 recall_dv2=     0.090\n",
            "f1_dt2=    0.0962 f1_dv2=     0.090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpytg2g-YxSV"
      },
      "source": [
        "#### LogisticRegression with One-hot vectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_clf_onehot = LogisticRegression(random_state=42)\n",
        "lr_clf_onehot.fit(onehot_train, labels_train)\n",
        "labels_predicted_lone_val = lr_clf_onehot.predict(onehot_val)\n",
        "labels_predicted_lone_train = lr_clf_onehot.predict(onehot_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I0Qc7kv2de1",
        "outputId": "ae334d44-4dc2-4fea-a898-fd36f02312d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=============on training set====on validation set \")\n",
        "accuracy_ltone = accuracy_score(labels_train, labels_predicted_lone_train)\n",
        "accuracy_lvone = accuracy_score(labels_val, labels_predicted_lone_val)\n",
        "print(f\"{accuracy_ltone=:10.3}\",f\"{accuracy_lvone=:10.3f}\")\n",
        "\n",
        "precision_ltone = precision_score(labels_train, labels_predicted_lone_train, average=\"macro\")\n",
        "precision_lvone = precision_score(labels_val, labels_predicted_lone_val, average=\"macro\")\n",
        "print(f\"{precision_ltone=:10.3}\",f\"{precision_lvone=:10.3f}\")\n",
        "\n",
        "recall_ltone = recall_score(labels_train, labels_predicted_lone_train, average=\"macro\")\n",
        "recall_lvone = recall_score(labels_val, labels_predicted_lone_val, average=\"macro\")\n",
        "print(f\"{recall_ltone=:10.3}\",f\"{recall_lvone=:10.3f}\")\n",
        "\n",
        "f1_ltone = f1_score(labels_train, labels_predicted_lone_train, average=\"macro\")\n",
        "f1_lvone = f1_score(labels_val, labels_predicted_lone_val, average=\"macro\")\n",
        "print(f\"{f1_ltone=:10.3}\",f\"{f1_lvone=:10.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFfkR-4B92IS",
        "outputId": "c5ba1c61-aceb-41de-b02d-56b9e2042781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============on training set====on validation set \n",
            "accuracy_ltone=     0.853 accuracy_lvone=     0.359\n",
            "precision_ltone=     0.935 precision_lvone=     0.293\n",
            "recall_ltone=     0.777 recall_lvone=     0.197\n",
            "f1_ltone=     0.839 f1_lvone=     0.213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvjm8mYtY68k"
      },
      "source": [
        "#### LogisticRegression with TF-IDF vectorization (default settings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_clf_tfidf = LogisticRegression()\n",
        "lr_clf_tfidf.fit(tfidf_train, labels_train)\n",
        "labels_predicted_ltfidf_val = lr_clf_tfidf.predict(tfidf_val)\n",
        "labels_predicted_ltfidf_train = lr_clf_tfidf.predict(tfidf_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmuXtPh_2eJN",
        "outputId": "077cc9fe-45b2-4e13-b981-11e0171388ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=============on training set====on validation set \")\n",
        "accuracy_lttfidf = accuracy_score(labels_train, labels_predicted_ltfidf_train)\n",
        "accuracy_lvtfidf = accuracy_score(labels_val, labels_predicted_ltfidf_val)\n",
        "print(f\"{accuracy_lttfidf=:10.3}\",f\"{accuracy_lvtfidf=:10.3f}\")\n",
        "\n",
        "precision_lttfidf = precision_score(labels_train, labels_predicted_ltfidf_train, average=\"macro\")\n",
        "precision_lvtfidf = precision_score(labels_val, labels_predicted_ltfidf_val, average=\"macro\")\n",
        "print(f\"{precision_lttfidf=:10.3}\",f\"{precision_lvtfidf=:10.3f}\")\n",
        "\n",
        "recall_lttfidf = recall_score(labels_train, labels_predicted_ltfidf_train, average=\"macro\")\n",
        "recall_lvtfidf = recall_score(labels_val, labels_predicted_ltfidf_val, average=\"macro\")\n",
        "print(f\"{recall_lttfidf=:10.3}\",f\"{recall_lvtfidf=:10.3f}\")\n",
        "\n",
        "f1_lttfidf = f1_score(labels_train, labels_predicted_ltfidf_train, average=\"macro\")\n",
        "f1_lvtfidf = f1_score(labels_val, labels_predicted_ltfidf_val, average=\"macro\")\n",
        "print(f\"{f1_lttfidf=:10.3}\",f\"{f1_lvtfidf=:10.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfpInZHC93vT",
        "outputId": "73fdb0b5-3b41-4d57-8c50-4e0767ad6ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============on training set====on validation set \n",
            "accuracy_lttfidf=     0.684 accuracy_lvtfidf=     0.341\n",
            "precision_lttfidf=     0.493 precision_lvtfidf=     0.123\n",
            "recall_lttfidf=     0.306 recall_lvtfidf=     0.140\n",
            "f1_lttfidf=     0.299 f1_lvtfidf=     0.123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMsS7Q0AY9iZ"
      },
      "source": [
        "#### SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLOR3pnpXYao"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "clf_svc = SVC(kernel='rbf')\n",
        "clf_svc.fit(onehot_train, labels_train)\n",
        "labels_predicted_svc_val = clf_svc.predict(onehot_val)\n",
        "labels_predicted_svc_train = clf_svc.predict(onehot_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=============on training set====on validation set \")\n",
        "accuracy_svct = accuracy_score(labels_train, labels_predicted_svc_train)\n",
        "accuracy_svcv = accuracy_score(labels_val, labels_predicted_svc_val)\n",
        "print(f\"{accuracy_svct=:10.3}\",f\"{accuracy_svcv=:10.3f}\")\n",
        "\n",
        "precision_svct = precision_score(labels_train, labels_predicted_svc_train, average=\"macro\")\n",
        "precision_svcv = precision_score(labels_val, labels_predicted_svc_val, average=\"macro\")\n",
        "print(f\"{precision_svct=:10.3}\", f\"{precision_svcv=:10.3f}\")\n",
        "\n",
        "recall_svct = recall_score(labels_train, labels_predicted_svc_train, average=\"macro\")\n",
        "recall_svcv= recall_score(labels_val, labels_predicted_svc_val, average=\"macro\")\n",
        "print(f\"{recall_svct=:10.3}\",f\"{recall_svcv=:10.3f}\")\n",
        "\n",
        "f1_svct = f1_score(labels_train, labels_predicted_svc_train, average=\"macro\")\n",
        "f1_svcv = f1_score(labels_val, labels_predicted_svc_val, average=\"macro\")\n",
        "print(f\"{f1_svct=:10.3}\",f\"{f1_svcv=:10.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df_JiiJjSILf",
        "outputId": "cc31ff02-99f4-4c7b-9f2d-895a067077af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============on training set====on validation set \n",
            "accuracy_svct=     0.676 accuracy_svcv=     0.324\n",
            "precision_svct=     0.802 precision_svcv=     0.199\n",
            "recall_svct=     0.342 recall_svcv=     0.132\n",
            "f1_svct=     0.375 f1_svcv=     0.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of the five methods above"
      ],
      "metadata": {
        "id": "gFVDf0ynFMmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "dummy1_train = [accuracy_dt1,precision_dt1,recall_dt1, f1_dt1]\n",
        "dummy1_val = [accuracy_dv1,precision_dv1,recall_dv1, f1_dv1]\n",
        "dummy2_train = [accuracy_dt2, precision_dt2, recall_dt2, f1_dt2]\n",
        "dummy2_val = [accuracy_dv2, precision_dv2, recall_dv2, f1_dv2]\n",
        "lr1_train = [accuracy_ltone, precision_ltone, recall_ltone, f1_ltone]\n",
        "lr1_val = [accuracy_lvone, precision_lvone, recall_lvone, f1_lvone]\n",
        "lr2_train = [accuracy_lttfidf, precision_lttfidf, recall_lttfidf,f1_lttfidf]\n",
        "lr2_val = [accuracy_lvtfidf, precision_lvtfidf, recall_lvtfidf,f1_lvtfidf]\n",
        "svc_train = [accuracy_svct,precision_svct, recall_svct,f1_svct]\n",
        "svc_val = [accuracy_svcv,precision_svcv, recall_svcv,f1_svcv]\n",
        "datas = np.array([dummy1_train, dummy1_val, dummy2_train, dummy2_val, lr1_train, lr1_val, lr2_train, lr2_val, svc_train, svc_val]) \n",
        "evaluations = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "types = [\n",
        "      \"DummyC1 TRAIN\",\n",
        "      \"DummyC1 VALIDATION\",   \n",
        "      \"DummyC2 TRAIN\",\n",
        "      \"DummyC2 VALIDATION\",\n",
        "      \"LRC1 TRAIN\" ,\n",
        "      \"LRC1 VALIDATION\" ,\n",
        "      \"LRC2 TRAIN\",\n",
        "      \"LRC2 VALIDATION\",\n",
        "      \"SVC TRAIN\",\n",
        "      \"SVC VALIDATION\"\n",
        "      ]\n",
        "\n",
        "dataframe = pd.DataFrame(datas, columns=evaluations, index=types)\n",
        "# print(dataframe)\n",
        "from decimal import Decimal\n",
        "\n",
        "plt.xticks(np.arange(len(evaluations)), labels=evaluations, \n",
        "                     rotation=45, rotation_mode=\"anchor\", ha=\"right\" ,fontsize=20)\n",
        "plt.yticks(np.arange(len(types)), labels=types, fontsize=20)    \n",
        "plt.title(\"Evaluation Matrix\",fontsize=20)\n",
        "\n",
        "for i in range(len(types)):\n",
        "  for j in range(len(evaluations)):\n",
        "    text = plt.text(j, i, Decimal(datas[i, j]).quantize(Decimal('0.000')), ha=\"center\", va=\"center\", color=\"w\",fontsize=20)\n",
        "plt.rcParams['figure.figsize']=(20, 20)\n",
        "plt.imshow(datas)\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eSo1g85FaXCO",
        "outputId": "dcb37464-94eb-4b14-ce31-f2d5066defc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple introduction of models:\n",
        "\n",
        "logistic regression: a widely used classifier that can deal with dependencies between features and incorporate arbitrary features easily, but it will overfit on very sparse data like the one-hot vectors in this trial. \n",
        "\n",
        "SVC: a model with a selected kernel which reflects data to higher dimension and then aims to find the largest margin among groups with a separation. It is effective in high dimensional spaces even there are more dimensions than the samples. It is designed for binary question, so the multiclass classificaiton is handled in one-vs-one scheme."
      ],
      "metadata": {
        "id": "xs4jgpTKFdOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using dummy results as baselines, firstly discussing the situation on the training set (LRC1, LRC2, SVC correlating to LogisticRegression with One-hot vectorization, LogisticRegression with TF-IDF vectorization, SVC respectively), it shows that all of them performed well by taking accuracy into consideration that they all classified over half of the conversations into the right genres, LCR1 did especially well in this perspective. When it comes to precision, LCR1 and SVC both achieved the goal that most of the conversations assigned to one genre are truly tagged with this genre, whereas LRC2 only did a little bit less than half of that. And for the recall score, only LRC1 took over 70% that conversations in one genre are correctly classified, neither LCR2 nor SVC just achieved nearly 30%. This is the same situation on f1 score. And all this trend occurred in validation set but with much lower socres in four measurements, which may be related to overfitting problem."
      ],
      "metadata": {
        "id": "QkySem8LMTte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More specificlly, accuracy is a kind of global measurement on all data which may be useless on unbalanced dataset, so precision and recall which explore the situation in each classification are more useful. Based on this, it can be seen from the figure above that for three models, overfitting occurred in LRC1 and SVC to some extent, since the scores of precision, recall and f1 on the training set displayed higher than those on the validation set, especially on model LRC1. For model LRC1, there are over 4000 feathers, which may be one reason for its definetly overfitting. However, on LRC2, both precision and recall scores of training set and validation set are under 50%, implying the underfitting of this model. "
      ],
      "metadata": {
        "id": "N_bN-7EcfT1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the section on the exploration of the dataset, to get a more balanced dataset, only the 10 most frequent genres were maintained, but showing their counts: \n",
        "\n",
        "('drama', 3240), ('thriller', 2522), ('comedy', 1776), ('action', 1535), ('crime', 1479), ('romance', 1395), ('mystery', 1086), ('sci-fi', 1078), ('adventure', 1066), ('horror', 836),\n",
        "\n",
        "there are over 3000 conversations in the most genre class which is three or four times more than some of the others, so it cannot be treated as a balanced dataset. And after vectorization, it can be seen that an apparent noise occurred: like 'didn't', '--name' and '$', they are stored as didn, t, didn't, --name, $, indicating an optimizing on tokenization and vectorization."
      ],
      "metadata": {
        "id": "WVXWp4silbVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarising, the best performance came from LogisticRegression with One-hot vectorization, f1-scores on each class are shown in bar charts."
      ],
      "metadata": {
        "id": "Auiu9-Awwzeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "# report_val = classification_report(labels_val, labels_predicted_lone_val, target_names=['drama', 'thriller', 'comedy', 'action', 'crime', 'romance', 'sci-fi', 'adventure', 'mystery', 'horror'])\n",
        "labels=['drama', 'thriller', 'comedy', 'action', 'crime', 'romance', 'sci-fi', 'adventure', 'mystery', 'horror']\n",
        "p_class_val, r_class_val, f_class_val, support_micro_val = precision_recall_fscore_support(\n",
        "                y_true=labels_val, y_pred=labels_predicted_lone_val, labels=labels, average=None)\n",
        "\n",
        "p_class_t, r_class_t, f_class_t, support_micro_t = precision_recall_fscore_support(\n",
        "                y_true=labels_train, y_pred=labels_predicted_lone_train, labels=labels, average=None)\n",
        "\n",
        "print(f_class_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f22pzTMGxIpb",
        "outputId": "7793f731-dc28-4866-de4a-96a4c0304a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.39652174 0.         0.4265233  0.3908046  0.22857143 0.\n",
            " 0.4        0.11764706 0.         0.17204301]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "bars2 = plt.bar(labels, f_class_t, align='center', alpha=0.5)\n",
        "plt.title(\"f1 scores of each class on training set\")\n",
        "for b in bars2: \n",
        "  height = b.get_height()\n",
        "  ax.annotate(np.round(height, 3),\n",
        "        xy=(b.get_x() + b.get_width() / 2, height), \n",
        "        xytext=(0,3), \n",
        "        textcoords=\"offset points\", \n",
        "        va = 'bottom', ha = 'center' \n",
        "        )\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "3KQuUN1o-PsV",
        "outputId": "2a260f6c-2f86-4e9c-849a-7fdf5ef36872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "bars1 = plt.bar(labels, f_class_val, align='center', alpha=0.5)\n",
        "plt.title(\"f1 scores of each class on validation set\")\n",
        "for b in bars1: \n",
        "  height = b.get_height()\n",
        "  ax.annotate(np.round(height, 3),\n",
        "        xy=(b.get_x() + b.get_width() / 2, height), \n",
        "        xytext=(0,3), \n",
        "        textcoords=\"offset points\", \n",
        "        va = 'bottom', ha = 'center' \n",
        "        )\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "oZ7VSSUR9_oU",
        "outputId": "6efd59c8-176f-4dd5-96d9-5640924d9441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForest Classifier\n"
      ],
      "metadata": {
        "id": "OQgevKDwA1sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "def text_pipeline_simple_spacy(conv_text):\n",
        "  tokens = []\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  nlp.remove_pipe('parser')\n",
        "  doc = nlp(conv_text)\n",
        "  for token in doc:\n",
        "    if not token.is_space and not token.is_punct:\n",
        "      tokens.append(token.lemma_.lower())\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "0y9_fIxiBgaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=text_pipeline_simple_spacy)\n",
        "# vectorizer = TfidfVectorizer()\n",
        "tfidf_train_spacy = vectorizer.fit_transform(texts_train)\n",
        "tfidf_val_spacy = vectorizer.transform(texts_val)\n",
        "\n",
        "print(f\"{tfidf_train_spacy.shape=}\")\n",
        "print(f\"{tfidf_val_spacy.shape=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuhdHyEnBd_N",
        "outputId": "8be61162-7230-4db5-b868-fbb823540d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf_train_spacy.shape=(3291, 9017)\n",
            "tfidf_val_spacy.shape=(1097, 9017)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf_RF = RandomForestClassifier(random_state=42, n_estimators=500)\n",
        "clf_RF.fit(tfidf_train_spacy,labels_train)\n",
        "labels_predicted_rf_val = clf_RF.predict(tfidf_val_spacy)\n",
        "labels_predicted_rf_train = clf_RF.predict(tfidf_train_spacy)"
      ],
      "metadata": {
        "id": "C1tU9BijARSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=============on training set====on validation set \")\n",
        "accuracy_gnbt = accuracy_score(labels_train, labels_predicted_rf_train)\n",
        "accuracy_gnbv = accuracy_score(labels_val, labels_predicted_rf_val)\n",
        "print(f\"{accuracy_gnbt=:10.3}\",f\"{accuracy_gnbv=:10.3f}\")\n",
        "\n",
        "precision_gnbt = precision_score(labels_train, labels_predicted_rf_train, average=\"macro\")\n",
        "precision_gnbv = precision_score(labels_val, labels_predicted_rf_val, average=\"macro\")\n",
        "print(f\"{precision_gnbt=:10.3}\", f\"{precision_gnbv=:10.3f}\")\n",
        "\n",
        "recall_gnbt = recall_score(labels_train, labels_predicted_rf_train, average=\"macro\")\n",
        "recall_gnbv= recall_score(labels_val, labels_predicted_rf_val, average=\"macro\")\n",
        "print(f\"{recall_gnbt=:10.3}\",f\"{recall_gnbv=:10.3f}\")\n",
        "\n",
        "f1_gnbt = f1_score(labels_train, labels_predicted_rf_train, average=\"macro\")\n",
        "f1_gnbv = f1_score(labels_val, labels_predicted_rf_val, average=\"macro\")\n",
        "print(f\"{f1_gnbt=:10.3}\",f\"{f1_gnbv=:10.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4m2NZGVZdVZ",
        "outputId": "acf74db2-88d3-41ed-a5d5-368ebb24b001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============on training set====on validation set \n",
            "accuracy_gnbt=       1.0 accuracy_gnbv=     0.373\n",
            "precision_gnbt=       1.0 precision_gnbv=     0.575\n",
            "recall_gnbt=       1.0 recall_gnbv=     0.184\n",
            "f1_gnbt=       1.0 f1_gnbv=     0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForest is an intrgration of many sperated decision trees (bagging method). When a sample is inputed, the class of it is decided by the count of classification result made by trees in this forest. And each inner node of the decisioin tree is a desicion based on attribution. After going through the decided branches, the leaf node is the decision result. Here selected the parameter n_estimators=500 as the number of trees in this forest."
      ],
      "metadata": {
        "id": "9Br3f64DuaGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spacy to do tokenization, the result shows that on the training set, all scores are 1 higher than the scores of five other models, indicating this model can classify all conversations correctly into their genres. But on the validation set, the scores are much lower than those in training set even if the precision score is over 50%. This may be due to overfitting."
      ],
      "metadata": {
        "id": "X2Fr7BflALBE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W4GtjU-5BXu"
      },
      "source": [
        "## **Part 4 Parameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Searching process"
      ],
      "metadata": {
        "id": "W4erJMaCRocW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "param_grid = {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        \"class_weight\": [None, \"balanced\"],\n",
        "        'max_df' : [0.6, 0.7, 0.8, 0.9],\n",
        "        \"sublinear_tf\" :[True, False], \n",
        "        \"max_features\": [None, 1000, 3000, 5000, 6000, 7000, 8000, 9000, 10000]}\n",
        "best_params, best_f1 = None, 0\n",
        "for params in ParameterGrid(param_grid):\n",
        "    vectorizer = TfidfVectorizer(sublinear_tf=params['sublinear_tf'],max_df=params[\"max_df\"], max_features=params['max_features'])\n",
        "    X_train = vectorizer.fit_transform(texts_train)\n",
        "    X_val = vectorizer.transform(texts_val)\n",
        "    \n",
        "    clf = LogisticRegression(random_state=42, C=params['C'], class_weight=params['class_weight'])\n",
        "    clf.fit(X_train,labels_train)\n",
        "    \n",
        "    labels_predicted = clf.predict(X_val)\n",
        "    \n",
        "    f1 = f1_score(labels_val, labels_predicted, average=\"macro\")\n",
        "    print(f\"  Evaluating {params=} {f1=:.3f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_params = params\n",
        "        best_f1 = f1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c311a53-0bb1-4f48-d5ba-f48a49cdfa2e",
        "id": "J0LdP5NYxnef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.078\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.078\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.079\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.079\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.079\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.080\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.081\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.082\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.127\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.113\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.088\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.088\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.111\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.106\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.118\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.107\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.126\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.113\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.123\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.113\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.127\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.113\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.128\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.091\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.092\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.118\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.105\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.121\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.104\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.117\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.120\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.123\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.120\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.127\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.121\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.128\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.128\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.091\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.092\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.118\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.105\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.121\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.104\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.117\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.120\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.123\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.120\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.127\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.121\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.128\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.128\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.124\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.091\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.092\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.118\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.105\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.121\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.104\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.117\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.120\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.123\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.120\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.127\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.121\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.128\n",
            "  Evaluating params={'C': 0.01, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.124\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.104\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.107\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.107\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.107\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.106\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.113\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.106\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.113\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.106\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.108\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.113\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.109\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.112\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.110\n",
            "  Evaluating params={'C': 0.1, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.153\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.145\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.118\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.116\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.135\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.128\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.137\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.147\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.138\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.149\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.151\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.153\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.145\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.120\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.115\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.130\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.147\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.154\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.120\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.115\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.130\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.147\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.154\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.120\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.115\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.130\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.147\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.141\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.150\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.142\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.154\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.144\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.152\n",
            "  Evaluating params={'C': 0.1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.111\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.130\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.132\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.133\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.132\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.131\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.132\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.131\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.132\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.131\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.134\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.220\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.172\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.166\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.196\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.196\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.205\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.206\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.211\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.208\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.216\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.213\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.216\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.215\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.220\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.221\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.173\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.174\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.199\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.204\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.210\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.213\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.219\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.220\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.221\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.221\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.173\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.174\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.199\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.204\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.210\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.213\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.219\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.220\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.221\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.221\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.173\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.174\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.199\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.204\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.210\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.213\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.219\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.214\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.220\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.218\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.223\n",
            "  Evaluating params={'C': 1, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 10, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': None, 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': True} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': None, 'sublinear_tf': False} f1=0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': True} f1=0.189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 1000, 'sublinear_tf': False} f1=0.194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': True} f1=0.222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 3000, 'sublinear_tf': False} f1=0.224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 5000, 'sublinear_tf': False} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': True} f1=0.244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 6000, 'sublinear_tf': False} f1=0.248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': True} f1=0.246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 7000, 'sublinear_tf': False} f1=0.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': True} f1=0.254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 8000, 'sublinear_tf': False} f1=0.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 9000, 'sublinear_tf': False} f1=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': True} f1=0.253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.6, 'max_features': 10000, 'sublinear_tf': False} f1=0.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False} f1=0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': True} f1=0.198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 1000, 'sublinear_tf': False} f1=0.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': True} f1=0.222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 5000, 'sublinear_tf': False} f1=0.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 6000, 'sublinear_tf': False} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': True} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 7000, 'sublinear_tf': False} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': True} f1=0.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 8000, 'sublinear_tf': False} f1=0.259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 9000, 'sublinear_tf': False} f1=0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': 10000, 'sublinear_tf': False} f1=0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': None, 'sublinear_tf': False} f1=0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': True} f1=0.198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 1000, 'sublinear_tf': False} f1=0.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': True} f1=0.222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 5000, 'sublinear_tf': False} f1=0.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 6000, 'sublinear_tf': False} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': True} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 7000, 'sublinear_tf': False} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': True} f1=0.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 8000, 'sublinear_tf': False} f1=0.259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 9000, 'sublinear_tf': False} f1=0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.8, 'max_features': 10000, 'sublinear_tf': False} f1=0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': None, 'sublinear_tf': False} f1=0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': True} f1=0.198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 1000, 'sublinear_tf': False} f1=0.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': True} f1=0.222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 3000, 'sublinear_tf': False} f1=0.223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 5000, 'sublinear_tf': False} f1=0.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': True} f1=0.238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 6000, 'sublinear_tf': False} f1=0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': True} f1=0.249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 7000, 'sublinear_tf': False} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': True} f1=0.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 8000, 'sublinear_tf': False} f1=0.259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': True} f1=0.255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 9000, 'sublinear_tf': False} f1=0.260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': True} f1=0.255\n",
            "  Evaluating params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.9, 'max_features': 10000, 'sublinear_tf': False} f1=0.263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{best_params=}\")\n",
        "print(f\"{best_f1=:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e35279-084e-4eee-8fa6-c35a48f69c78",
        "id": "QphZFkL9xvP6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_params={'C': 100, 'class_weight': 'balanced', 'max_df': 0.7, 'max_features': None, 'sublinear_tf': False}\n",
            "best_f1=0.263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a728a08-adc8-40f6-c362-1fab837363af",
        "id": "Tgo84_wn7zRk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf_train_best.shape=(3287, 9949)\n",
            "tfidf_val_best.shape=(1096, 9949)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=False, max_features=None, max_df=0.7)\n",
        "tfidf_train_best = vectorizer.fit_transform(texts_train)\n",
        "tfidf_val_best = vectorizer.transform(texts_val)\n",
        "\n",
        "print(f\"{tfidf_train_best.shape=}\")\n",
        "print(f\"{tfidf_val_best.shape=}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_clf_tfidf_best = LogisticRegression(C=10, class_weight='balanced')\n",
        "lr_clf_tfidf_best.fit(tfidf_train_best, labels_train)\n",
        "labels_predicted_best_val = lr_clf_tfidf_best.predict(tfidf_val_best)\n",
        "labels_predicted_best_train = lr_clf_tfidf_best.predict(tfidf_train_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9de109b-ffce-41a0-d837-a9baefa96b49",
        "id": "FHRyjcQt7uz1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy_bestt = accuracy_score(labels_train, labels_predicted_best_train)\n",
        "accuracy_bestv = accuracy_score(labels_val, labels_predicted_best_val)\n",
        "\n",
        "precision_bestt = precision_score(labels_train, labels_predicted_best_train, average=\"macro\")\n",
        "precision_bestv = precision_score(labels_val, labels_predicted_best_val, average=\"macro\")\n",
        "\n",
        "recall_bestt = recall_score(labels_train, labels_predicted_best_train, average=\"macro\")\n",
        "recall_bestv = recall_score(labels_val, labels_predicted_best_val, average=\"macro\")\n",
        "\n",
        "f1_bestt = f1_score(labels_train, labels_predicted_best_train, average=\"macro\")\n",
        "f1_bestv = f1_score(labels_val, labels_predicted_best_val, average=\"macro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSPeWvPeQcch",
        "outputId": "4e0e85a5-8bec-4e60-dc2f-5b5aaca2710a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation and Analysis"
      ],
      "metadata": {
        "id": "2lEFx1uKR0FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from decimal import Decimal\n",
        "# due to the disconnection of colab, variables are lost\n",
        "lr2_train = np.array([0.648, 0.493, 0.306, 0.299])\n",
        "lr2_val = np.array([0.341, 0.123, 0.140, 0.123])\n",
        "lr2_tuned_train = np.array([Decimal(item).quantize(Decimal('0.000')) for item in[accuracy_bestt, precision_bestt, recall_bestt,f1_bestt]])\n",
        "lr2_tuned_val = np.array([Decimal(item).quantize(Decimal('0.000')) for item in [accuracy_bestv, precision_bestv, recall_bestv,f1_bestv]])\n",
        "scores = np.array([lr2_train, lr2_tuned_train, lr2_val,  lr2_tuned_val])\n",
        "evaluations = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "types2 = [\n",
        "      \"LRC2 TRAIN\",\n",
        "      \"Tuned_LRC2 TRAIN\",\n",
        "      \"LRC2 VALIDATION\",\n",
        "      \"Tuned_LRC2 VALIDATION\",\n",
        "      ]\n",
        "\n",
        "dataframe2 = pd.DataFrame(scores, columns=evaluations, index=types2)\n",
        "print(dataframe2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797cd335-c161-425b-df88-003f3638fbfc",
        "id": "3Ar8jJHW7jqL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      accuracy precision recall     f1\n",
            "LRC2 TRAIN               0.648     0.493  0.306  0.299\n",
            "Tuned_LRC2 TRAIN         0.959     0.964  0.982  0.973\n",
            "LRC2 VALIDATION          0.341     0.123   0.14  0.123\n",
            "Tuned_LRC2 VALIDATION    0.342     0.323  0.229  0.250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for the result, it can be seen that after tuning, using these best conbination of parameters, the performance got the best result on training set, also an improvement is on validation set, especailly precsision increased nearly 30% which is obvious compared to the 10% increase of other three measurement scores. But the result illustrates that the problem of overfitting still exist."
      ],
      "metadata": {
        "id": "pctgue51QMTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best parameters for vectorization keep consistant with default ones, so it can make a guess that the improvement are all related to the process of doing LogisticRegression. "
      ],
      "metadata": {
        "id": "6BcKz2TDWWzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eRCKtU83WGMU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VEttd095Bsk"
      },
      "source": [
        "## **Part 5 Context Vectors using BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding text && LogisticRegression Classification"
      ],
      "metadata": {
        "id": "xDT6tI1ApxX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC6XY661xRip",
        "outputId": "54ca89d9-89a6-4731-cbb3-7bb087a9abf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys \n",
        "from transformers import pipeline\n",
        "from transformers import RobertaTokenizer, TFRobertaModel"
      ],
      "metadata": {
        "id": "Ip7xe6aaxSVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_features = pipeline('feature-extraction', model=\"roberta-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "70b0513a376f412ca5f4553b09e1ee43",
            "4fcfb90046284121b6e3159176651e3c",
            "eb0f8786efc9481199e204014e335c7b",
            "7eab3fa83cae47dcaf08bbe2cc9082cd",
            "cc4c73c6c1f04134a0f92c2c7d721c89",
            "b45520dfb5f24d0abb6ecc0b3d25a2cd",
            "f6f64a46d9524dc39b8280b3b66c2075",
            "4486f5192669409f8acadd997a769328",
            "84bb2ad8785a40919edf94bccc72cd39",
            "fe82996bd9d84288a9c15a234480a628",
            "0299f19ef3414c5b9eda6d107995b048",
            "03aeefeb18614c61a80f6d4c15075490",
            "2da34a3298f54e96b9ff27ee9c2d813d",
            "d49f046b79524561ba089e668cb0e07f",
            "dce995808c674d62bdee514deee10572",
            "7e989da963ab48a1812c4b4c5d643a74",
            "684c1761ab604349b3647df639082987",
            "fd3fc3c33eea486dbba47ac3cc13be4f",
            "c4b968ef8f1c458f865b75e2aa300a17",
            "17688e0369c348a9bb639a302c1623e1",
            "0cd5b852d6494bd183d30202ab0a318a",
            "cf34b219123e4691827b403a217ef892",
            "ed63770749a949b3b4123783fbe1e5d7",
            "9fe2fe4174c34154967f49b2c7a2397e",
            "31e2ab94fceb4099aa092454884068e9",
            "c60fd96d5f864300be9f0a16b8bb29a8",
            "67ef8a12ff12465db77c4ff4e6e1ef30",
            "d026a591d1a0433cab335aad8dd90fa0",
            "971c8f7f758c405e8e7eeef504faf3f3",
            "a1c360234c174cca9e245b73c0ff07f1",
            "315555863cfe4db4aff639399aff731e",
            "0b43662ab5b049abaa823ff038dc7fbe",
            "39002aa039884397a3e00bc31e2a1afe",
            "b4842d1ab7524311829d667fd2cf6044",
            "fa19af44f90c47b2b1f0fe54983d4eb9",
            "e9375c3467324168805aa2640363119e",
            "94fcbe56075f4004a970e141364b5054",
            "61cb4777eed148f8b55f3fc7372ae527",
            "32cd7cede3a547d6914eba30e0e01286",
            "a91c7bbfb9dd48d09c361dd747f0906e",
            "846036bbbf694bb1a0aa37bc844f45e4",
            "887f862a9b0c4c56ba739f1b120ed1ef",
            "cb0271a3ace24313800be28e235e4450",
            "0fedb96a8bdc4822aa247090b6a71e00",
            "a792a857208a443db18a98362ce7cd10",
            "75b22fcdda06470fafc1f588c2ff289c",
            "7ebc5d18840a4adf817e1863f2985c5d",
            "6f68c304c9f44310b0392c4c39a3266e",
            "00a4ea835e4e459584b830c798c655a6",
            "210fb92f4f8e4043868ef067630a5b54",
            "fb0852227002409ba7203a61cb620b3c",
            "0d28bcbfc3664d98aa7479543c17e843",
            "7ef64c51d16542528269426f66d22de6",
            "9997d321bc6c4c188a951d6382297063",
            "c082edceb1dd4cedb1f67c214c6cc173"
          ]
        },
        "id": "4OYyMlNS29Zk",
        "outputId": "e56ed77d-105f-4cdb-8590-1f7b26b04543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70b0513a376f412ca5f4553b09e1ee43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03aeefeb18614c61a80f6d4c15075490"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed63770749a949b3b4123783fbe1e5d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4842d1ab7524311829d667fd2cf6044"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a792a857208a443db18a98362ce7cd10"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0A-zGV7CIcm",
        "outputId": "910515c2-4d7d-4b1a-d0e8-2470a86c9bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Are you aware of the pride there is in what you're saying?\\nI have nothing to reproach myself for. All I know is that I've changed. With all my strength, which is not much, I will follow the road that the Lord has shown me. One can also serve outside a convent.\\nIs there some grave impediment which prevents you from taking your vows? There must be something.\\n\", 'Oh, madame! This is a restaurant, not a meadow.\\nRaw carrots and beets.\\nI think this is the first time you have been to my little place. Your face is new to me. Now, what shall it be?\\nThis will do.\\nThis way, madame. Are you alone? By the window perhaps?  Or a nice little corner table?\\n', 'Look! Look! The front page!\\nMother!\\nAh-ah-ah!\\n', \"No one leaves a star.  That makes one a star.\\nGoodbye.  Norma.\\nI'm the greatest star of them all.\\nNorma, grow up.  You're a woman of fifty.  There's nothing tragic about being fifty - not unless you try to be twenty-five.\\nYou heard him.  I'm a star!\\n\", \"Let's say today, we make it your business.\\n'Course, it's none of my business.\\nWhat are you talking about?\\nMaybe you don't want to remember.\\n\", \"Mister Worf... find Data if you can, but your top priority is isolating the Borg.\\nWil... Data was down there.\\nThe Borg.  Some of them must've beamed over before we destroyed their ship.  Seal off that entire deck with emergency force fields.\\nSomeone...?\\n\", 'Any time you decide to let me in on it.\\nYou missed.\\n', \"Quiet, Max.  What do you think, this is a conversation?\\nMy god ... it's true.  The Penguin- Man of the sewers ... Please, don't h--\\nActually this is all just a bad dream.  You're home in bed. Heavily sedated, resting comfortably, and dying from the carcinogens you've personally spewed in a lifetime of profiteering. Tragic irony or poetic justice? You tell me.\\n\", \"We've looked at half of one tape.\\nI think we get the gist.\\n\", \"Blow me.  Yellow means go, Kimmy.\\nDano, shouldn't we stop and help your mother?\\n\", 'Andy...\\nOh God. Shit...\\n', \"I'm here.  I'm here.\\nYOU HAVE TO!\\nI am.  I'm here.\\nYOU CAN'T DO THAT!  YOU CAN'T FREAK OUT LIKE THAT!  YOU HAVE TO STAY HERE WITH ME!\\n\", \"Enterprise, I want that signal again. Transmit!\\nAnd plug the signal directly into the circuitry at this...  ... point here, Captain. It should pick up even a tricorder playback.\\nCapture God! In order to retrieve V'ger's data, the Creator has to physically come here!...\\nThe fault is here, sir, not in the transmission.  The antenna leads have been disconnected. V'ger never intended to accept a transmitted signal.\\n\", \"No, a <u>pact</u>.  No money involved.  This is more important than any bet.  Now here's the deal: We all get laid before we graduate.\\nLike a bet?\\n\", \"I don't know, look around.\\nCan't get to it.  Have to pull this panel off. You go any tools?\\n\", \"I had exactly twenty thousand lire. I had just cashed a traveler's check and put it in my wallet.\\nThe representative of the police wants to know how much money you had, M'sieu Filiba.\\n\", \"Margo. Margo's great. She knows it. That's the trouble. She can play Peck's Bad Boy all she wants, and who's to stop her? Who's to give her that boot in the rear she needs and deserves?\\nMargo hasn't done badly by it.\\n- everything a playwright first thinks of wanting to write about... until his play becomes a vehicle for Miss Channing...\\n- don't run out of adjectives, dear.\\nOf course she knew! For one thing, Addison told her how superbly Eve had read the part-!  Karen, let me tell you about Eve. She's got everything - a born actress. Sensitive, understanding, young, exciting, vibrant-\\nIt's just possible she didn't...\\nThen a childish, heavy-handed routine about not knowing Eve was her understudy-\\nThat's on time for Margo.\\nAnd again and again! Two hours late for the audition, to begin with-\\nMargo, again...\\nUp to here! That's where I've got it - up to here! Of all the star ridden, presumptuous, hysterical-\\nLloyd, what happened...?\\n\", \"Goddamn it. I want Louis.\\nI don't have to ask if you can feel that.\\n\", 'The head shrinker at the prison.\\nWho is he?\\n', \"Where are we going?\\nI think so. I think that's one of the clues. It's a clue that tells us... to keep going.\\nGod exists because you're horny.\\nBecause I'm so horny.\\nWhy?\\nI think now that he might...\\nIf God exists?\\nI have to obey him... because he saved my life. He controls me and I have to obey him or I'll be left all alone... and I'll never figure out what all of this means...\\n\", \"I got a new deal for you.\\nAll right, so I made another deal.\\nDon't lie to me.\\nI was called away on urgent business, Snake.\\n\", \"C'est la vie... We're lucky in lots of ways, but... Surely it's worth a brownie.\\nNo.  Not true...\\n\", 'To complete my cure.\\nWhere are you going?\\n', \"Mac.  What's shaking?\\nOfficer Taylor, how are you this evening?\\n\", 'Quite a compliment coming from you, doctor.\\nMr. Kelson, what a pleasure to meet you. I have to say your instincts about the criminally insane are impeccable.  I am a fan.\\n', \"August! Tomorrow is the 4th of July, and we are going to open for business. It's going to be our best summer in years. If you're so concerned about the beaches, you two, you do whatever you have to to keep them safe, but with you or without you, the beaches stay open this weekend.\\nLarry, we can re-open the beaches in August.\\nYou'd love to prove that. Getting your name in the National Geographic.\\n\", \"For the record, whenever you hear me sliding out of control, I'm never alright.  When I secure the line, come on up.\\nGabe are you alright?\\n\", 'Tell me later.\\nI love you, too.\\n', 'Sure.\\nThe question is not whether I am paranoid, but whether I am paranoid enough.  You want to rub my neck?\\n', \"Make them talk and you got Williams, Pinky!\\nCome on, Pinky! Give 'em a little third degree.\\n\", 'His wife changed the channel.\\nWhat?\\nHe was watching a ball game on television.\\nWhy did he shoot her?\\n', \"Yeah, well sure... you can if you want to... it's just I don't want all these guys in there at once... you know...\\nReally? Can I see it?\\nOh God no. This is just junk I have for sale or trade. The record room is off-limits.\\nYou're right about that.  So this is your record collection?\\nI... I didn't think you would have any interest in this get together... I mean if you had told me you were coming I would have warned you -- it's not like a real party or anything.\\nWhat was all that stuff about enlarged holes and tight cracks?\\n\", \"You're going to miss Mexican Halloween. The Day of the Dead.\\nI'll call you from Detroit, baby.\\n\", \"Where are you from, Captain?\\nSome.\\nYou know classical music?\\nIt seemed appropriate.\\nI heard.\\nI've been humming it.\\nYes.  Do you know Sibelius' Fourth Symphony, The Normandy?\\nIt looks like a Renoir.\\n\", \"Library of Congress.\\nWhere?\\nHave him meet me with pictures of Lecktor's books...\\nWillingham, when he tossed his cell, took Polaroids so they could get everything back in place...\\n\", 'A big one....\\nA big one....\\nA drink with an umbrella in it....\\n', \"...I had a great time tonight, really.  It was like the Nuremberg Trials.\\nGod!\\nRight.  Yeah.  I had a...\\nYou know, it's probably my fault. I've been a little depressed lately.\\nYeah.  Me, too.\\nOh, look, I'm sorry it didn't work out.\\n\", \"You're not as quick as I thought. I figgered out that rats breed faster than cats-- cat skins make good fur-- The cats eat rats. Rats eat raw meat. That is, they eat the carcasses of the cats. So --the cats eat the rats--the rats eat the cats. And I get the skins--simple ain't it?\\nBut where do the rats come in?\\nThat's it. You get it quick.\\nOh, I see. Cats' fur.\\nOh, no. It's my own idea. I'm in the business. In the fur business.\\nWhat' s the idea [of wanting so many] do you sell them [to the doctor]?\\n\", \"Three.\\nI don't believe this.\\nIn just a second. Two...\\nWhat, you gonna shoot me now, Bo?\\nHow 'bout I give you to three, then I organize your fuckin' brains all over the wall back there. One...\\n\", \"This rabbit'll do anything not to do time, including wearing a wire.\\nHe's so spooked he'd turn over his momma, his daddy, his two-panny granny, and Anna and the King of Siam if he had anything on him.\\n\", \"All right.  How about I feel better when you take them?\\nI know, I know.  But still, somehow I don't feel better.\\nYou're better when you take them.\\n\", \"No man. Gonna walk.\\nOkay man I ain't going to push. But remember the revolution isn't going to wait for anyone. Come on, we'll give you a lift.\\nYeah... let me think about it.\\nYou are down for protecting Malcolm's widow aren't you?\\nI don't know.\\nJudge... we're doing security for Betty Shabazz's visit next week. I'd like to have someone who knows there way around a pistol there. Someone like you.\\n\", \"Exactly. Happy birthday.\\nBut I'm allergic to Retlax.\\nFor your eyes. For most patients of your age, I generally administer Retlax Five to restore flexibility of the lens.\\nUh -- what are they?\\nFour hundred years old. You don't find many with the lens still intact.\\nCheers. Bones, these are... charming.\\n\", \"I don't think it's that serious. You want to sit up here?\\nShall I call the doctor?\\nYoung woman cut herself.\\nWhat happened?\\nDeke, have you got a first aid kit back there?\\n\", 'Uh Brock, today you are without me...\\nExactly. I don\\'t know what I\\'d do without you.\\nAt some sanctimonious celebration of condescension. Nothing like appeasing half the population with a two hour luncheon.\\nFine. You\\'re angry. Don\\'t be. The important thing is we\\'re together now..\\nI got home fine. How\\'s the \"warehouse.\"\\nSelina, did you make it home, all right?  I tried calling, but your mother said that there was \"no extension in the Hut.\" Whatever that means..\\n', \"Bullshit.  We've played with each other, pushed each other.  This is different. Like you want to prove that you're better than me.  Who's that for -- Evelyn?\\nWe've always tried to beat each other.\\nNo.  You're trying to beat me.\\nWhat do you mean?  I'm just doing what we've always done.\\nDanny, what the hell are you trying to do out there?\\nFun today.  Like old times.\\n\", 'I need that report, Lt.\\nIt knows us. This one knows us.\\n', 'Well, yeah.\\nReading dirty comic bocks.  And you admitted it?\\nWell, a little.\\nYou were up late last night, Brother.\\n', \"Girls, I wanta apologize.  For my language back there.\\nI wish somebody would tell me I'm gonna live long enough for it to be a habit.  My parent, she'll be okay. My husband, he'll be okay.  I even know who the bum is gonna marry. Terrific.  She'll take good care of him.\\n\", \"Check your driver's license.\\nWhat's a concubine?\\nAbsolutely not.  I mean the wicked.  The heedless.  And their Las Vegas concubines.\\nYou mean black people?\\nAnd all those like you.\\nAre you talking about me?\\nI joined this convent some thirty years ago.  At that time, the world knew some measure of peace.  And hope.  Our order was a beacon of hospitality, to families, to children, to a neighborhood filled with promise.  And, as the years have passed, I have watched that promise destroyed.  Drugs.  Gangs.  Spandex.  And. so I have made this convent an oasis, a retreat from horror. And now that horror has invaded these sacred walls.\\nWhat?\\nIt has come to pass.\\n\", \"You look sick if you ask me. I hate to leave you with the dishes and all, but I'm supposed to go to a meeting of the Garden Club this evening.  I'll need the car keys.\\nNo, ma'am, I'm just fine.\\nRose, you're in an awful dither tonight. What's the matter with you, honey, are you sick or something?\\n\", 'No wonder he was looking at me like that.\\nTwenty-two years ago.\\nYour real father was an Officer candidate like me?\\n', \"I'll prove it. We'll repeat their procedures...put a man in our POD, bring it down, and then...inject him.\\nImpossible.\\n\", \"She only worked here a month. Nice girl. Never seemed to get here on time though. Ask me she had a little problem with --\\nFederal Bureau of Investigation, Special Agent Chet Desmond. I'd like to ask you a few questions about Teresa Banks. Jack said you knew her. How well?\\n\", \"At least let me maintain some semblance of managerial control here.\\nHe's blunt, but he's got a point.\\n\", \"Come on! We'll burn the hangar. That will make light for them!\\nThe power house - they've blown it up! The planes can't land without lights.\\n\", 'Yes. \\'Intimate.\\' They had privacy.  Everybody else was dead.\\n\\'Intimate? !\"\\nMrs. Leeds was a good-looking woman. I\\'d want to touch her skin in an intimate situation, wouldn\\'t you?\\nWhy do you think he took his gloves off?\\nThe report didn\\'t mention nails and eyes.\\nOur people swear he wore surgeons\\' gloves the whole time. They dusted everything.\\n', \"If we don't strike soon, there may not be much of an America left to defend.\\nAbove American soil?\\nI spoke with the Joint Chief when they arrived at NORAD. They agree, we must launch a counter offensive with a full nuclear strike. Hit 'em with everything we've got.\\n\", \"I know what people say, Bill. I... Remember, back in high school I worked at my father's gas station?\\nWas always curious why you... married Grant in the first place... Just never seemed outta love.\\n\", \"Well.  It's for you.\\nWow.  Who's that for?\\n\", 'Got it.\\nGive me 30 seconds, then flip every switch you can reach.\\n', 'You don\\'t have to tell me. It\\'s written all over that pretty face of yours.. You came here to be an actress. I just hope you\\'ll remember there\\'s never been a great poem called \"tits and ass.\"\\nWell, I ...\\nDamn lot of corn raised in Hollywood these days too.\\nYes, he is. He raises corn.\\nI guess it was your grandfather, was it ... he called me to check in, said you were on your way and for you to call when you get in. Nice man... farmer I hear.\\n', \"In a minute, look it's hot, sweetie you'll burn your nose, look out.\\nWhen??\\n\", 'You got that right. Low became obsessed with the Green River murders, the case had been inactive for ten years at that point. He argued the Green River Killer had actually become Suspect Zero, this master murderer who killed without pattern, killed literally hundreds of victims -- male, female, old, young, straight, gay -- and who was still killing, even though there were no bodies. It went against everything we knew. Low became increasingly paranoid. Every suspect was potentially Suspect Zero. Anybody tried to talk sense into him, he\\'d accuse them of being out to get him. Deputy Director Koessler was \"out to get him.\" The decision was made to relieve him.\\nPacific Northwest is a hotbed for serials.\\n...well, there was some friction: I wanted to write up my work, educate the public, but Koessler wouldn\\'t allow it. Low felt Koessler was more interested in career advancement than catching killers. Koessler had Low reassigned to the Pacific Northwest, Seattle. You know when they say, stick it where the sun don\\'t shine? That\\'s where they stuck Dick Low.\\n', \"Well .  .  .  I moved back in. My lawyer said it would give me a better claim on the house in the property settlement.  Don't change the subject... you owe me two hundred bucks.\\nI thought you moved out.\\nTwenty here, fifty there... I figured my wife's boyfriend was taking it.\\n\", \"I don't think she willed it to me out of sentiment.  She didn't even know who I was.  It's just... tradition.\\nGod rest her soul.\\n\", \"It ain't nobody. It's a company.\\nWho's the Shawnee Land and Cattle Comp'ny?\\n\", \"Why don't we all turn in?  It's been a long day.\\nI'm getting tired.\\n\", 'One night I will get the perfect photograph.\\nI think so too.\\n', \"Thanks, Chuck.  I need the money.  Can I still work the hours around my classes?\\nThe pay's the same but you'll make more in tips.\\nI don't know.  I kinda had my heart set on being a cashier the rest of my life.\\nNot really.  Do you think you can get here on time if I put you on the floor as a waitress?\\nAren't I worth waiting for?\\n\", \"You see a Ricky Slade?\\nYeah. Ricky Slade.\\nHow is it you're good? You on a list?\\nYeah, but, we're good. You know what I mean?\\nThe line's over there.\\nWatch out, man. Sorry. I'm on the list, man.  Hey, bro.\\n\", \"We're all very proud of you, Harry, but what the hell is it?\\nIt's a single sequence repeated over and over.\\n\", \"The land and the King are one, my son. If he stutters we falter. He's getting batter, and so are we.\\nYou said that last year, Dad.\\n\", 'Henry, no!\\nYou ruined me! You destroyed me!\\n', \"Yes. I have seen styles on television.\\nIt is quite amazing how those clothes have come back into style.\\nI am allowed to go to the attic and select any of the Old Man's suits. They all fit me very well. I can also take his shirts, shoes and coats.\\n\", \"Let's explore this. What makes you feel you're a werewolf?\\nI think I'm a werewolf.\\n\", 'You are very pretty. I would like to kiss you.\\nBonjour Monsieur Philipe.\\n', \"Hell is precisely what is going on, Joey.  And we have to stop it.  I because of a special obligation, you because you're the only person who can help. And because you know what is right, and just, and true.  Will you walk with me a while?\\nCaptain Spenser.  Elliott.  I ... What the Hell is going on?\\nWell done.  Brave girl.  You've probably never shaken hands with a ghost before, am I right?\\n\", \"Don't crowd me, Craig.\\nWhy?\\nI have to go back tonight. At eight Exactly.\\nHow was it?\\n\", \"I don't think so.\\nThat you come with me.\\nWhat do you suggest?\\nDon't go home.  And don't go to work.  Either one could be bad.\\nI'm going to find him.  Because he'd find me.\\nNo idea.  Honest.  What are you going to do?\\nWhere do you think Jerry is?\\nJonas builds assassins for a living.  Several of whom may be in place already.  We'd like to kill a few birds with one stone.\\nHe's shown himself.  Why haven't you arrested him or killed him or done whatever it is you do?\\nHe's why I watch Jerry.  Jerry's the bait for Jonas.\\nAnd Jonas?\\nI'm, it really doesn't matter. Think C.I.A. and exponentiate. I'm a government employee and I've been watching Jerry for awhile.\\n\", \"Will you shut up already?\\nWe're going to die.\\n\", \"No.\\nI didn't think it was going to be this big, did you?\\nYeah.\\nIt's so pretty.\\n\", \"You'll be able to throw it further than I could.\\nYOU do it!\\nSh! Hurry!\\n\", 'Oh yeah.\\nSmokes?\\n', \"She seemed to leave happy. I thought I'd helped her.\\nThank you.\\nI believe that Sammy should be physically capable of making new memories.\\n\", 'You better.\\nI will see it next time.\\n', 'That depends on the man.\\nI still say this is no test. A catapult can throw a stone farther than a man can.\\n', 'We should have a turbo. I\\'m always saying \"activate turbo boosters\", right?...\\nThey\\'re still behind us...\\n', \"It was my master Keitel's idea.\\nBut --\\n\", \"Annabelle, how's your video rep...\\nWhat is this?  The Betty Ford Center?\\nWe don't have any coffee.\\nCan I please have a cup of coffee?\\nEvery Tuesday except the 3rd Tuesday of the month when it's switched to Friday except in April when she rides on Thursday.  It's not that hard.  Didn't you have a mother?\\nThis is Friday, her riding lesson is on Tuesdays.  I got it right here...\\nHow do you hold down a job?  It's 8:10. You were supposed to be here at 7:00. She's missed her sunrise Groom'n Ride.\\n\", \"Thanks.\\nBe careful.\\nYes, Basil?\\nI'm leaving!  Oh, and Austin?\\nI think if everyone were honest, they'd confess that the lady looks exactly like a man in drag.\\nAll right, Austin, I think you should go.\\n\", 'Yes.  Isn\\'t it there?\\nI\\'m overworked as it is.  \"Raban\" did you say?\\n', \"I'm not sick, Richard. I'm just not tired, now go back to sleep before you're up for the whole night too!\\nAgain? Maybe you should see a doctor.\\nI can't sleep.\\nNow?!  It's after eleven.\\nI'm not tired. I thought I might finish Carolyn's skirt.\\nWhere you going?\\nLater. Go back to sleep.\\nWhat time is it?\\n\", \"OK, we sit tight until the old man can give us the lead.  But Tom, I want you to stay inside the Mall. You too, Mike, no chances.  Tessio, you hold your people in reserve, but have them nosing around the city.  The hospital is yours; I want it tight, fool-proof, 24 hours a day.\\nWe're all tired...\\nOh Christ Tom, I didn't mean it that way.\\nI was as good a son to him as you or Mike.\\nThat's easy to say; it's not your father.\\nWithout your father's political contacts and personal influence, the Corleone family loses half its strength.  Without your father, the other New York families might wind up supporting Sollozzo, and the Tattaglias just to make sure there isn't a long destructive war.  The old days are over, this is 1946; nobody wants bloodshed anymore.  If your father dies...make the deal, Sonny.\\nTom, you're the Consigliere, what do we do if the old man dies?\\n\", 'Look unimpressed.\\nMy goodness gracious!  This place is something!\\n', \"You see what's hanging on the wall?\\nGary, I --\\n\", \"Don't give up. You're the only one who can help me. I know it.\\n--What?\\n\", \"Hide us Uncle Birdie! He's a-comin' with his knife!\\nDon't!\\nUncle Birdie!\\n\", \"That feels good. Lower--down into my neck.\\nThere's explanations. Rational explanations for everything that's happened. We'll drive ourselves crazy if we keep obsessing on supernatural what-ifs.\\n\", \"Thank you.\\nOh, it's you!  You got a lot of nerve -\\nAnything today?\\n\", \"I just want to know while we were seeing each other... I just don't want girls looking at me and knowing and me not knowing...\\nWhat difference does it make?\\nI just want to know, that's all.\\nWhat do you wanna know for?\\nHow many?\\nObviously.\\n\", \"You need counseling.\\nBut this guy is like a one-of-kind, rare butterfly, and we have to follow him back to his natural habitat...\\nNo.\\nCome on, Josh... don't you want to see where he lives?\\nForget it.\\n\", \"Ok. Linda:  Earl is not gonna make it. He's dying. He is.  He is dying very, very rapidly --\\nI need to sit down.\\nDo you wanna sit down?\\nI'm listening.  I'm getting better.\\nWe can fix that, because I can give you -- are you listening?\\n-- you don't understand: it's more pain than before and the fucking morphine pills aren't working, he's -- past two days it's like he can't really swallow them and I don't know if they're going down -- I can't see inside his mouth anymore -- I'm up all night staring at him and I don't think the pills are going down and he moans and he hurts --\\nIf you're happy with Phil taking care of him and helping you, that's fine, but contact Hospice to arrange for the body --\\nYeah.\\nPhil's one of the nurses from the service?\\nHe has Phil right now.\\nWell that's what Hospice will take care of for you.  They will send a nurse, someone who can take care of all of that for you --\\nI just, I just -- I just -- I'm just in a fucking state, I know he's going and it's like I don't know how -- just tell me <u>practical</u> things -- What the fuck do I do with his body? What happens when he dies? That next moment: What? What do I do?  Then What?\\nI can help you through this the best I know how but there are certain things you are gonna have to be strong about and take care of, now we can go over them, but I need to know that you're listening to me, ok?\\n-- he's fucking dying, he's dying as we're sitting here and there isn't a fucking thing -- jesus, how can you tell me to calm down?\\n\", 'Goodbye, David.\\nThe Genesis planet is gone.\\n', \"Just trying to get to my boat...\\nYou trying to take over my show Finnegan, that what you trying to do?\\nThat's where my boat's moored.\\nWho gives a shit about aft?\\n\", \"Birds are not aggressive creatures, Miss. They bring beauty to the world. It is mankind, rather, who...\\nI just came from the school, madam. I don't know about their brain pans but...\\n\", \"... and while they are cooped up in your fort, what if the French send war parties to raid their homes?\\nWill you men help us stop the French?\\nFirst place, you started it with the French over fur-trapping claims to the head waters of the Ohio.  Now you're sayin' these people have a fight on their hands...\\nReally? Do you want them to overrun all New York colony?\\nI said... France is your enemy. Not ours.\\nWhat did you say?\\n\", 'No!\\nHis name is Robert Paulson!\\n', 'What?\\nToo much sun.\\n', \"Don't you understand? The mere act of sending matter back in time would change the course of events, and changing history is a responsibility that I do not wish to bear.\\nSo what? We'd be rich!\\nMarty, that would alter history.\\n\", \"Well, I forgive you this time. But I'll be in tomorrow night with a breathtaking blonde, and it will make me very happy if she loses. Uh huh!\\nPut it down as a gesture to love.\\nWhy do you interfere with my little romances?\\nYeah? Why?\\nAs I suspected, you're a rank sentimentalist.\\n\", \"You should work for yourself.\\nDo you ever look at those girls who work at Swenson's? They're beautiful. And I have to stand out here and watch them six nights a week.\\n\", \"Why you're alive?  You're alive because you're special. Because she kept you alive.  Because we want you back on our side.\\nYou didn't answer my question.\\nSounds like a threat.\\nDon't need it. I remember everything.\\n\", \"Oh-h, I'm sorry. I'm sorry, Mr. Hillyer... I just... couldn't help myself. I'm sorry...\\nGoddamn you, girl! You've made me make a fool out of myself, damn your hide, but let me tell you I am standing at the pass of Thermopylae and I won't budge! The very idea, my own home with children in the house, to say nothing of my wife -- oh-h, you had better believe I am standing at Thermopylae, you little nut, you had better believe it! What are you, crazy? A man is supposed to be a fool like this, but a woman is supposed to have some control and sense! Are you a nitwit? What's the matter with you?\\n\", \"Chuck... look.\\nI'm gonna be sicker than all of you, man.  Now I gotta spend the whole weekend totally straight...  I don't think I can make it, man!\\n\", \"Valentine wasn't even there.  If he was into something, if she was involved -- who can say.  But I'll tell you something.  She stood in front of these dudes, man.  Eyeballing <u>them</u>.  Checking <u>them</u> out.  I felt like she was covering <u>my</u> ass that day.\\nDid Jenny know?\\nBad place, man.  Bad people.  Some guys loading some trucks.  Some kinda deal goin' down.  I don't know and I don't care.  Maybe they're shipping fava beans to Eskimos.\\n\", \"That's a little vague, Spock -\\nI am here, Captain.\\nSpock! My only concern is getting the ship back. When that's done and Sybok is in here then you can debate Sha Ka Ree until you're green in the face. Until then, you're either with me or you're not.\\nSybok possessed the keenest intellect I have ever known.\\nBut the center of the galaxy can't be reached. No ship has ever gone into the Great Barrier. No probe has ever returned.\\nThere Sha Ka Ree is fabled to exist.\\n\", \"What's the matter? Are you afraid to try?\\nOnly the dead reach Asgaard, Erik.\\n\", \"Don't! You're making me lose my grip.\\nI can't do it!\\n\", \"Try lookin' at it from this angle\\nC'mon.  It's not that bad\\nI guess I never told you I'm afraid of heights.\\n\", \"Tom... she's dead.\\nShe's all I've got.  She's the only witness.\\nWhat?\\nMrs. Christian.\\nWho are you calling?\\n\", \"Hope you enjoy it.\\nOh, it's wonderful.\\nAw, forget it.\\nMr. Gower . . . Mr. Gower . . . thanks ever so much for the bag. It's just exactly what I wanted.\\n\", \"They like my stories. They like the way I think. They're into fantasy. I turn them on.\\nI always meant to ask, what is it that makes you so special? Why is it you have this special rapport with multiple killers? Why you?\\nWhat I do requires confidentiality.\\nI'll take it into consideration.\\n\", \"It's my subway defense system.\\nShit!!\\n\", \"How about that guy at the bank?\\nJust moved in with his girlfriend.\\nHow about the other one?  His friend.\\nNo.  He smokes.\\nWhat about that guy you work with, Mike what's-his-name?\\n\", 'Have a nice day  sir\\nUh, not today Jenkins, I need the fresh air\\nWill you be wanting your wheels today sir?\\n', \"Christ, you know it's crazy...  I lived through so many other people's nightmares, you know. Always cool and calm, but... but I never thought I'd be the one needing help, ya know?\\nHer part in this I can't figure... but I will.\\n\", \"Thanks.\\nNaw, it's all fixed.  I also loaded up a program that'll analyze your games three hundred percent faster.\\n\", \"Stop calling me 'sir! I ain't no officer. My name is Byron.\\nThank you, sir!\\nOkay, okay. You win.\\nWhat for?\\nCome back here, kid!\\n\", 'Happy New Year.\\nHappy New Year, darling.\\n', \"Sit down.  You'll be a billionaire.  Better than being broke.  I've got terrorists and other pharmaceutical companies standing in line.  Ball's in your court, Mr. McCloy.\\nOutrageous.  I won't let you take control of my company.\\n\", \"Russian women prisoners?\\nHey, Schulz -- as long as you're going to move somebody in -- how about a couple of those Russian broads?\\n\", ') They are now.\\nHey! I thought these things were supposed to be extinct!\\n', 'Your witness.\\nNo -- never.\\nDid you ever see Mr. Marsh use cocaine?\\n', \"It's grand!\\nIsn't it beautiful?!!\\n\", 'This is what you get for brain- picking an old CIA spook.  but if I needed to control the outcome of this test case, that\\'s how I\\'d do it.  A man-in-place.  Makes everything very controllable.\\nI don\\'t...  What, someone on base?  A \"mole\"?\\nOkay, so now work it from the other end.  Think about California -- and how things might be handled there.\\nEven tough I don\\'t talk to her every day -- I still talk to her every day.  Know what I mean?\\nI take it this file is still open.\\nChrist, I don\\'t want to see her take a fall.  She thinks I do, but...\\nThis ain\\'t about some little soldier girl sloggin\\' her way through commando school.  The implications go way beyond.\\nI don\\'t know.  Seems...\\nIf you were the President, wouldn\\'t that put a little piss in your shoes?\\nWomen.\\nDon\\'t for a second think she didn\\'t leak this story.  \"G.I. Jane\" gives DeHaven a symbol that taps into the biggest constituency of them all.\\nThe first female President?\\nThe White House.  If Jordan wins, DeHaven wins in spades.  Why?  Well, it\\'s been said that the only man the President fears -- ain\\'t no man.\\n', 'Well, listen to this. He got mad at me one day. Boy, did he get mad at me. He took the light bulb, ya know. I thought he was going to cram it right up me, ya know. But he ate it. He ate the whole fucking light bulb.  Now, tell me! Is he crazy? Tell me! Is he crazy?\\nSure, I know Chuck. I know him.\\n', \"Thank you, Spiro. Shirley, what are you going to have?\\nBetween you and me, Madam, today the number two.\\nI can't make up my mind whether to have a number one or a number two. What do you recommend, Spiro?\\n\", 'Here we are.\\nOkay, I guess.\\nHow are you today?\\n', \"Nobody knows where he is.  We know he's all right, but that's all.\\nI've tried calling and writing.  I want to reach Michael.\\nKay, we weren't expecting you.  You should call...\\n\", \"Oh.  Good.\\nNo.  You don't do anything.  I do it.\\nOh.  I thought... you lead me --\\nNo -- you don't have to --\\nWho we gonna beat?\\nWho we gonna beat?\\n\", \"Don't talk down to me, Johnnie Farragut. I know what volition means, and that's why I want Sailor Ripley off the planet! He's pure slime and it's leakin' all over my baby. Maybe you could push him into makin' some kinda move and then kill him dead. You'd only be defendin' yourself, and with his record, nobody'd fuss.\\nHe served his time for what he did. Another thing... If Lula went with him of her own volition - willingly, that is - there ain't much can be done about it.\\nI knew this would happen. Soon as that piece of filth got out of Pee Dee, I knew there'd be trouble. He's just got some kind of influence over her I can't decipher. There's somethin' wild in Lula I don't know where it comes from. You gotta find 'em, Johnnie.\\n\", \"He's had some kind of...reaction to exposure from the cloud.  And he's not the only one.\\n<u>Ben</u> did this?\\nBen did this.\\nJust a little banged up.  A couple scrapes.  Why?\\n\", \"No. But does it make any difference?  How did you get to be the way you are?\\nIs that the way it was with you?\\nDid you ever live in a room with six people and you didn't have any money, any food, any furniture? Have your brother come out, his car break down, he can't get a job? Your friends stealing food, going through trash behind a supermarket?\\nOkay, tell me. Why do you live like you do?\\nYou don't understand shit.\\nOkay. Sucking off. Now does that make me as good as you?\\n'Sucking off.'\\nSay what?\\nYou never met a working girl before, have you? You think I like sucking off guys all night? Maybe I do. So what?  You can't even say it, can you?\\nYou know what I'm talking about.\\nWhat rate?\\nYou won't keep growing at this rate.\\nAt least I'm a growing person.\\nYou really shouldn't eat like that. All that sugar. It's not good for you.\\n\", 'Listen, Father! \"Young Lochinvar smitten with Susan Paine\"!\\nDo I actually *see* this--?\\nSuch pretty knees for a big boy!\\nHis first \\'whiff\\'!\\n', \"Asshole.\\nHuh?\\nWho do you like in the opening game, Sam?\\nTha's your problem.\\nDon't know if I can hold it that long, Sam.\\n\", '-- This is my lucky day.  I arrive in this big bad city and I not only find a doctor, a beautiful woman as well.\\nYes, I could.  I have an office in the hospital.\\n', \"Sure, sure. I'd like to believe in fairy tales, but a guy that's fake isn't gonna jump off any roof.\\nThat man is gonna be on that roof. Don't ask me how I know. I just know. And you know it as well as I do.\\n\", 'You heard voices, you know you did.\\nThere is a medical explanation for everything that happened.\\nAnd was he in distress when you opened the door?  Or was he sitting here, relaxed, at this table?\\nThe patient was in great distress. Naturally he was...yelling.\\n', \"I don't know, you're not supposed to see them. They're supposed to see you.\\nWhere are they?\\nThat's the building. She lives on the Seventh Floor. Don't stop to look long, the police are watching.\\n\", \"Hey...\\nWell, thanks for the lift.\\nWell, at least it hasn't been boring.\\n\", \"Trust me, if you open any door, you'll be dead a whole lot sooner.\\nWait for what ? i don't wanna alarm you but we'll be the special at the International House of Pancakes in about five minutes !\\nNO ! Don't open ANY of them ! They're all wrong. Just wait.\\n\", \"Congress and the Pentagon share a lot of plumbing.  They'll never know whose leak it is.\\nThink I overplayed it?\\n\", 'Hey, at least it\\'s not Goofy.\\nGood eye. Not one club owner got it. They all ask me why I got Donald Duck on my card.\\nYeah. Nice touch. It\\'s the logo from \"You Bet Your Life\", right?\\nWhy, you like the duck with the cigar?\\nNeither is acting. Not if you\\'re serious about it.  Can I have one of these?\\n...Yeah, but entertainment law isn\\'t something you just jump into...\\nDidn\\'t you tell me to be patient with my career?\\nWhen I lived in New York they made it sound like they were giving out sit-coms to stand-ups at the airport. I got off the plane in L.A. six months ago and all I got to show for it is a tan.\\nIt\\'s not going to well?\\nIf and when I get a real gig I\\'ll call you.\\nI\\'ll have to come see you sometime.\\nYeah. And an actor.\\nYou\\'re a comedian?\\n', \"Then let her get away.  I thought you were a pro -- you're supposed to be a fuckin' tracker!\\nShe was gonna get away.\\n\", \"Great.\\nSeriously.\\nSeriously?\\nDad I'm kidding, you can have the room.\\nOh yeah, right-between the power mower and the weed killer.\\nDon't do that, you've got the whole garage.\\nHey, why don't you just keep your room and we'll put the baby in the back yard.\\nMaybe I should just move to China. One kid per family, that way you don't lose your room.\\n\", \"God, I want to do everything!  I'm going to explode.\\nIf you have built castles in the air, your work need not be lost.  That is where they should be.  Now put foundations under them.\\n\", 'You have a butler working that floor?\\nRight now? A six man security force, plus a member of our Butler staff. So seven men total.\\n', \"Let's go upstairs, okay?\\nWhat is it?\\n\", 'Master Kong.\\nMaster safety.\\n', \"Mister Data, set a pursuit course. Maximum warp.\\nGood hunting. Hayes out.\\nAbsolutely.\\nThe new quantum torpedoes are doing the trick, Jean-Luc. We've destroyed forty-seven Borg ships so far... and only lost fifteen of our own.  But one of the Borg ships has broken through our defenses, and it's heading directly for Earth. Can you handle it?\\n\", \"I think it be a better idea if you and Thomas go out, because otherwise she's gonna start a fight with you, and everybody's gonna be yelling.\\nWe didn't tell her anything yet. We thought that we'd leave it to you. We thought you'd put it like how you were lonely, and why don't she come to live with you. Because that way it looks like she's doing you a favor, insteada we're throwing her out, and it won't be so cruel on her. Do you want Tommy and me to stay here with you?\\n\", \"Disgusting?  Bingo night was sold out for six months after that!  They raised enough money to build a day care center.\\nThat's disgusting.\\n\", \"The knife destroys the evil and saves the soul of the possessed.\\nI don't understand.\\nBy freeing them.\\nBy killing them?\\nThe dagger is used to free those possessed if stabbed directly into the heart, according to ancient beliefs -\\n\", 'I swear to God, it works with lions sometimes!  There we are -- your baby is free --\\nAre you insane?!\\nBorn free, as free as the wind blows. As free as the grass grows --\\n', 'Kill me? Lex Luthor? Extinguish the greatest criminal flame of our age? Eradicate the only man on earth . . .\\nWhy do you say this to me when you know I will kill you for it?\\n', \"I was looking for you tonight. I don't know if you have a boyfriend...\\nWhat's up?\\n\", 'I\\'d love to stay and chat, but I\\'ve got to get back to work.  I still have my job.\\nWe\\'ll see.\\nYou\\'re going to be very disappointed.\\nYes.  Conviction.  Conviction that I\\'m onto the truth.  You\\'re going to do the same thing to \"poor bastard number four\" that you did to the last three.  You\\'re going to run again.  And I\\'m not leaving until you do.\\nDid something happen to make you care about reality?\\n', \"I'm out of here.  Jenny.\\nYou left me a message?\\nYou weren't home.  Like always.\\nYou broke up with my machine?\\nDidn't you get my message?\\nNo, you didn't.  When?\\nOh, come on, Buffy.  You know what's going on.  It's not working out at all.  I've got to move on.  I mean, I've got needs, too.  I told you about all this.\\nI don't understand.\\n\", \"I brought Denver to twenty million. Denver deals with me all the time. You listened to Sugar? You let that snake in the door.\\nI'm sorry, I --\\nSo you empowered Bob Sugar to deal with Denver behind my back?\\nHey, I'm learning as I go.\\nSaid who?  Sugar?\\nApparently, Denver wanted to deal with him instead of you.\\n\", \"I got twenty or so in my purse. Give me what you've got.\\nThirty-three here. Totals one-ninety. Not enough.\\nTen here.\\n\", \"What'd ya think?\\n...this is a snappy song....\\n\", \"Could be worse, he could've named you Reticulum.\\nI used to hate it.  Now I like it.\\nAdhara.  I told your father, if you're looking for a name, you can't go wrong with a constellation.\\n\", \"\\nThat bum.  So what if he has a Porsche, he can't treat you like that...it's Friday night for crissakes.\\n\", 'Your father liked shifting. He said it brought him closer to the engine. \"Like a woman, purring with ecstasy.\"\\nSome of them have to be automatics.\\nIt\\'s your car. These are all your cars.\\nI don\\'t even know if I should be doing this. It\\'s a very expensive car.\\nThe first time can be awkward. You just have to get on and try again.\\n', \"What, all of a sudden you got a degree in supernatural law?\\nHey, you can't kick me! You're an apparition!\\n\", \"Not yet.\\nHello! You're like, a lawyer.\\n\", \"But I got you out. Didn't I, Doc. I did it. I got you out.\\nYeah... I know.\\nI couldn't have... much longer.\\nThanks again.  I 'm glad you waited.\\n\", \"J.M. Inc. Be all that someone...\\nBut I gotta go now. I've got to go be Johnny.\\n\", 'Sure.\\nAnother time.\\nMy brushes, I have to clean my brushes.  Thanks, though.\\nWhat?  How about a drink?\\nI should be going.\\nGood.  I hate to worry.  I got ulcers.\\n', 'Just GO! GO! DAMMIT PUNCH GO!\\nWhere?\\n', 'Thank you.  I confess to a preoccupation with acquiring precisely that which the world says I cannot have.\\nI must compliment you on Miss Thompson... an exceptionally charming assistant.\\n', \"You gotta admire the man's determination.\\nWhat are you thinking?\\n\", \"Hell is precisely what is going on, Joey.  And we have to stop it.  I because of a special obligation, you because you're the only person who can help. And because you know what is right, and just, and true.  Will you walk with me a while?\\nCaptain Spenser.  Elliott.  I ... What the Hell is going on?\\nWell done.  Brave girl.  You've probably never shaken hands with a ghost before, am I right?\\n\", 'He would invite these girls home from the staple factory to our condominium in Palm Springs. He had a device he called the Intruder.\\n-- is not a challenge. I need something I can sink my teeth into, professionally speaking.\\n', \"He could be brought back.\\nWade's in Cryo-Prison.\\n\", \"Gus Gorman, this is my Psychic Nutritionist.\\nShe's not his mama either.\\n\", 'They made you leave your hotel...  ... you caused a disturbance.\\nWhat happened?\\n', \"Maybe that's it, then.  Everything's upside down and backwards...\\nWon't do, love.  It's not spring.  It's nearly autumn..You're in Oz.  everything's upside down and backwards here.\\n\", 'Many times.\\nYou have taken business class?\\nMaybe. But in business class there are fine wines, linens, Belgian chocolates.\\nThat is why they call it coach?\\nIt depends where you sit Zozo. In coach it is like the bus to Giterama.\\nWhat is it like to fly on a plane, sir?\\n', \"All right, you got it, you got it!\\n...you crazy bastard.\\nCome on...\\nCome on.\\nCome on.\\n\\n...Pete.  Come on.\\nCome on, old...\\n...wanna give it a try!\\n...Night Shift, but you might...\\n...-hood of you knowin' any prayers is slim...\\nI know the likeli-...\\nCome on, old Pete.  Oh.\\n...Pete!  Come on.\\nOh, shit.  Come on, old...\\nCome on.\\nGet over!  Get over!\\nLeft, left, left!\\n...easy, easy, easy.\\nOh...\\nI'm goin'.  I'm goin', baby.  I'm goin', I'm goin'.\\nGod, man!\\nCome on, old Pete.\\nCome on!\\nCome on, old Pete.\\nOh, shit.\\nGo, go, go, go, go.  Oh, shit.\\nHey, hey!  Whoo!  Okay.\\nI should've had that dog bite me.  I would've gotten rabies! Could've went to the hospital, had a pretty nurse!\\n\", \"The Naming of Names is not necessary. Just concentrate on her face. Your Shadow will do the rest.\\nWhat do I do?  I don't know her name.\\n\", \"Hell, yeah. I like it cold. Colder the better.\\nThat's a good thing?\\n\", \"That'll be nice. A home with mother. A real honeymoon. In Albany, too. Ow!\\nJust for the first year.\\nI know you will, Bruce. Are you going to live with your mother?\\nWell, I'll try to give her one.\\n\", \"Well, that's good. I like to viddy the old films now and again.\\nSomething like that.\\nYou mean like going to the pictures?\\nIt's quite simple really. Were just going to show you some films.\\nWhat exactly is the treatment here going to be then?\\nSomething like that. You are a little undernourished, so after each meal were going to give you a shot. Roll over on your right side please, loosen your pyjama pants and pull them half-way down. He does, somewhat reluctantly. She gives him a shot in the bum.\\nVitamins will it be then?\\nOh no, nothing of the sort.\\nWhat's the hypo for then? Going to send me to sleep?\\nI hope so, Missus. She inserts a needle into the medicine vial.\\nWe're going to friends now, sir.\\nI realise all that, Missus, and I'm very grateful to all concerned.\\nGood. In a few minutes, you'll meeting Dr. Brodsky and we'll begin your treatment. You're a very lucky boy to have been chosen.\\nFine... fine.\\nHow're you feeling this morning?\\nIndeed it is. May I take this She removes his tray.\\nGood Morning, Missus. Lovely day, isn't it?\\nGood morning, Alex, my name is Dr. Branom. I'm Doctor Brodsky's assistant.\\n\", \"Oh. Oh!\\nSouth Dakota - but on the top.\\nSouth Dakota - north?\\nSouth Dakota north.\\nWhere is your farm?\\nYes, ma'am.\\nFarmer?\\nChristian Svenson.\\nWhat is your name?\\n\", \"What, all of a sudden you got a degree in supernatural law?\\nHey, you can't kick me! You're an apparition!\\n\", 'Beautiful.\\nHot dog! Just like an organ.\\n', 'I had my breakfast while you were still asleep.\\nEdward will give you your breakfast, Amy.\\n', \"And have <u>you</u> found Peace..?\\nWell, if he hasn't, I'm vastly mistaken.\\nOh, yes.  He's found Peace.\\nNo, you're wrong about Verger.\\nMason Verger.  For he cannot be free. Dr. Lechter refashioned his body so it mirrors his soul, what an impossible injustice.  Can <u>you</u> be free....?\\nWho would that be.\\nBecause we both know who's buying the Lechteriana.\\nAnd why is it a vacant exercise?\\nRich, comic book freaks.\\n<u>Who</u> are these guys...?\\n\", \"You could. It'd be a waste of your time, though.\\nI could take you downtown.\\nI wasn't feeling well.\\n\", '\"I want to tell you something, Harriet...\"\\nFuck you care...?\\n', \"It's better. It's a construction depot. They'll have the strongbox and some ammo and explosives for us to take. That way we can take on a bigger job.\\nIt's not a bank.\\n\", \"It falls asleep and you die, so wake it the hell up.\\nI'm the one who can feel my leg, and it's falling asleep.\\nIt's not falling asleep.  You've only been standing there for three minutes.\\nMy leg is falling asleep.\\n\", \"I'll kill you!! I'll fucking kill you!! I'll do it! I'll do it! I'm not playin' around here!\\nNobody's shooting nobody...come on, just let us through the hatch!\\n\", \"Yeah it was all just an accident.\\nIt's called spontaneity, Leonard. Get with the program.\\n\", \"Here please.\\nI don't have to look at anything. I don't have --\\nFrom the beginning I don't know why the hell he's messin with you. If he was me he'd know better. If he was even a city boy he'd know better. You're a whore Miss Daniel, that's the truth of it, right? Now somethin I'd like you to look at.\\nIs that what he set me up for? Everything he's told me from the beginning? -- don't worry, don't --\\nNo, that's not --\\nWhat was better? I made better bait?\\nI guess he figured it was better.\\nIs that why Klute didn't tell me?\\nWe don't want to just lock him up; we want a conviction, we wanted him to do something more.\\nWhy isn't he locked up?\\nWell we're pretty --\\nYou said someone killed them, you said you know who, you said that.\\nNow there's a picture I'd like you to --\\nArlyn and Jane commited suicide. He said they commited suicide.\\n\", 'I knew him very well. We were extremely close until I dropped out.\\nWhy? Didn\\'t you know him well?\\nYes. I was surprised that he accepted when I asked if I could stay here until I found a place.\\nNine years?\\nNine years.\\nHow long has it been since you\\'ve seen John?\\nExactly. I would say the fact that you feel the first year of your marriage has gone by quickly means lots of things. Or could mean lots of things.\\nYou mean like \"time flies\"?\\nAnyway, I think the mind is very flexible as far as time is concerned.\\nOh, uh-huh.\\n', 'Yes.\\nYou want me to break into the office tonight and steal the leads?\\nYes.\\nDave.\\n', \"I've been a good worker, Solomon. A hard and loyal --\\nYou were gonna ask me weren't you?\\nI don't know.\\n\", \"Don't worry, I don't want to give up any more.\\nBe careful with it.\\n\", \"Where we goin'?\\nC'mon, Sal.\\nYou must be guilty of something or you would have never come in saying the things you said.\\nI'm not guilty of nuthin'.\\nI didn't say nuthin'.  You must have a guilty conscience.  What are you guilty of?\\nSal, if you want me to deliver any faster, get me a jet rocket or something, cuz I can't run with pizzas, all the cheese ends up on one side and shit.\\n\", \"I'll take your word.\\nYou'll have to take my word for it.  We don't make slides until we have perfect prints for comparison.\\n\", \"We did not approach you.\\nAs you remember it -- when you approached us...\\nThank God we're beginning at the beginning.\\nSince, on the eleventh of June of this year -- when the first negotiations for a merger...\\n\", \"You can't get away with..\\nAh, the good guys always triumph in the end. It's what allows our children to sleep at night.\\n\", \"I know it is -- but sit down anyway so we can get it over with, okay?\\nIt's bullshit.  You know it is.\\nAs long as Internal Affairs wants you to, I suppose.  Sit down, Nick.\\nI'm fine.  Come on, Beth!  You know I'm fine!  How the hell long do I have to keep doing this?\\nHow are you, Nick?\\n\", \"Groovy.  I've never felt better in my life.\\nRay -- Ray -- How do you feel, man?\\n\", \"I NEED THE TWENTY!\\nIt'll just be a few minutes.\\nYou bastards, I said twenty!\\n\", \"The only Great White in captivity, Calvin.  All those people standing in line...\\nA shark could have a hundred embryos in its sack.  You know how many are born?  Maybe five. They eat each other before birth! They're born killers!\\nIt's one of the world's greatest creatures.\\nThree hundred million years of evolution have created the greatest butcher the world's ever known.\\nCalvin, I insist I be allowed to capture it!\\nIt'll chop up half a million dollars of your tropical fish in two minutes.\\nNo!\\nCalvin, let me go down there and kill the bastard.\\nI've handled sharks before. Bigger than this one.\\nYou can get killed!\\n\", 'Absolutely.  Look at this.\\nYou mean to tell me, a mugger would stay away from someone because they walked a certain way?\\n', \"Don't thank me. You're doing all the work.\\nThank you, doctor.\\n\", \"I wouldn't be opening this place tomorrow if every single thing down to the beheaded Beanie Babies hadn't tested 100% safe.\\nSources have told this reporter that the real reason your Park's opening has been delayed was a near-fatal accident on one of the rides here. Comment?\\n\", \"...ready before the morning crowd blows in.  I got it...\\n...ready before the morning crowd blows in.\\nTwo pots of coffee...\\n...like April.  It gives him gas. And make sure there's  two pots of coffee...\\nAnd don't give him any ice cream...\\nBe sure you feed Bosco.\\nYeah?\\n\", \"The gun?  It's in the upstairs closet.\\nNothing.\\nKeep what handy?\\n\", \"I guess so.\\nLittle rum in yours, too?\\nOh -- I'll take the same, I guess. And coffee.\\n\", \"Look at me -- I'm not even listening to a word you're saying.\\nWell, I think it's good to keep all your options open. You can always enroll for the winter quarter.  You could even live here and go to the city college part time, and still get a job if you wanted to.\\nLook, I told you I'm not going to college.\\nIt's tough to find a good job without any kind of training.\\nWill you get off my back for once?\\n\", \"Neuter a locust, feed the world.\\nNot bad.  I was traveling around studying the reproductive and migratory patterns of locusts when Maggie met me.\\nGuess you'd know about that.  You're an entomologist, right?  How's business?\\nAh, can't stay away from her, can you? Like a moth to a flame.\\nWell, I'm writing another article on the cacophony.\\nShe's a cacophony of contradictions.\\nThanks.\\nAnd end up in the papers?  I've been humiliated enough already to last a lifetime, thank you.  I'm sorry she got you canned.\\nYou could have told me you were fiance number three.\\n\", \"I have a job for you and I don't have much time.\\nYou may speak.\\nI'm on a special phone, may I speak freely?\\nYes.\\n\", \"You're the star.\\n<i>A show.</i>  Then who am I?\\nA show - that gives hope and joy and inspiration to millions.\\nThe creator of what?\\n\", \"I am not conscious of it, sir!  But since you have thought it necessary to tell me so, we part!\\nI must tell you sir you treat me with disrespect!\\nSir, Arnold is a traitor.\\nDon't lie to me, Hamilton!  If I had not court-martialed Arnold...\\nSir, I think no such thing.\\n\", 'Only one way to find out.\\nThink we can fit up in there?\\n', 'Sure!\\nYes. I can do \"Swanee River.\" Would you like to hear me?\\nHe did?\\nThe drums. He taught me to play some.\\nWhat did he play?\\nYeah, I know.\\nHe did? I play the tuba\\nHe played in the town band, too.\\n', 'Don\\'t be insecure, Doc. You\\'re a big help.\\nThis isn\\'t a joke. Your parents spend a lot of money to send you here. I\\'m trying to help you.\\nWhy? I like spending time with you. You know, you\\'re quite attractive for a woman your age. You have killer legs. Killer.\\nNo. This is going to be our last session.\\nSame time next week?\\nI think that\\'s all the time we have for today.\\nLet me tell you something, doctor. Chicks love a guy with a bad rap. They say they don\\'t, but they don\\'t mean it. They all think that they\\'re the ones that are going to \"save me.\" The trick is to let them think it\\'s true.\\nDon\\'t you want to change that?\\nI do.\\nBut you said you have the worst reputation.\\n', 'What do you mean \"you think so?\"\\nI think so.\\n', \"You want to kiss me...\\nWell...\\nAnd what?\\nYou know... we've been going together for a week and a half...\\n\", \"It's not too bad working here.\\nWe used to be in the supply section, carrying boxes of medicine about but we dropped too many of them.\\nThey keep switching us from department to department.  He doesn't mind because he can't sit still.\\n\", \"...Polo.\\nMarco...\\nOkay, someone's expecting us. Turn on your locators -- Anyone sees anything...\\n\", \"I guess so. I gotta stop off home too.\\nYou wanna run by the park and see what everybody's doing? Get zooted?\\n\", \"I don't care what he thinks.\\nMary, Honey. I talked too much, like always -- he thinks you told Elaine the things I told her.\\n\", \"Oh, baby, you cruel...\\nCarter, just because you saved my butt, doesn't mean it's yours.\\nYou're killin' me here.\\n\", \"20 minutes.  Susan?\\n20 minutes. If you don't go sight-seeing anymore.\\nWe'll be back at the platform in...\\n\", \"Homesearchers.\\nThen let's narrow it down.\\n\", \"Fuck them.\\nI'm not. But you should check your backpack 'cause those guys like to steal shit.\\nDon't look so freaked.\\n\", \"All right.\\nWe better get started to your house, because the buses only run about one an hour now.\\nI'll wait for your call.\\nThe reason I can't be definite about it now is my Aunt Catherine is probably coming over tomorrow, and I may have to help out.\\nI'd like that very much.\\nI'll call you up tomorrow morning. Maybe, we'll go see a movie.\\nNothing.\\nWaddaya doing tomorrow night?\\n\", \"I'm gonna do it, Carmen. I'm gonna sign up for Federal Service.\\nAre you the boy for me ?\\n\", \"Don't you think there are more important things to talk about than my record collection?\\nHave you tackled the Great Reorganization yet?\\n\", \"You really are pathetic, you know that? Mathew Grimes is only one letter in my alphabet. He is nothing compared to me.\\nDEAD! YOU HEAR ME?\\nI'm sorry, the party you're trying to reach is not answering. Is there anyone else you would like to talk to?\\n\", 'Well... no...\\nHave you tried to ask the God for anything?\\n', \"Oh, them.  Hell, I seen them.\\nClouds, Kid. We was lookin' at them clouds on account of we got a storm ridin' up our ass.\\n\", \"Now that is very interesting --  Take a chair, Joseph,\\nYes.\\nYou're welcome to my little nest, Joseph -- is it not? That's right -- you have something to say to  me -- something very private.\\n\", 'Forget it.  We work for it.  We keep it.\\nMaybe we should get a pimp. Carlos likes you and --\\nLooks slow tonight.\\n', \"I don't know what you're talking about.\\n...That's what's important.\\n\", \"Caesar, what are we going to do?\\nSo what?!  So fucking what?  Use your head, Violet.  The money is gone.  Gino is coming here to get it.  You think he's going to believe me if I tell him his piss-hole son stole it!  Is that what you think? I don't.  You know what I think?  I think I'm a dead man.  I'm one in the brain.  That's what I think!\\nBut you know he did it.\\nI hate that little fuck!  I hate him!  I hate him!  I should've done him!\\n\", \"Are you finished?  You okay?\\nWell that's great!  That's just fucking great, man.  Now what the fuck are we supposed to do, man? We're in some real pretty shit now!\\n\", \"No.  I can look around by myself. I'll take the upstairs.\\nLet's spread out and check all the rooms.  Outside, too.  You stay with me.\\n\", \"Holy cow!\\nOh, look, look, there's that... that's that's my old house. That's where we used to live.\\n\", \"I've been worried about you.\\nReally?  Lieutenant?\\nYeah.  You look... great.  I mean it..\\nI am?\\nJesus, look at you   You're glowing.\\nI know that.  This isn't about me.  It's about the choir.  You should hear them. They're good.  Really good.  And I taught them.\\nListen to yourself.  This isn't a career opportunity.\\nI really can't talk about it now.  The place is packed.  SRO.  I've got a show to do.\\nYou're supposed to be hiding out!\\nIt wasn't my fault!  They just showed up.  It's helping the convent.\\nWhat if I was Vince?  You'd be dead right now.  What are you doing giving interviews on TV?\\nEddie!  You scared the hell out of me!\\n\", 'How extravagant you are, throwing away women like that. Someday they may be scarce.\\nHello, Louis.\\n', \"We have a very nice pasta today. Alla Putanesca.\\nWhat's good?\\n\", \"... I guess I may as well be honest about my feelings, Chauncey, as I know you are I am in love with you... I love you and I want you... And I know that you know it and I'm grateful that you've decided to wait until... Until...\\nYes. That could be true.\\nI know you are, Chauncey...  ... You conquer a woman from within herself, you infuse in her the need and desire and the longing for your love.\\nYes, Eve. I'm very glad that you didn't open.\\n... I'm grateful to you, Chauncey... I would have opened to you with a touch, and you know that...  ... But you're so strong - I can trust myself with you. I'm glad, Chauncey - I'm glad that you showed so much restraint...\\n\", \"They were my husband's.  My late husband... may he rest in peace.\\nYou know, these fit... perfectly.\\n\", \"What do you want from me? There are no cars on this road. I didn't ask for this stupid storm.\\nYou promised.\\nI will. I will. We still have another day.\\nYou promised you would get me to my ship.\\nNo one's traveling in this weather.\\nI can't stay on this planet.\\nWe're not going to get a ride tonight.\\n\", \"A serum test possibly.\\nCan there be... some kind of test? To find out who's what?\\n\", \"No thanks, I think we ought to get going though.\\nYou were right Jahn, it's a great space..... Hey, Chris. Do you want to come and look?\\n\", \"I don't know anymore.\\nWhat's the matter? You on their team now? You think I'm the guy?\\n\", \"Oh nonsense young lady. You're going to start your day with a nice big breakfast.\\nActually--I'm not real ... hungry.\\nI put blueberries in them just the way you like.\\n\", 'I was thinking Alice Cooper.\\nJesus, I look like a raccoon.\\n', \"Guy who killed all the kids in the '40s.\\nRuins of the Rustin Parr house.\\n\", 'I need help. How many times if just one little thing that I needed would\\'ve happened, it would\\'ve changed everything. If I had a few dollars when an opportunity came along or... the tumblers just never clicked for me.\\nSo you saw me on TV and you said \"Hey, let me jump on this.\"\\nThe last two years, I\\'ve been a limousine driver, but I don\\'t see well anymore, so...\\nOh, man! So...\\nCheck forging.\\nWhat...\\nNo. Two times.\\nThe whole time? Eighteen years?\\nI was in jail.\\nIt could be, if you fixed it up. How did you... ? I mean how does anyone ... wind up like this?\\nQuite a shithole, isn\\'t it?\\n', \"A blue one, OK. And it's still there.\\nI will ask you once more, what sort of car --\\nAlex, how shouldI know? I'm just a girl.\\nWhat sort of car?\\nSo what's wrong with that?\\nHe's got a car?\\nHis car's still there.\\nHugo.\\nDavid?\\nMaybe he didn't like us.\\nSo I gathered.\\nDavid hasn't seen him either.\\n\", 'Agh--\\nMy dirty undies.  Laundry, Dude. The whites.\\nWhat the hell is this?\\n', \"That's Daddy!\\nGet in the skiff, Pearl, goodness, goodness, hurry!\\n\", 'Yes, Doctor.\\nGo about your duties.\\nDr. Hirsch?\\nCan I be of service, Miss Price?\\n', \"The picture's fine. That's the way she looks in real life.\\nThe picture looks blurry.\\n\", \"Thirty a month.\\nWhat's his rent?\\nWell, his rent's past due and he said to call you in case of an emergency.  He lose his job or somethin'?\\nWouldn't blame him if he was.\\nYou think he's drunk somewhere?\\n\", \"I have a date, Daddy.  And he ' s not a captain of oppression like some men we know.\\nI'm missing something.\\n\", 'You...him...UST.\\nSue who?\\n', \"Well tomorrow noon there's a... I got a...\\nSure. All right. We'll have breakfast. I get up about one o'clock. Tomorrow.\\nNo, I mean really. This is nothing for a person to do.\\nThat's not hard to do.\\nCan I see you again?\\nI understand, mister. It means something, really.\\nWell, I tried.\\n\", 'We need another one.\\nNegative. The Beryllium sphere will have to be replaced.\\n', \"Don't flatter yourself. You'll be lucky to last ten seconds with me Ben.\\nThat's an awful lot for ten minutes of beasting?\\nOk not your imports. All your other CDS and your K2 snowboard.\\nNot my imports.\\nI get all your CDs.\\n\", \"It was a piece of wood.  A shitty piece of wood.  It's not fair....\\nHe was doing his job....\\nHe was such a good-natured kid... Always going out of his way....\\n\", \"I wasn't dating him.  I was fucking him.\\nHow long were you dating him?\\n\", \"...I always thought so...\\n...that's what the swing is <u>there</u> for...thats its <u>purpose</u>, <u>isn't</u> it...\\n\", \"Don't touch me!  I'm <u>allergic</u> to you!\\nI'm sorry, okay?  Let's not fi...\\n\", 'Oh yes, more than enough.\\nThe money in the estate was enough to cover your costs?\\nWhen your father died I saw to it that the grounds were kept up.\\n', 'What a terrible misunderstanding. Of course, Signora Colombo can stay in the flat.  Who were those miserable tenants to complain about noise from a poor animal...when they pay such low rent.\\nYes.\\nExcuse me, I hope I am not a disturbance, Don Corleone.\\n', \"You can cut the crap.\\nChasing the dragon?  Whaddya mean?  You sure you don't want a pancake?\\nYou like chasing the dragon, Walter?\\n\", \"And if that were true?\\nWell, I was thinking, maybe an enemy attack is not in progress?\\nThat's a good question, Major.  Maybe if you think hard, you can think of the answer yourself.\\nWell, sir, I was thinking, if an enemy attack is in progress, how come the radio's still playing music?  It's supposed to go off, and all we should hear are Civil Defense broadcasts.\\nYou know the regulations well, Major.\\nAnd a condition red means enemy attack in progress, doesn't it?\\nThat is correct, Major.\\nWell, General Ripper, sir -- I was thinking -- we're on a condition red, aren't we?\\nCertainly, Major Mandrake.  You're a good officer, and you can ask me a question any time you want to.\\nYes, sir.  General, can I ask a question?\\nI didn't mean for anyone to play anyone else's radio either, Major.\\nOh, it's not my radio, sir.  I picked it up in the communications center.\\nI see you're playing your radio, Major. Isn't that contrary to my instructions for the personnel of this base?\\n\", \"Ever since you were a little boy...  Come with me.\\nYou did?\\nI always knew that you were special.\\nIt is?\\nJeffrey... this is wonderful.\\nI use it... to fight evil.\\nBut... the silverware?\\nOh, Mother, I'm sorry. I know how much you wanted me to be a doctor or a lawyer with a family -- but it's just not who I am!\\n\", 'I told you.\\nVoodoo.\\n', 'DAD!\\nPAUL!\\n', \"We'll change your appearance.\\nThey'll know who I am.\\n\", 'Goodnight, Senator--\\nWell, goodbye, sir--and thank you again.  Well--it--it was nice seeing you, Miss Paine--\\n', \"Zuzu, be quiet.  Put in Colleen's disc.  Number two.\\nThis is boring, guys.\\n\", \"Since you're going to be my escort, you'll need a new tie.\\nBeg pardon?\\nWould you pick one out, please?\\n\", \"Huh!\\nI was misinformed.\\nWaters? What waters? We're in the desert.\\nMy health. I came to Casablanca for the waters.\\nAnd what in heaven's name brought you to Casablanca?\\nIt was a combination of all three.\\n\", \"Thank you, Master.\\nIf you like -- just hurry!\\nIt's always been one of my favorite names.\\nWhy?\\nOh!  May I call you 'Master'?\\nHe only died two weeks ago -- I'm sure they'll still have him. Hurry now.  I'll prepare the body.\\nI'm getting excited just thinking about it.  What if he's not there?\\nCan you imagine that brain in this body?\\nThat's not bad.\\nAnd he wrote seventeen cookbooks.\\nHmm!\\n\", \"The ADM! Quickly! Where is it?!\\nOh shit! No! Andy, don't let them take me back there!\\n\", \"Don't run away from this, Dude! Goddamnit, this affects all of us!\\nAll right, I'm leaving.  I'm sorry ma'am.\\nLady, I got buddies who died face- down in the muck so you and I could enjoy this family restaurant!\\n\", \"I do my best.  Every day I dress her just as beautifully as if she was well.  It's just like dressing a great, big doll.\\nWe'll see if we can't make her well, Alma, you and I.\\nShe was very sick and then she went mindless, Miss.\\nShe must have been beautiful.  What happened to her, Alma?\\nMiss Jessica used to say this is the only way for a lady to break her fast -- in bed, with a lacy cushion to bank her head up. If you'd only seen her, Miss Connell. She looked so pretty.\\n\", 'Wait two seconds then go.\\nOkay, now. But-\\nCommander, you and Lt. Madison will have to go through the crushers one at a time in three second intervals. Tell me when the first crusher hits the bottom...\\n', \"Exactly!\\nSort of a Jack London style?\\nLet him have everything he wants.  Now hustle and write me a story from the point of view of the escaped man.  He hides, cowering... Afraid of every light, of every sound... hears footsteps... his heart going like that... And all the time they're closing in... Get the sense of an animal at bay!\\n\", \"No.  I'm getting used to it.\\nIcarus, please, if you want me to give you a bath just say so.\\nOh no?  Smell this.\\n\", \"Eh?  I'm like the Rest of America.  I don't <u>care</u> -- I'm just <u>addicted to it</u>...\\nWhat the hell do <u>we</u> care?\\n\", 'Really?\\nYes.\\nCameron?\\nCameron.\\n', \"It's perfect, Spock: a male, a female, together in a contained space can beam them up together and consider ourselves damn lucky...\\nThey are mature humpbacks, weighing 45,000 pounds each. They wandered into San Francisco Bay as calves and were brought here. We call them George and Gracie.\\n\", \"So inhale!\\nIt's like smoking without inhaling.\\nYou're not giving yourself a chance. Don't fight it.  Relax.\\n\", \"Jesus, James. Wow.\\nColon cancer.\\nGlaucoma?\\nIt's my father. He gets it from his doctor.\\nMaybe...\\nHumboldt County?\\n\", \"So -- you're behind all this. I should have known.\\nThey wouldn't let me in so I had to get you out.\\n\", \"Alcohol does horrible things to a developing young mind.\\nYou called the cops on us?\\nOur state has a zero tolerance policy for underage drinking.\\nWhat law, Merk? Having fun? Letting down our hair on prom night?\\nMr. Nelson, you disappoint me. First you break our school board rules. Then you break the law.\\nYou kicked us off your turf, Merk. Don't rain on our parade.\\n\", \"Try and live a lifetime before Friday. Cram it all in.\\nWhat?\\nI can't do this, honey.\\n\", \"...I think it's the best stuff I've done.\\nSo how's it going?\\n\", \"No you're not.\\nI told you, I'm shooting Iraqis.\\nReady for what?  What are you supposed to be doing?\\nDon't get pissed at me, just 'cause I want to be ready when this war starts.\\nKnock it off!\\nAt-at-at-at-at.\\n\", \"Please, I need my hands to work -- Christ, don't bust my thumbs.\\nMr. Gazzo says I should get two hundred or break the thumb.\\nHonest to God I'm broke -- Gimme a break.\\nMr. Gazzo wants the two hundred now!\\nDon't hit the face! Not the face!!\\n\", \"Droppen Sie dead!  Raus mit dem Ofen. Los! Los!\\nHey, Schulz! I got a deal for you. Suppose you help us escape. We'll go home and have everything ready for you in Madison Square Garden. For the world championship! Schulz, the Beast of Bavaria versus Halitosis Jones!\\n\", \"'Find out who you are and do it on purpose.' Dolly Parton.\\nLower. Same page.\\n'What is a friend? A single soul dwelling in two bodies.' Aristotle.\\n\", \"You are Paul Muad'dib, and your mother shall be a Sayyadina among us....  We welcome you.\\nCould I be known as Paul Muad'dib?\\nWe call that one Muad'dib.\\nWhat do you call the mouse shadow in the second moon?\\nYou have strength... real strength... You shall be known as Usul, which is the strength of the base of the pillar.  This is your secret name in our troop.  But you must choose the name of manhood which we will call you openly.\\n\", \"Well hell, I didn't know Pudge was gonna hit the home run.\\nWould have been nice to catch that game though.\\n\", \"We've put a tap on Dr. Hudson's phone.  I know you won't mention it.\\nOkay.\\nShe is a writer, writing best selling books about serial killing. Giving lectures she's well-paid for.  Her interests are not the interests of law enforcement.\\nHow come you're so up on Dr. Hudson?\\n\", \"What, you think everyone's as ignorant as you?\\nYou think that's what McDermott was raving about in the ambulance?\\nYou never heard the story of Saint Severin driving the werewolves from Paris?\\nSaint Severin...\\n\", \"That's a lucky C note for our new deal.\\nWhat's this?\\n\", \"Have a good time.\\nOkay. Later.\\nThey probably stopped off somewhere. Have her call me when she gets back. I've got Lyndsey here and I want to know what time to put her to bed.\\nWell, she's totally not here.\\nNo. I thought she'd be home by now. She went to pick up Paul.\\nIs Annie around?\\nNothing. I was just sitting down for the first time tonight.\\nHi, Laurie, what's up?\\n\", \"Like...ah...\\nWhat's he like?\\nI do know him...slightly.\\n\", \"Yes, thank you.  It wasn't bad.\\nDid you have a pleasant trip?\\n\", 'Thank you, Mr. Harris.\\nGreat.\\nDo you want I send it to your room?\\nOk.\\nMr. Harris, we have fax for you!\\nHello?\\n', 'Sure thing.\\nThanks for everything, Max.  Wow...\\n', \"Steve, dear, please forget all of this.  What can it matter after tomorrow?\\nBut he could be reporting back to your brother, couldn't he?\\nDarling, I don't care - really I don't.  Sidney'd had a secret crush on me for years, but nothing we do is his business -\\nHis stooge, Falco, is around - I saw him walk in.  He's been spying on me for weeks, Susie.\\n\", \"You got a maiden name, Daddy?\\nThat's a maiden name.\\n\", \"Dynamite is dangerous.\\nSo maybe this is my dynamite.\\nSo?\\nDynamite.\\nCream rinse?\\nI saw this show once. It was about logging. I was home sick, there was nothing else on. Do you know how they break up really bad log jams? You know, when they're really tangled... ?\\nWhy do it?\\nHey, believe me -1 know I've got a great chance of making a fool of myself, here.\\nAre you sure about this?\\n\", \"No, I... I never killed anyone!\\n... so you were in the middle of the battlefield, with your sword in your hand, waving it above your head... charging against the enemy, screaming and yelling... fighting for your life... and you want us to believe that in the middle of all this excitement you never killed anyone?\\nYes, maybe... I don't remember...\\nYou held up your sword and flourished it about in the air? Like this?\\nNo, I... I held it up to...\\nDid you use the sword that you held in your hand?\\nYes, but...\\nSo sometimes you were carrying just your sword?\\nYes, probably... maybe...\\nWe have numerous witnesses who can confirm that you were not always carrying your banner...\\nNo!  I warned the English to go back home -- I begged them not to force us to fight -- they knew the defeat I would bring on them... why didn't they listen to me?\\nThen perhaps the temptation to kill would have been stronger... too strong perhaps...?\\nNo, of course not... I never killed anyone...\\n\", \"... I'm pregnant, Bonita!\\nI think, he's really nice... He's a funny guy... sometimes even very moving.\\nYes.  Not like Enzo.\\nHe's shy, not like Enzo.\\n\", \"And you're the only one who'll ever hear that speech.  Just you.\\nThat's that's that's that's beautiful...\\n...that it has been vouchsafed to him.\\nOf course.\\nOf course, of course that's what I'm <u>saying</u>.  As the Old Mill goes around, he <u>sees</u>...\\nAs the Old Mill goes around...\\n...that...he <u>sees</u> that...\\nI understand...\\nOf <u>mercy</u>...off...\\nYes...\\nThat: in an act of...\\nNo, no, I see...\\nBut, but but but...\\nYes...\\nThat, he says, there <u>are</u> no second chances...that he's been presented what he <u>prayed</u> for...and: he's ruined it.\\nYes.\\n...that...that he <u>prayed</u> for a second chance.  But...do you see?\\n\", 'Oh. Okay.\\nI want you to talk dirty to me.\\nYes Esther.\\nMordechai?\\n', 'On a mountain of skulls in a castle of pain, I sat on a throne of blood.  What was will be, what is will be no more.  Now is the season of evil.  Find me a child that I might live again.\\nCommand me, lord.\\nI, Vigo, the scourge of Carpathia, the sorrow of Moldavia, command you.\\n', 'Take as much room as you want\\nWhat a surprise.\\n', \"Damn, life is strange. I had you figured for this cold-blooded, calculating bitch -- Not that I didn't admire you for it.\\nDrugs? God, no! I'm totally against drugs.\\nSo you weren't involved with him in his pathetic attempt to diversify?  Were you mixed up in the drugs, Betty?\\nI have no idea what he was mixed up in... it was always something.\\nYou really... didn't have anything to do with what Del was doing, did you?\\n\", \"I said I'd listen to you, not necessarily believe you. You're telling me my people are in a plot against me. You're telling me my husband wants me killed. What do you expect?\\nI knew you wouldn't believe me.\\n\", \"I don't care to meet anybody until I get paid--come on--come on. One dollar each, please, for the Milk Fund.\\nHere, here, Susan--this is Jeff Smith-- our new Senator.\\n\", \"So when she wanted a divorce...I was...I didn't know what I'd done wrong. I didn't see it. I didn't see it....\\nYeah, all right, listen...\\nIt's about time I did. I was one of those guys, workaholics. I worked my ass off for them - my wife, my daughter.  That's just what I thought I was supposed to do.\\n\", \"Because we, and our associates, have paid out hundreds of thousands of dollars to shyster lawyers like you, <u>because</u> of shyster lawyers like you, and we'd just as soon sit back and sip a beer while you get ass-banged by as many people as possible.\\nWhy not?\\nYes. But we can't.\\nYou would?\\nWell we'd sure like to help you.\\n\", 'Oh, Rupert and Carla had a little hassle and went home.\\nWhere are the others?\\n', 'Ah, baby...I thought this was you.  Craig, what are you doing to me?\\nWhat you doing to my nephew?\\nHuh?\\n', \"Because he's doing just what he said he'd do. He's coming for us.\\nAnd why is that?\\nI don't think we need to keep looking for him anyway.\\n\", \"That's up to Pilgrim.\\nWell, it's worth it, really? I mean, how much longer do you think you need to work with Pilgrim?\\nI don't have a problem with that. It's up to Annie.\\n\", \"Yeah, well you wrote me you were six- foot-four, baby.  So don't talk to me about little white lies.\\nAsh... didn't you write me that you don't eat chocolate?\\nCan't survive on our bodies alone, Nick.  Hurry up!\\nAshley, Jesus --\\n\", \"I'm stuck...\\nIt's okay...come on...\\n\", \"I'm one of the few who know every inch it, now that John is gone.\\nThis place is a maze.\\n\", 'Absolutely.  Abigail...\\nYou sure about this?\\n', 'Maybe I can help you find him. I know a\\t\\t\\t* lot of people.\\nKill him.\\n', \"It's just an excuse for us to spend time with you.\\nWhy do you even need a ride? You could walk there in two minutes.\\n\", \"Good-by, Tommy.\\nSure. Good-by.\\nGood-by, Tom. Later--when it's blowed over--you'll come back? You'll try to fin' us?\\nMe neither.  It's jus' stuff I been thinkin' about. Gimme you han', Ma. Good-by.\\nI don't understan' it, Tom.\\nThen it don't matter. Then I'll be all aroun' in the dark. I'll be ever'where--wherever you look. Wherever there's a fight so hungry people can eat, I'll be there. Wherever there's a cop beatin' up a guy, I'll be there. I'll be in the way guys yell when they're mad--an' I'll be in the way kids laugh when they're hungry an' they know supper's ready. An' when our people eat the stuff they raise, an' live in the houses they build, why, I'll be there too.\\nThen what, Tom?\\nWell, maybe it's like Casy says, a fella ain't got a soul of his own, but on'y a piece of a big soul--the one big soul that belongs to ever'body-- an' then...\\nHow'm I gonna know 'bout you? They might kill you an' I wouldn't know. They might hurt you. How'm I gonna know?\\nNo, Ma. Not that. That ain't it. But long as I'm a outlaw, anyways, maybe I can do sump'n. Maybe I can jus' fin' out sump'n. Jus' scrounge aroun' an' try to fin' out what it is that's wrong, an then see if they ain't sump'n could be done about it.  But I ain't thought it out clear, Ma. I can't. I don't know enough.\\nYou don't aim to kill nobody, Tom!\\nThey gonna drive me anyways. Soon or later they'll get me, for one thing if not another. Until then...\\nTommy, they'll drive you, an' cut you down like they done to Casy.\\nI been thinkin' about us, too--about our people livin' like pigs, an' good rich lan' layin' fallow, or maybe one fella with a million acres, while a hundred thousan' farmers is starvin'. An' I been wonderin' if all our folks got together an' yelled--\\nHe was a good man.\\nYou know what I been thinkin' about, Ma? About Casy. About what he said, what he done, an' about how he died. An' I remember all of it.\\nAwright, Tommy. What you figger you gonna do?\\nI know you would, Ma. But I ain't gonna let you. You hide somebody that's kilt a man an'... an' you'd be in trouble too.\\nI could hide you, Tommy.\\nI'd like to stay. I'd like to be with ya--  --an' see your face when you an' Pa get settled in a nice little place. I sure wish I could see you then. But--  --I guess I won't never be able to do that. Not now.\\nIt had to come, I reckon, soon or later.\\nThey was some cops here, Ma. They was takin' down the license numbers. It looks like somebody knows sump'n.\\n\", 'Yeah, I know.\\nThis is where it happened. Right over here.\\n', 'Hey...\\nOkay, Jack.  Nice doing business with you... Cash is about to take off...\\n', 'You look like... Brian.\\nHi, Helen. Man, you look great.\\nBrian?\\n', 'Looks like it was your only date. Unless you go to his funeral.\\nNo, it was our first date.\\nDid he come home with you? Did you go to his apartment?\\nYes... I... we had dinner.\\n', \"So do I.\\nLook at that woman. She's what? Fifty? Fifty-five? But she hasn't let herself go. I appreciate an older woman who has a commitment to her body.\\n\", \"I looked. Suspicious bastards got them all locked up.\\nWe could steal one of the choppers.\\nThere's no transportation anyway this time of night.\\n\", \"See you Thursday... regular time.\\nWhere ya going? I've got a million questions.\\n\", 'You get the idea.\\nGo ahead.  Say it.\\nHe farted on meringue; he sneezed on braised endive; and, with creme of mushroom soup, well...\\n... Oh, yeah.  Oh, yeah.\\n', 'We could have had some kind of birthday party for him.  We could have taken Paul to the fight with us.\\nWhat then?\\n', \"Yes, sir.\\nOh, sure, sure. But first the property settlement has to be worked out -- then it takes six weeks in Reno -- meanwhile, I'm going to enjoy being a bachelor for a while.  Oh, by the way, you can now have lunch in the executive dining room --\\nWell, it's just that so many things have been happening so fast -- I'm very pleased -- especially for Miss Kubelik. Now that I've gotten to know her better, I think she's the kind of girl that definitely ought to be married to somebody --\\nMy assistant, Roy Thompson, has been shifted to the Denver office, and you're taking his place.  What's the matter, Baxter? You don't seem very excited.\\nMine?\\nYou like?  It's all yours.\\n\", \"I -- uh, yeah, I guess, yeah.\\nWithout peer?\\nI am. Yeah.\\nAre you a fucking colossal idiot?\\n...I'm sorry...\\n\", \"-- I like the car I have.\\nIt could help with a new car --\\nI don't want his money.\\nYour father dropped off an extra check.\\n\", \"Did you confess?\\nA twelve-year-old policeman came by the house this morning.\\nYou heard? How?\\nI heard.\\nWalter's on campus, being the good soldier for WordFest. But he's a basket case. Someone stole Marilyn's jacket last night. And Poe's missing, too.\\n\", \"The pelt is black.  It's a kind of man.\\nSome kind of animal?\\nMy parents died in an automobile accident when I was sixteen years old.  They left me a brewery and a baseball team--and other things.  I live for a living.  I've just come back from Kenya--in Africa.  I've been hunting Mau Mau there.\\nWhat do you do for a living?\\nDestiny often seems that way. You're going to marry me.\\nThis is crazy.\\nYou are woman.  I know woman well.\\nYou don't even know me.\\nDaughter--I love you very much.\\nI'm engaged.\\nDid you ever daydream that you would one day meet a friendly millionaire?\\nI'm sorry, sir, I'm engaged to be married.  My boyfriend would be mad if I went out with another man.\\nWhat time do you get off work, my child?\\nYes, sir.  It was a very unusual automobile. It was a Cadillac, but it had water buffalo horns where the bumpers should be.  And what to drink?\\nRaw hamburger, please--and a whole onion.  I want to eat the onion like an apple.  Do you understand?\\nSir?\\nA springbok, an oryx, a gemsbok--a gazelle.\\nEighteen--  and a half.\\nI think so, daughter.  How old are you?\\nCan I help you, sir?\\n\", \"And the Sheriff at the time was Big Charley Wade. Charley was one of your old-fashioned bribe-or-bullets kind of Sheriffs, he took a healthy bite out of whatever moved through this county.\\n'57, 1 believe--\\nThe two of us were the only deputies back then me and Buddy--it's what-- '58--\\n\", \"Look, I know there's no going back for us.  That's not it.  I just want to make this up to you.  Clear my relationship karma.  And I know the area, I've got some cousins down there.  Please.  Let me do this for you.  Let this be my gift to you.\\nOh no, I don't think that would be a good idea.\\nIt's in the south.  I've still got my rental car.  I can get you there by morning...\\nWhere's Positano?\\nI went back to that restaurant and I spoke with his waiter, just on an off chance, you know, and, anyway-- he knew the name of the guy's hotel.  So, I called there, they said he left yesterday.  But, apparently he comes here every year, always follows the same route.  He always goes from here down the coast to the Le Sirenuse hotel in Positano.  I called.  And they told me he's checked in.\\n\", 'Can I come in?\\nMore cough syrup?\\n', \"Well, we have to end apartheid for one. And slow down the nuclear arms race, stop terrorism and world hunger. But we can't ignore our social needs. either We have to stop people from abusing the welfare system. We have to provide food and shelter for the homeless and oppose racial discrimination and promote civil rights while also promoting equal rights for women but change the abortion laws to protect the right to life yet still somehow maintain women's freedom of choice.\\nLike what?\\nOh come on. Price. There are a lot more important problems than Sri Lanka to worry about. Sure our foreign policy is important, but there are more pressing problems at hand.\\n\", 'Hola, viejo.\\nHola, chica.\\n', 'Everything. Just for the record.\\nEverything?\\nOh, things. Photos. Tapes. I tape everything that goes on in this office.\\nSo what do you keep in there?\\n', \"Jesus.\\nYour dried cum.\\nI don't know. Panties and --\\nGuess what I'm wearing.\\nYou must have been crazy.\\nGod, I can't believe I ever hated you.\\nI'm just, y'know, passing the time best I can till I can see you.\\nHey, lover. Whatcha doing?\\nHi, it's Joel.\\n\", \"What's that?\\nNot so funny...if you recollect what 'Nawyecka' means...\\n\", \"Haven't you had enough of us, Dad?\\nDinner?  Again?\\n\", \"Uh huh.\\nWould you?\\nI'd like that.\\n\", \"No thank you, I'm fine.\\nWould you like more tea?\\n\", \"'Cause it stinks.\\nWhy?\\nNo.\\nThere...Ya happy?\\n\", \"Be glad I made you what you are! You'd be dead not if I hadn't.\\nWhy yours alone? Tell me how it was done!!!!\\n\", \"Nothing I understand better than a mess.\\nIt's just such a mess. With Jake I mean...\\nTry me.\\nYou'd never understand.\\nGrace, I've been fucked over too many times, by too many women.  You're becoming the queen of hot and cold.\\nGet out of town, Bobby, as quick as you can.\\n\", 'Helllp!\\nDawn!  Dawn!\\n', \"You can do it. Come on. Easy. Just give it a try.\\nWhat? By myself?\\nPerfect. We got it.  Okay. Let's just give it a little try. See if you can stand.\\n\", \"But...\\nHoney, I can't talk now...I've gotta run, bye.\\nCharlie...I've got to see you.  I want to talk...\\nTeresa?...Yeah, Charlie.  Listen, I'm looking for your cousin.  Yeah, well today's Tuesday, payday.  I haven't seen him all day...no...well, have you got any idea where he is?  I know it's early yet but I'm just getting worried that's all...\\n\", \"Reed, we're running out of time.\\nCome on, Ben, come on...\\n\", \"You see how nice things are when we go slow?\\nYeah maybe you're right.\\n\", \"Let's say today, we make it your business.\\n'Course, it's none of my business.\\nWhat are you talking about?\\nMaybe you don't want to remember.\\n\", \"I already did, it's a totally barbaric custom...but on you, it looks good.\\nMarriage can really complicate things. So, aren't you gonna say anything about my earrings?\\nGood. I mean...\\nOh no, we wouldn't be from the same bloodlines or anything. We'd be like two total strangers who...accidentally had relatives that got married.\\nBut we wouldn't really be related right?\\nYeah I guess...sort of.\\nAnd you'd be my cousin?\\nRight.\\n\", \"Thank you.  You are 'Horse and Hound's' favorite actress.  You and Black Beauty.  Tied.\\nWell, it was nice to meet you. Surreal but nice.\\n\", \"Now, let's hear from you . . .\\nYeah  yeah . . . sure I will.\\nSay hello to New York for me.\\n\", 'No English lord would trust an Irishman!\\nI thought I was dead when ya pulled that dagger!\\n', 'I stand corrected Mr. Decker.  Navigator, lay in a conic section flight path into the cloud center; bring us parallel to whatever we find in there...\\nFive minutes to Cloud boundary!\\n', \"I confirmed it on the scanner. I knew something was up because Puffy used to bark like hell whenever he saw him and you know Puffy only barks at bad people.\\nYou picked that up on the scanner. We gotta move.\\nMeaning these are the people you live amongst, you got a right to know if they're creeps. For instance, did you know there's a guy down the hall cheating on his wife?\\nMeaning?\\nThese ain't strangers, they're neighbors. This only picks up signals in a half-mile radius.\\nNeighborhood Watch? Is that what you call listening in on stranger's phone conversations?\\nBet your ass I have. It's an important job, Neighborhood Watch is.\\nHave you been up all night again?\\n\", \"I wouldn't be surprised if it were a true wishing ring.\\nIt's a pretty ring.\\nMost surely that was a nice lady to give a ring to a little girl.\\nA lady threw it to me.\\nThat's a fine-looking ring.\\nLook at my ring.\\n\", \"I'm not interested.\\nAm I right?\\nPlease!\\nLook! To maintain Velocity, a swallow needs to beat its wings four hundred and ninety three times every second.   right?\\n\", \"Breakfast ... Ah! ... Come along friends, Ma Stone's a cook this side of heaven.\\nMa says breakfast's ready, Mr. Webster!\\n\", 'Hey Chief. Bring my cup back.\\nLove ya.\\n', \"Here's Judy Garland!\\nMontgomery Clift, wait 'till I tell Shelly.\\nNever heard of her.\\n\", 'Uh-huh\\nI picked them out for you, Jake.  I thought you would like the colors.\\nThey look nice.\\n', \"Very interesting, even to a layman.\\nAnd?\\nDr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.\\nOf course, you don't.  I'm glad you came. My callers are all professional. Clinical psychiatrists from cornfield colleges somewhere. Second-raters, the lot.\\nNo.\\nEmotional problems, I hear. He was a very promising young officer. Do you ever have any problems, Will?\\nStuart is fine.\\nAnd how is Officer Stuart? The one who was the first to see my basement.\\n\", \"That's affirmative.\\nIs it safe for human transport?\\n\", 'Al ???!!!\\nMy name is AL.\\n', \"The market in Lecter hints is way down, today, okay? I've got two good men dead in Memphis, and three civilians. I've got -\\nBut then he had to cover up, make her seem just like all the rest of them. That's what Lecter was hinting!\\nStarling -\\nMaybe he lives in this, this Belvedere, Ohio, too! Maybe he saw her every day, and killed her sort of spontaneously. Maybe he just meant to... give her a 7-Up and talk about the choir. But then -\\n\", \"Why you son of a --\\nFuture wife? Get real, man -- you're nothing more than a glorified brother in her eyes.\\nWhat line? The day you first laid your oily rap on my future wife you started a war!\\nHey, hey, hey, I'm not the one who started telling bald-faced lies about the competition -- that's crossing the line!\\nOh yeah, the plan was going along just fine until you showed up.\\nNice.\\nFriend. Baseball bat.\\nHow'd you manage that one?\\nJust dumb luck. I delivered a pie to her one night and she answered the door in her nightgown -- that was it for me. I went home that night, shaved my beard, and a week later I was laid out in her office with a broken back.\\nAnd you met Mary how?\\n...So then in '94 I went back to Dade Community College for a semester and when the Wal-Mart cashier job fell through I hooked up with the Pizza Barn.\\n\", \"They lied.\\nThey said they wouldn't hurt them, wouldn't hurt them if...\\n\", 'Good. You neednt worry about the Austrian anymore. Hes dead. Ill be in touch.\\nFine...\\nHows the boy doing?\\nHe showed me a book with a picture of a couple caught kissing in a street...\\n', \"Yes'sir.\\nGet me ten bags of mulch.\\nYou won't at the square this mornin'.\\n\", \"On the three days Paulie was sick this month, he got calls from a payphone across from the old man's building.  We got people in the phone company.  Thank God it was Paulie...we'll need Clemenza bad.\\nHow can you be sure?\\nYou're right, kid, Clemenza is okay. It was Paulie.\\nClemenza?  No, I don't believe it.\\n\", 'Can we put Rebecca Lawson at the scene?\\nCocaine is the last thing a man in his condition would want.\\n', \"Well that don't matter now, 'cause you got about two fuckin' seconds to live! Richie!\\nI never said help us!\\n\", \"I haven't seen you my whole life and now you show up and want a relationship? I hate you!\\nHello, Scott. I'm your father, Dr. Evil.  I have a son! I have a son! Everyone, I have a son!  Someday, Scott, this will all be yours.\\nHi.\\n\", \"Jeanne, you can't leave us like this!\\nI'm not afraid of the fire anymore. It will purify me...\\nNo, you must come, Jeanne -- we need you -- so much has happened since you left... I have a new horse now, a white one, just like yours... and La Hire hardly swears at all anymore...  You can't stay -- they'll burn you!\\nI'm staying, Jean.\\n... and maybe the king will give you some money, and a little land, and a title even...  ... wouldn't that be a fine thing? You, a lady of title!\\n\", \"Daddy?\\nHelp me.  I'm so sorry.  But please, help me...\\n\", \"I'm no spy -- where do I fit in?\\nYour mission is simple. Find out how and why these agents died.\\n\", 'I told you I had a vivid imagination.\\nI thought that business with the scarf was pretty nifty.\\n', \"According to what Mr. Crandall says, the road's a lot more dangerous than the operation. Church will be just the same. Well--almost the same--and we won't have to worry about him getting turned into catburgers by one of those damn Orinco trucks.\\nDon't be silly. Church is not going to die.\\n\", \"I think we can classify this as an emergency situation.\\nI'm not crazy...I'm not crazy. He's here...  We've got to get to the radio and call for help.\\n\", \"I've never dealt in black-jacks. Never.  Everybody knows that.\\nCause you're the man, right?  The Magic Man.  If it's got something to do with the wire, sooner or later it washes up on your beach.\\n\", \"Well it's just one of those things. You know... purely an accident, um. My husband had oh... been drinking, and he came home about three hours late, so he wasn't exactly in the greatest mood that night.  And well Danny had scattered some of his school papers all over the room... and my husband grabbed his arm, you know, and pulled him away from them. It's...it's just the sort of thing you do a hundred times with a child - you know, in a park or on the streets - but on this particular occasion my husband just... used too much strength and he injured Danny's arm.\\nHow did he manage to do that?\\n\", \"What does he want?\\nHope I'm not interrupting...?\\n\", \"Nineteen fifty --\\nWhat year?\\nMarch 11th.\\nOperator, what's today's date?\\nI'm sorry, there's no answer.\\n\", 'Down there.\\nHe knocked me out the sonofabitch. Where is he?\\n', 'Segovia begged me for me secret but I said, \"No, Andres, you\\'ll have to try and make it without me.\"\\nSegretti criss-crossed the country over ten times in six months--and never stayed anyplace over a night or two.  Switch to another station, huh? You\\'re driving me crazy with that.\\n', \"Why? What's wrong with my teeth?\\nNo, actually. I don't know how to put this really. Well, there have been fabulous advances in the field of dentistry.\\nLet me guess. The floss is garotte wire, the toothpaste contains plastic explosives, and the toothbrush is the detonation device.\\n\", \"Is there a woman?...Tell me the truth...  There is a woman.\\nNothing, there's nothing. It's all out of focus.\\nNow what can you see?\\n\", \"No, Mom.  It wasn't a plane.\\nWhat do you think he saw?  Could it have been something from the air base?\\n\", 'Thanks, I appreciate that.\\nYou may you find all the solitude you want.\\n', \"Me?  Internal medicine.\\nEverybody's a doctor around here. This apartment house is all green pajamas and slippers.  The guy I'm waiting for to vacate is a doctor. What kind of doctor?\\nHow'd you know?\\n\", 'Good night...Barbara...\\nGood night, mother, good night.\\n', \"I am?\\nListen Meurice, you're gonna help me with a problem.\\n\", \"Definitely.\\nReally?\\nSounds like kind of a good idea.\\nNo, Doc, a private secret! It's perverted, it's pitiful. What am I -- Dr. Frankenstein? Aren't you repulsed?\\nAnd that's your secret?  You meant -- like a trade secret?\\nYou don't think I'm insane?\\nHow's it coming?\\n\", \"I'm just going to get some ice.\\nNot you, Sugar.\\n\", 'Thanks.\\nYou look great.\\nHi, Ted.\\nHey.\\n', 'Only me --  -- and Dr. Wynn.\\nNo -- there had to be someone else. Who knew?!\\nNo one.\\nWho else knew I had the baby?!\\n', \"You want me to write down all your messages?\\nTwo?  That's it?\\nYeh, I wrote a couple down.\\nSo -- any messages?\\nGroovy.  You should do more of this stuff.\\nNo, they were prescription, so I could see all the fishes properly.\\nThere's something wrong with the goggles though...\\n\", \"My lips are sealed.\\nBut who could've ... no, don't say it.\\nWell, um... funny thing, your penguins... they're not responding to the launch command.  Fact they're kind of turned around now... Like someone jammed our signal...\\n\", 'SHUT UP!  Are you serious? Of course I will...of course.\\nTeddy? Would you please please please take me to the prom?\\n', \"It will.\\nPaul, this will be our legacy.\\nI'm glad you like it.\\nIt WAS Windthorne. I knew it--what does that do to her love for Ian?--  --of course, if she hadn't thought Windthorne was murdered she never would have fallen in love with Ian in the first place.  Sorry, it's just that this is so wonderful.\\n\", \"We all have, he's got nine lives, or he's bulletproof, or some damn thing.\\nWade's right, it's some kind of scientific, magnetic thing, I can't explain it, but I've seen it.\\n\", \"I'm not proud of what I did. But *you*.\\nI, uh...\\nI'd understand if you'd walked in here. Socked me in the nose. Whatever. I deserved it.\\nBig Dave--\\nWhat kind of man *are* you?\\n...Huh?\\n...What kind of man *are* you?\\n\", \"I was only trying to be helpful. He was having difficulty with a problem.\\nThe Professor's secretary says she found you in Barnhardt's room, making marks on his blackboard.\\n\", \"Same old stuff.  I'm watching a man who won three Overseas Press Awards pitch an hors d'oeuvre idea.\\nHi, Aaron...What's doing?\\n\", \"Where's your friend? He go with the chiquita?\\nGot a little lucky.\\nYou a good pool player.\\n\", 'I am uneasy with that being our only hope of more information.\\nSpock? Concerned about his chances?\\n', 'Then we will simply alert him.  Beloved, are you certain he still wants you? After all, it was you who did the leaving in the Fire Swamp. Not to mention that pirates are not known to be men of their words.\\nYes.\\nI could never cause you grief; consider our wedding off.  You returned this Westley to his ship?\\n', \"You wake me out of a sound sleep at four in the morning and then tell me I look like hell?  Of course I look like hell, you don't look so hot yourself, Jack.  I'm freezing here, thank you for checking, can I go?\\nYou don't look so good.\\nStop asking me that.  I'm fine.  Who called you?\\nCan we come in?\\n\", 'Too late for that sir.  Give my love to the misses.  Get those bastards!\\nJenkins.  Can I get you some help?\\n', \"You want fair pay, make hamburger for Mickey D. Otherwise, please to sign.\\nYou're kidding, right?\\n\", \"Most articles focus on the first half of her name--describing some feline monster.  I want the woman of Catwoman. After all, if it was a man dressed as a cat, the story would be on page 23--just another loony. Oh, I want this one. I want her bad..\\nYeah, she's okay.\\nWhat is it with women and Catwoman? Men have the courtesy to punish the weak, but women love punishing the strong. Don't get me wrong--this Catwoman is a terrifying, subversive menace to everything this community stands for and she must be stopped. It's just, I like her a lot.\\nI don't know what came over me.\\n\", \"Sharp. Sometimes I'm so sharp it's frightening.\\nYou did?\\nIt takes a gimmick, Animal, and I figured us a little gimmick.\\nHow? Pinky Miller from Barrack 8 tried to get over there and they shot him in the leg!\\nSure, Animal! I'll get you over there!\\nYou'll fix me up!\\nCut it out. Animal! I'll fix you up with a couple of those Russian women!\\nNot for me! Betty! Betty!\\nSo what? There's other women!\\nLook at her! Isn't she beautiful! Married an orchestra leader!\\n\", 'A dollar? .. Not worth giving up for a dollar ..\\nSomeone bet me a dollar ..\\nHow\\'d you do it, old man?\\nI don\\'t mean this \"system\" shit that keeps you sucking, I mean <u>stop</u> .. I was exactly like you are .. I used to wake in the night - heart going so hard I coulda made love with my left tit .. If I can stop, <u>you</u> can ..\\nI <u>am</u> stopping smoking ...\\nYou gotta stop smoking ..\\n', \"Yes, that's why I said it.\\nI believe that, don't you?\\nIt's a saying.\\n'Love will find out the way'?\\n'Love will find out the way'.\\nWhat will we do?\\n\", \"It's not my body I want to save. It's my soul.\\nI know, Jeanne... it was the only way to save you from the fire!\\nAnd you?  Why did you lie?  You promised I could be confessed...\\nI don't understand, Jeanne... why did you do it?  Why?\\n\", \"A shark attacked Thea and some kids...She's going after it....\\nI hope she's a good sailor....\\n\", \"There's a message for you at the office. Your kid was hurt.\\nYeah?\\n\", \"Opening up just a little wouldn't kill you, ya know.\\nI don't feel like talking, if you don't mind.\\nWhere'd you learn to fight like that?\\nThere's always a choice.\\nHe didn't give you much choice.\\nI shouldn't have done that. I should've walked.\\n\", 'Sure.\\nCan I take your coats?\\n', \"Really.  Maybe that's why you're from Homicide.  How?\\nHe was murdered.\\nHow did he die?\\n\", \"Closer than that.\\nI hope you realize, Bill...in your office this morning, that was your time.\\nNo --\\nYour doctor?  Did your doctor say anything about a tiny, undetectable hole in your aorta?  Did he mention an irreparab- ly weak vein in the further reaches of your famous brain?  Were there any prognostications about the possibil- ilites of a fatal collision on a golf cart or suffocating in an avalanche on a ski vacation in Gstaad?\\n...I just saw my doctor, he told me everything was fine.\\nThat's the best I can do.  ...but minute-by-minute, I find myself lingering.\\nWhen you go, I go.\\n\", \"He's all yours. I was holding him back.\\nRogers Hornsby?!?\\nWait! Please wait!  I'll make a deal with you!  I'll give you a Rogers Hornsby, if you'll take me to the hotel!\\n\", 'Hello, Mrs. Robinson.\\nSay hello to Mrs. Robinson, Benjamin.\\n', \"Oh, to hell with it, never mind. Don't tell her anything. Don't even tell her you saw me.  Got to run, Buddy, I'll miss my plane.\\nWhat do you want me to tell her, Dave?\\nYou might see her. She writes your Daddy, and she and that doctor came to see him, didn't they? I wish you'd tell her something... I never could write letters.\\n\", \"The little creep hates it that Eric actually does what the company hired him to do.\\nBest thing is, Meyerling has to chase around to find us.\\nI call it 'the Turtle,' as in carrying your home on your back.\\n\", \"The navy will get us to within 400 miles of the Japanese coast.  We'll launch off the carriers from there.\\nAnd where's the secret base, Sir?  The one we t-takeoff from.\\n\", 'Hi, hi, hi there, my little droogies.\\nGood evening, my boy.\\n', 'He was the tragedy -- the tragedy of this war.\\nWho is he?\\n', \"Behave!\\nKiss me.\\nThat's fab, because I love you, too, Vanessa.\\nI love you, Austin.\\nLay it on me.\\nI have something to tell you.\\n\", 'She died a long time ago.\\nWhat about Mrs. Marschal?\\nMr. Marschal gets real lonely.\\n', \"You look!  It's not here!\\nJohn Whorfin will kill us!\\nNot here!  No Overthruster!\\n\", \"That's what I been telling everybody!  Wanta dance?\\nYou need a nickname.\\nEbby Calvin LaLoosh.\\n\", \"No, this is Jack.  Jango was hit by a UPS truck.  Can you believe it?\\nIs that Jango?\\nHer name's Hannah.\\n\", 'What about the last of the big-time spenders.  You make him?\\nPolicy man in Queens.\\nWho is that guy?\\n', \"I know.\\nI'd rather drive up myself and... maybe go into Canada after... And I can't stay long, Tita, probably a week, at the most.\\n\", \"That's right.  So we put up welded barricades at these intersections...  ...and seal these ducts here and here.  Then they can only come at us from these two corridors and we create a free field of fire for the other two sentry units, here.\\nWe gotta figure on them getting into the complex.\\nAll right.  There's a fire door at this end.  The first thing we do is put a remote sentry in the tunnel and seal that door.\\n\", \"Take no offense Trevor. I see many patients a day and have an awful memory.\\nI've been in here before.\\nHave we met?\\nAmbrose. I know.\\nHello I'm Dr...\\n\", \"No.  Maybe I believe God has a bigger dream for me than I had for myself.  Maybe I believe the journey, the big adventure, never ends...\\nBecause you have hope that you'll get better?\\nIt's gone now.\\n\", 'Not in a thousand years.\\nTell me, Cindy. Would you ever tell me \"Stop. If you loved me you\\'d stop.\"\\n', \"I appreciate you telling me.\\nI didn't want to be going around your back.\\nDon't know if I'll still want it.\\nHow 'bout you?\\nYou'd do a good job.\\nYeah.\\nThey want you to stand for Sheriff next election.\\nSam? I--the Committee--you know Jorge and H.L. and all--they asked me--\\n\", 'A secret service agent. It must be.\\nWe checked the manifest. Everyone was accounted for.\\nWho did this?\\n', \"Just warmin' up.\\nGiving up?\\n\", \"Surgical assassination?! But that means you suspect --\\nThey know they failed to kill Benes. Security thinks they'll try again, first chance they get. We're afraid of medical sabotage -- or surgical assassination.\\nAt an operation?\\nWe need you for Security purposes, Mr. Grant.\\n\", \"Nixon.\\n'68.\\nWhen?\\nWho'd you vote for?\\nSure.\\nRepublican?\\n\", \"All the insides are gone!\\nWould you like part of this? ....It's not much.\\n\", \"I'm not interested yet.\\nI'm not interested in a woman who's interested in that boy.\\nWait, Crash--don't go--all I want is a date.  I'm not gonna fall in love with you or nothin'.\\n\", 'I would.\\nYou sure?\\nNegative.  The T-1000 will definitely try to reacquire you there.\\nCan I stop by my house?\\nWe have to leave the city, immediately.  And avoid the authorities.\\nWhere we going?\\n', \"I know how he feels.\\nOkay.  Look, if it helps, Bud hates himself for what he did.\\nI work for Patchett.  I had a feeling that there was someone else, but I never knew who.\\nA police captain.  I think he's behind all of this.\\n\", 'A nickel.\\nRight!\\n', \"Well, I'm-I'm gonna take another in a series of cold showers.\\nWhere are you going?\\n\", \"No. I'm just not tired. Let's go.\\nAre you angry at me?\\n\", 'Well, this was another way to go.\\nNo.\\nYou never had The Talk, did you?\\n', \"That Dillinger's on to him.\\nOf what?\\nTo warn him.\\nWhat for?\\n\", \"Everybody needs a hob...\\n<u>Bobby</u>...\\nWell, yeah, I'm fine, I reached over to, the girl had to be home, I don't know, it's a <u>schoolnight</u>, something...\\nAre you Okay?\\n\", \"I don't know.\\nBullshit.  Bullshit. You know the answer to every goddamn question and I knew the answer to those questions and I'm not half as smart as you are so What Happened?\\nI didn't know the answer --\\nWhy didn't you answer those questions?\\nI'm fine. nothing.\\nWhat's the problem, what's the problem here?\\n\", 'I\\'m waiting for your offer, Clarice. Enchant me.\\nA moth... How did you predict that?\\nWas it a butterfly?\\nShe had an insect deliberately inserted in her throat. That hasn\\'t been made public yet. We don\\'t know what is means.\\nMmm. And what else...?\\nThey all were.\\nBig through the hips. Roomy.\\nYes.\\nLife\\'s too slippery for books, Clarice. Typhoid and swans came from the same God.  Tell me, Miss West Virginia - was she a large girl?\\nBy the book, he\\'s a sadist.\\nThat is both impudent and untrue... Tell me, how did you feel when you viewed our Billy\\'s latest effort?  Or should I say, his \"next-to-latest\"?\\nI was your choice, Dr. Lecter. You chose to speak to me. Would you prefer someone else now? Or perhaps you don\\'t think you can help us.\\n', \"Because he's what we say... &quot;connected&quot;... You wait, see what happens to those guys from Ninth Avenue.\\nWhy?\\nI wish they had.  He takes fifty dollars a week from my father's cash drawer.  But you can't kill a man like Fanucci.\\nIn Sicily, when you attack a man, you had better finish him.\\nNah.  Those guys aren't murderers. They wanted to scare him, that's all.  Make him look bad.\\nNo, I didn't know.  Is he dead?\\nSome guys from Ninth Avenue jumped Fanucci today; slit his throat from ear to ear.\\nWhat?\\nI bet you can't guess what happened?\\n\", \"Here I am.  Shall I come in?\\nBut I wasn't sure you'd come.\\nYou could have called me yourself.\\n\", 'Here.\\nIf you could see your way to lending me some cash -\\n', 'Adds flavor....\\nAre you all right?\\n', \"Eh...  Five dollars more.\\nHow much more a month?\\nI've already rented it; I cannot disappoint the new tenants.  They're paying a higher rent.\\nI told her I would speak to you, that you are a reasonable man who acted out of some misunderstanding. She has gotten rid of the animal that caused all the trouble, so why shouldn't she stay.  As one Italian to another, I ask you the favor.\\nI have already rented the apartment to another family.\\n\", 'Make me a woman!  Yes!  Make me a woman!\\nOh, baby!\\nOh, yes!  Yes!\\n', \"Then you're going to have to dye it, Anthony. We've got to hide our identities. Especially after Bob crashed the car.\\nDignan, I can't get my hair cut. That's just not possible, all right?\\n\", \"Knock yourself out.\\nThey'll probably make a run for the border, which would bring 'em this way. And if we get our hands on those shit asses, we're talking payback time. We'll get 'em all right. I gotta piss. I'm gonna use your commode.\\n\", \"Honey, we're goin' to bed now and it's time to change the subject.\\nHonest, Lula. I prob'ly ain't precisely got all the facts straight, but it's about what they said.\\nSailor Ripley! You stop! You're makin' this shit up and I ain't gonna sit for it!\\n\", \"You've done without it long enough, ma.\\nWell I don't know --\\nAh, hell I'll get one soon enough.  Besides, I want you to have it.\\nDon't you want it?\\n\", \"You've got a deal.\\nYes?\\nBill --\\n\", 'I am in a room with lights.\\nDo you know where you are?\\n', \"I don't know.  It's hard figuring you two as brothers.  Seems like the hospital might've scrambled the babies somewhere.\\nHis kid's sick.\\nWhere's egghead?\\n\", \"So work with me, not against me. Okay?\\nWe're a team.  We work together.\\nWhat I mean is --\\nNo. We are not supposed to be backing you up.\\nI mean, you're supposed to be backing me up, right?\\n\", \"I know, I'll lean forward and show more cleavage!\\nJust a couple more . . .\\nHurry up! I'm freezing!\\nStay still or they'll be blurry . . .\\n\", \"I'm alright. go hide. This won't take long. Be quiet.\\nFrank? .can you stand up?\\nOh shit.\\n\", \"Shep, no! I'll bet you have exciting things happen all the time down there.\\nYeah, something like that, but as I say, they didn't have time to tell me very much.\\n\", \"Did you change cabs?  It didn't work, something moved there--\\nWhat?\\n\", 'Tell me, how do you do that?\\nYes.\\nYou do the sawing of the box in half trick with you inside?\\nYes.\\n', \"C'mon.  It'll be great.\\nOh, I don't ...\\nGood ... Good ... Look, why don't you come over?  You know, nothing heavy, little drink maybe, little talk.  Just see how we both feel?\\nYes. 53\\nThat's so good to hear, sweetheart.  It really is. You know, I .. are you alone?\\nI miss you too.\\nI know.  I know.  It's bad.  I'm a bad person.  But I try not to be, Terri.  I really do.  And I really miss you.\\nWell ... of course I have.  I've thought.  I've ... Oh, JP, you were so horrible.  You really hurt me ...\\nHey, it has been known.  C'mon Terri, I'm not that bad a guy.  I have regrets. I'd like to put things right.  Don't tell me you haven't thought about me. Huh?\\nYou're apologizing?\\nNo.  I mean, really?  Because I'm concerned for you, sweetheart.  I care about you.  I guess I miss you.  I'm sorry we split up.  I'm sorry I ...\\nYeah really.  I'm ...\\nReally?\\nOh.  Yeah.  Yeah.  Well ... I'm fine. Things are great here.  Joey's going to get me a job at the TV station.  I'm meeting lotsa new people.  It's really great.\\nWill you relax?  Your little girlfriend left a card, remember?\\nYeah right.  How'd you get this number?\\n\", \"Mike. We don't have to watch it. Come on.\\nI don't know why we have to watch TV.\\nYou can change it if you want to.\\nWhat are watchin' this junk for?\\n\", 'Oh.\\nThe kids take me.\\n', 'I would have made you immortal.  Tell Burbage he has lost a new play by Will Shakespeare.\\nWill!\\n', 'Well, we sell other things too. Cool stuff. \"Man from U.N.C.L.E.\" Lunch boxes. \"Green Hornet\" board games. Shit like that. But comic books are main business. There\\'s a lot of collectors around here.\\nI didn\\'t even know they had stores that just sold comic books.\\nFour hundred bucks.\\n', '\\nWhoo-hoo-hoo!  Whoo-hoo-hoo!\\n', 'At any rate I arrived in town not ten days ago, full of dreams and aspirations, anxious to make my way in the world --\\nOh, Buzz is pretty harmless, really --\\nWhat a horrible little person.\\n', \"It won't leave much of a balance in the bank...  Mr. Robard?  Could you locate him?\\nPay the rent.  Let the tailor wait.\\nThe renting agent and the tailor.\\nWho else phoned?\\n\", \"Fair enough.  The river.\\nI've found a bungalow to rent up the towpath, Clive. I never want to leave the river again. The children have had such a wonderful summer.\\n\", 'Money thing is done in advance in places where there are no guns. Because when money and guns get together, there is violence...\\nWhat <u>you</u> get <u>from</u> us is a date.  A place.  \"In the parking lot of a Jack-in-the-Box in north Miami.\"  At such-and-such time, an eighteen- wheeler will be there.  Keys in the ignition.  Ready to roll.  You pick it up.  And you drive it...  ...away.\\nThe people on shore who handle a load we run are our people, not your people.  No tweakers, dopers, first- timers we don\\'t know.  They didn\\'t do time with us, they ain\\'t doing crime with us.\\n', \"What you find tonight on the floor between the seats?\\nFifty lire...\\nSignora Maria, don't do that. He's just a kid.  And why are you telling fibs?  We let him in free. He must have lost the money inside the movie theatre...  How much did you have?\\n\", \"We keep it quiet. It's not the kind of thing you'd want to talk about upstairs. Not with the press around.\\nSure. It's all on the Q.T., but everybody who matters is with us all the way down the line.\\n\", 'Give me a hand with these, will you, Ayegor?\\nYes.  I am a bit tired, after all.\\nReady, darling?\\n', \"Believe me, I don't blame you.  This is the Governor, Lynn. Say hello.\\nFor not believing you.\\nFor what?\\nI would also like to apologize.\\nI...1 would like to...thank you, Mister Wat... Gene.\\n\", \"Explosive. We're blowing the walkway.\\nBomb?\\n\", \"Okay, I won't.\\nDon't even think about it.\\nKaraoke -- perfect.\\n\", \"I said I never heard of him.\\nHis own people were selling him to a gang of Hungarians. Most likely the same Hungarians that Sate all but wiped out back in Turkey. The money wasn't there for dope. The Hungarians were going to buy the one guy that could finger Soze for them.\\nI never heard of him.\\nHe was a stool pigeon for the Justice Department. He swore out a statement to Federal Marshals that he had seen and could positively identify one Keyser Soze and had intimate knowledge of his business, including, but not exclusive to, drug trafficking and murder.\\n\", \"I was hoping you were gonna say an engagement ring, Claude.\\nSeason tickets to the Yankees. Right there on the first base line.  What's wrong, baby?\\n\", \"Will it be fun?\\nDelivering gossip and goodwill. It seems we'll both be doing Brighton.  Perhaps you should come too.\\nSorry.  Got held up.  What are you doing here?\\nFortunate that I was here to keep Susan entertained.\\n\", \"It's about time.\\nYou're vulnerable.\\n\", \"Now I want you to be ABSOLUTELY, totally, genuinely honest with me. Did you really, truly, honestly like it?\\nNo, no... It was very... er, nice.\\nWe're just not a very musical nation...\\n\", \"I don't know what good that'll do.  Chickens won't eat.\\nThere's nothing wrong with those chickens, Mitch. I'm going to call Fred Brinkmeyer right now.\\n\", 'Helen... excuse me, we...\\nShe wants me to check the phone booth for a note.\\n', 'Yes.\\nAre you busy tonight?\\n', \"I'm not putting him down, I just have a commitment to the truth.\\nShut up!!!\\nHe should have kept his head tucked down.\\n\", \"No, sir.\\nUnless you'd like to stay.\\nYes, sir. But Captain... are we... all going back?\\n\", \"Well, thank you very much, and please let us know if there is anything we can do to make your trip more comfortable.\\nNo, of course, Captain, I can understand your concern.\\nWell, fine. Thanks very much, anyway, and I hope you don't mind me asking?\\n\", \"Just waiting for you, tubs.\\nIS THERE ANY PARTICULAR REASON WE'RE NOT OUT OF HERE YET?!\\n\", \"Yeah, and he's lost a lot of blood. I'm afraid it's hit more that just the lung.\\nIt's in pretty deep.\\n\", \"I'm not jumping blind.\\nNot yet.\\nWhere?\\nWe like you.  At least the computer at Langley likes you.  Pulled your file because of various factors.  Service record. Area familiarity.\\nWhy me?\\nClassic special forces op... hit fast... in and out.  Two men.  Two days.\\nWhat's the job?\\nI'm authorized to get you out of here.  I thought that's what you wanted.\\nI don't work with spooks.  Not after that op in Cambodia.\\n\", \"Good point.\\nWait!  How are you going to know they're all following it?\\nI think the ground's getting closer.  I think we do it.  We're gonna save our asses here!\\n\", \"Don't move.\\nEthan -- Ethan, it's okay.  It's Claire. Ethan what's wrong with you?\\n\", \"Yeah Tell him to lay off Kip and them Tell him it's on\\nYeah -- ?\\n\", \"I'm sure I will.\\nWell, it was nice meeting you. Enjoy the rest of the trip.\\n\", \"Nope. I told him that you absolutely, positively would not be here at this bar between ten o'clock and eleven o'clock tonight. And then he came anyway.\\nKarla! Did you tell him I'd be here?\\n\", \"I'm not sure I...get your drift.\\nDid you ever have a roll in the hay?\\n\", \"I won't be a moment.\\nWho brought me in here?\\n\", \"My brother's wife.\\nThank you.  But who sits there?\\nAnd that --  is my chair.  And this --  is Miss Connell -- who is beautiful.\\nI like her already.\\nNo -- she just runs the place. She's everything else -- amazing woman, mother.  You'll like her.\\nIs she a doctor?\\nAnd that chair --  is the particular property of Mrs. Rand -- mother to both of us and much too good for either of us. Too wise, in fact, to live under the same roof. She prefers the village dispensary.\\nYes -- on the boat.\\nIt seems we are having dinner by ourselves, Miss Connell.  But I may as well introduce everyone to you, anyway.  There -- in the master's chair, sits the master -- my half-brother Paul Holland.  But you've already met him.\\n\", \"I know, sir, but it's the principle of the thing.\\nLet him have it.  You're winning anyway.  It doesn't make any difference.\\n\", \"The neighbors from hell.  The kind that lay in wait.  I'd rather move actually. Wouldn't I?  Wouldn't I?\\nWhat's that?\\n\", \"It's not ours to decide. All that live must die. It's God's will.\\nHow could all my father's knowledge and skill fail to save her?\\n\", \"This is a joke, right?  You just want to rattle me.  Right?\\nHow do you know if you've never tried?\\nI've done a few things but I'm not a murderer, Mr. McKenna.\\n\", 'You did, did you?\\nOfficer, I killed those people last night.\\n', \"Point conceeded, Mr. MacLeod.\\nYou're no match for Scot, Mr. Romirez.  We're raised as riders.\\n\", \"I understand.\\nI don't know what we're facing.\\n\", 'My favorite.\\nRaptors, mostly.\\nAnything good?\\n', \"That's my sea anchor.  My second one. Made it out of part of the sail.  It keeps you from capsizing in a storm.  In theory.  And this, this I used to collect water. About half a cup a day.\\nWhat's that?\\n\", 'What... whattaya do?\\nYes.\\nAre they going to torture us?\\n', \"It might behoove us to turn back at this point.\\nNo. Too... upright. Might've been a person.\\nA deer?\\nI don't know. Something.\\nWhat?\\nDid you see that?\\n\", \"Let him get some sleep.  He's going to need it.\\nMaybe we should pay Luther a visit.\\nYou missed ... Luther took a taxi to the hotel across the street. Made a phone call.\\nWe missed.\\n\", \"Yes. I mean the real McCoy. Listen, you play saloon with me, and I'll introduce you to every wit, every nit-wit, and every half-wit in New York. We'll go on a twister that'll make Omar the soused philosopher of Persia[7] look like an anemic on a goat's milk diet.\\nBinge?\\nWell, you'll not only see those, but before the evening's half through, you'll be leaning against the Leaning Tower of Pisa - you'll mount Mt. Everest. I'll show you the Pyramids and all the little Pyramiddes, leaping from sphinx to sphinx. Pal, how would you like to go on a real, old-fashioned binge?\\n\", \"I am -- I'm Captain B.L. Willard. This is Chief Warrant Officer Phillips -- it's his boat. We were shot up bad downriver and need repairs and food -- we can pay you in gold.\\nI --\\n\", \"And he left when he found out what Sethe did?  What you say casts a different light on it, I guess...I thought-\\nSure he knew her. Her boy Halle, too.\\nHe knew Baby Suggs?\\nHe didn't know nothing. And nobody. Except her, from when they was at that place Baby Suggs was at.\\nYou didn't tell me that. I thought he already knew.\\nI told him...Showed him the newspaper. About Sethe. Read it to him. He left that very day.\\nYou?\\nI run him off.\\nWhat run him off? Tell me that!\\nWhat's any of that got to do with Paul D.?\\nI ain't got no friends take a handsaw to their own children.\\nElla.\\nTill she showed herself.\\nYou was friends.\\nAnything white floating around in the woods - if it don't got a shotgun, it's something the Lord tells me I don't want no part of.\\nAw, no, Ella.\\nI ain't sure I know that. Baby never laid eyes on her till she showed up here. And how'd she make it and her husband didn't? And where is he? And how she have that baby in the woods by herself? Said a whitewoman help her. Shoot. You believe that? Well, I know what kind of white that was.\\nYou know she married Baby Suggs' boy.\\nWell, who can tell what went on in there? I never even knew who Sethe was or none of her people.\\n\", \"Then close your eyes, and tap your heels together three times.\\nYes, I'm ready now.\\n\", \"Well, not just one wish. A whole hatful, Mary. I know what I'm going to do tomorrow and the next day and the next year and the year after that. I'm shaking the dust of this crummy little town off my feet and I'm going to see the world. Italy, Greece, the Parthenon, the Colosseum. Then I'm coming back here and go to college and see what they know . . . and then I'm going to build things. I'm gonna build air fields. I'm gonna build skyscrapers a hundred stories high. I'm gonna build bridges a mile long . . .\\nWhat'd you wish, George?\\n\", 'Kris Kristofferson.\\nYeah. Yes. I don\\'t follow music too much.\\nThe singer?\\nOh. Who was that again?\\nI didn\\'t mean that, Travis. Just the part about the contradiction.\\nI\\'m no pusher, Betsy. Honest. I never have pushed.\\nThat song by Kris Kristofferson, where it\\'s said \"Like a pusher, party truth, partly fiction, a walking contradiction\".\\nWhat?\\nYou know what you remind me of?\\nIt keeps ya busy.\\nYou must be rich.\\nSometimes 76 or 80. Sometimes I squeeze a few more hours in the morning. Eighty miles a day, a hundred miles a night.\\nYou mean you work seventy-two hours a week.\\nI work a single, which means there\\'s no replacement -- no second man on the cab. Six to six, sometimes eight. Seventy-two hours a week.\\nWhat hours do you work?\\nWell, no... not really... had some famous people in the cab.  I got this guy who makes lasers. Not regular lasers, not the big kind. Little lasers, pocket sized, small enough to clip your belt like a transistor radio, like a gun, you know. Like a ray gun. Zap.\\nThat\\'s very romantic. Some of your fares must be interesting. See any stars, politicians, deliver any babies yet?\\nI didn\\'t mean you. But just ordinary people. A guy I know -- Dough-Boy -- met his wife that way. They got to talking. She said she usually caught the bus so he started picking her up at the bus stop, taking her home with the flag up.\\nWell, I don\\'t go to the Plaza every night.\\nYou\\'d be surprised how often you see the same people, get the same fare. People have patterns. They do more or less the same things every day. I can tell.\\nThat would have been quite a coincidence.\\nSouth Bronx. The worst. I tried to ditch him, but he was already in the cab, so I had to take him. That\\'s the law. Otherwise I would have picked you up.\\nThe DMZ?\\nIf it wasn\\'t for a drunk I would have picked you up. He wanted to go to the DMZ.\\nYou\\'re right! Now I remember! It was after the Western regional planners were in town and the meeting went late. The next day I was completely bushed. It was unbelievable.\\n', \"'Fraid not, baby. He belongs here. With me. We're the same. Brothers. Equal and opposite. Pure appetite. Pure banality. Too much feeling. None at all.\\nNo! He SHOULDN'T be here! It SHOULD'VE been a trick!\\nSee? He's here. You should learn to believe your Uncle Frank.\\n\", \"Franz!  Help!  Lunatic!\\nGRRRHMMNNNJKJMMMNN!\\nNow just one moment.  There's no need for roughhousing.  Have you ever tried a tip?\\nFoooooood!\\n\", 'He did the best way he knew how, Junebug is stubborn just like you.\\nHe never showed it.\\n', \"Oh, stay...  Don't you want to join the chorus of praises for Narcissus' glory?  Just remember, he is a married man.\\nI was suddenly thinking about going to bed.\\nMore wine, sister?  Surely you can drink more than that.\\n\", 'Now play the volume as loud as you want but don\\'t touch my levels. I got them set just the way I want \\'em.\\nOkay.\\nYou ain\\'t got to do nothing. Just point at it and push the button. You\\'ll hear the car go \"bleep.\" That means the alarm\\'s off and the doors are open.\\nWhat do I do?\\nThis one\\'s for the ignition...  ...but you gotta hit this thing to shut the alarm off and unlock the door.\\n', 'I just buzzed him in.\\nWhat do you mean \"on his way up\"!?\\nActually, I was just checking to see if you were here - your friend Seymour is on his way up.\\nI-I\\'m not sure... yeah, maybe.\\nShe understands what you\\'re going through and she really wants to help you. She says that job at Computer Station is still available if you want it.\\nI was in a horrible mood - tell her not to worry, I\\'ll be completely out of her life in a few days.\\nYeah, I heard about that.\\nAbout what?\\nAre you going to yell at me?\\nPumpkin, are you in there?\\n', 'Where are you hurt, son?\\nOh, my God!\\n', \"You mind?\\nYeah? First door on the right. It ain't that dirty.  Just kinda' filthy is all.\\nYou got a bathroom I can use before we hit the road?\\n\", 'I order you not to!\\nIt must end here... or I am the future.\\n', \"Scent glands.  Insects use 'em to identify themselves to each other.\\nWhat are you putting that crap on me for?\\n\", \"Then I won't bring you....\\nBring your friends.... We're not his friends.\\n\", \"Thanks.\\nYea, OK. I will be, I will be, I promise.\\nLook I don't care about the money right now: I just want some sleep. So if you could be quiet -\\nTen dollars. I'll find a way to pay you back. How would you like to know About somewhere special? The perfect beach. Paradise. No one else knows about it. That's got to be worth something. What do you say?\\nYeah, that's right. Now could you be quiet so I can get some sleep.\\nYou. You're the guy that lent me the money.\\n\", \"Why?  Oh, I'm sorry.  Are you two, uh - - are?  I wish you the best luck.\\nWhy?\\nBy the way, Dr. Sattler - she's not like, uh, available, is she? - -\\n\", \"We can protect you.\\nIt's a death sentence.  I'll never make it to the trial.\\nOne chance here, Eduardo.  Make us believe you got a boss.  No boss, it's all on you.\\n\", 'What does he know about it?\\nOur field.\\n', \"Oh, all right--  See you later, Toots.\\nH'mm ... no wonder I have trouble rounding up this show--Don't you know there's a rehearsal going on?-- And you wanted a raise.--Come on--get going or you'll get it!\\n\", 'Bugger the Buffs.\\nYou always were, Clive. Steady the Buffs.\\n', \"You know I don't like to wear any underwear, don't you, Nick?\\nI don't know anything that isn't police business.\\nYou know all about me.\\nHow do you know all this stuff about me?\\n\", \"Thank you...\\nI'll take you to your machine tomorrow.\\n\", \"And what do they say?\\nWe've got three.  Guy mowing his yard, couple of kids playing hoops.\\nWhat about witnesses?\\nNot that stuff about her running away, him trying to find her.  That's bullshit.\\n\", 'Yeah.  Pull in here.  Park it.\\nWhere?\\n', 'Everyone else was wrong and the one fucking lunatic was right!\\n\"The killer wasn\\'t Chinese\"... Cecil Stipe was right. .!\\n', \"Oh, yeah. Yeah, I'm a believer. She was there.\\nYou think she was really there?\\nThen we're all meeting your grandmother for the first time.\\nWhat past? She has never once, not once, ever said a word about being on the Titanic until two days ago.\\nMaybe she wants to make peace with the past.\\n\", \"He had the detonators!  Theo?  Theo!\\nHe wasn't lying about Marco:  He's thirty stories down on the street. The other man is Heinrich, and I found his body upstairs.  And his bag is missing.\\n\", \"Sure you did.  Help me with this hold cover, willya?\\nNaw, I had that around for a while.\\nSee you replaced a mooring line, lately.  This one's new.\\n\", \"I'm his Lt. He's my responsibility. I'll handle it. Me.\\nWe gotta go to Rimgale, Stephen.\\nI'll handle it.\\nWhat are we going to do about this?\\nAnything else?\\n\", 'Vell.  Goodnight.\\nTony and China?\\nThey seemed, closing by much\\nTwo of your friends left earlier MARK Uh huh.\\nUh yes, thank you.  It was quite enjoyable.\\nI hope you enjoyed your visit.\\n', \"Right, Gordon. We'll be in touch.\\nGOOD LUCK, CHET.  SAM, YOU STICK WITH CHET, HE'S GOT HIS OWN M.O. MODUS OPERANDI.  YOU CAN REACH ME AT THE PHILADELPHIA OFFICES. I AM FLYING OUT TODAY.\\n\", \"Ah, she's okay.\\nIs that old crab still with you? Mary, you said you were putting her up for a month -- it's been a year and a half.\\nI don't know, it was complicated. He's in San Francisco, I'm in Miami.  Besides, Magda's psychic dog hated him.\\n\", 'It\\'s vile!\\nTime out.\\nYou\\'re going to be hungover for three days. Like those guys on \"Oprah\" that get drunk and have disgusting sex with prostitutes and then say their vows with the stench of cheap hotel whore sex all over them.\\nYou expect more what?\\nHe\\'s weird. And I expect more from you.\\nHe\\'s a great chef.\\nWhy? What\\'s his problem?\\nHe just doesn\\'t talk a lot.\\n', 'I can? Okay.  Okay...\\nYou can!\\n', 'What are you doing here?\\nHey.\\n', \"It's not for the.  They only get to convict him.\\nI did.  But the jury won't s...\\nDid you write that column?\\n\", \"Because I didn't want to get hurt, taterhead.\\nYou could host American Bandstand in here.  Why did you duck at the auction, asshole?\\nFor those kind of wages, I could have built the factory in America! They're Vietnamese, can't we just give them more Bart Simpson shirts? I hear depressing news like this and I want to commit genocide!  Alfred, hold my calls.  So, Hawk! The Hawkster!  What do you think of the vehicle?\\n\", \"Reed, I'm not talking about Debbie.\\nYeah, you and Debbie and perfect --\\nYou go through something like this, makes you appreciate having the right woman in your life.\\n\", 'Eh? In here!\\nSir August ... ? Sir August ... ?\\n', \"'Be deliriously happy'.  I'm going to do my upmost --\\nBe deliriously happy.  Or at least leave yourself open to be.\\nThat's all?\\nWell, it worries me.  I want you to get swept away.  I want you to levitate.  I want you to sing with rapture and dance like a dervish.\\nDon't get dirty, Dad --\\nOh yes, I am.  Not an ounce of excitement, not a whisper of a thrill, this relationship has all the passion of a pair of titmice.\\nYou're not listening --\\nThat's for me.  I'm talking about you.  It's not so much what you say about Drew, it's what you don't say.\\nSo what's wrong with that?\\nListen, I'm crazy about the guy -- He's smart, he's aggressive, he could carry Parrish Communications into the 21st century and me along with it.\\nWhat were you going to say?\\nWell, I wasn't going to say that --\\nAnd I'm your daughter and no man will ever be good enough for me.\\nSusan, you're a hell of a woman. You've got a great career, you're beautiful --\\nUh oh --\\nDon't get carried away.\\n\", \"And the fact that you didn't know is basically the fault of yours truly. And even when you were throwing up, I could tell you cared.\\nMust be.\\nSee, I was always best looking after someone. Must be something in the genes.\\n\", \"Cover up that transmitter!\\nI did.  Hold the wire.\\nTell 'em to hold the wire.\\nThey got him!  Wait a minute -- hold the wire.  They got Earl Williams surrounded -- the Riot Squad has -- in his house.\\nWhat is it?\\nSssh. Wait, Fred.  What?... Where?... Where? Holy Moses!\\nAnd you can tell 'em as an afterthought that I want your resignation now!\\nAh, Fred --  Hello... this is Hartman --\\nTell 'em the party is through in this State on account of you.\\nWe got to think fast before those lying reporters get hold of this. What'll we tell 'em?\\nDementia praecox Oh-h-h!\\n\", \"Thank God!\\nDon't worry -- I'm fine.\\nI'll never forgive myself --\\nAnd the head.  You hit me, Dad!\\nOh, it breaks the heart.\\n\", 'Page...<u>three</u>.  Now:  \"It\\'s a nice evening.\"  I\\'m not gonna say that... \"It\\'s a nice...\"\\nWhat else?\\n', \"Not anymore. Oh, some time ago, my late husband owned a good deal of beach property in Long Beach, but we lost it.\\nWell you own a lot of land.\\nI'm not.\\nDid you know that you're a very wealthy woman?\\nWhy?\\nI've been wanting to meet you.\\nYes.\\n\", \"But, Amy!  Amy...I'm dead.\\nOh, it is, it is! Death's terrible.\\nAmy, listen to me. Death isn't such a terrible thing.\\nBut she's dead!\\nYou mustn't be afraid.\\n\", \"Your pain ends now.\\nNo, wait, no WAIT, that's too much, man, that's like overkill, nobody can take that much, you're wasting it -- !\\n\", 'What the hell is happening to me?!\\nMight wanna fasten your seat belt, Jack...\\n', 'Goodnight.\\nGoodnight.\\n', \"You're about to be a media darling -- you might want to dress the part.\\nWhat?\\n\", \"Surely you don't think Beverly was involved!\\nI know this sounds weird, Mr. Sutphin, but the Department of Motor Vehicle's computer shows only one blue station wagon registered to a parent of any of Mr. Stubbins' pupils.\\nDetectives, what is this about?\\n\", \"And you don't.  What's she doing in there?\\nYou do --\\nDon't call him a dipshit.\\n\", 'Excellent.  Bill, find Mr. Weathers a position to suit his condition.\\nNo, sir.\\n', \"The ones in this town, yes! You encourage their growth, their habit. You're the source in this area, and we're going to shut you down for good! For good, cancer-merchant!\\nLike I'm responsible for all the smokers!\\nWhat better place than this? To stamp it out, you gotta start at the source!\\nYeah, but not in here.\\nWe're not moving! We have a right, a constitutional right, to assemble and be heard!\\nThat's it, everybody out.\\n\", \"You'll be fine.\\nDo you have any particular choice of girl?\\nOkay. I'll take a half hour.\\nYou may do that in the privacy of your room.\\nI just want to ask some questions.\\nDo you want to take a session?\\nYeah.\\nWe offer Female Wrestling, that is, nude body to body contact, with a girl of your choice in a private room. Twenty dollars a half hour, thirty dollars hour. Any other arrangements may be discussed in the privacy of your room. Tipping is permitted. We accept Bank Americard, Master Charge and American Express.\\nNo.\\n\", 'Anderson.\\nWhat did you say your name is?\\nWalker and Williams.\\nWhere was that?\\nYeah.\\n', \"Right, whatever, have fun.\\nThat's what I'm trying to calculate. And it's not rubber.  It's muscle, tendon.  I seem to have the ability to manipulate the malleability of my molecular structure and redistribute my density to --\\n\", \"Rose, my patience is wearing thin. First a scruffy man who runs like a deer and now a sulky boy who wont go away. This is getting to be a regular monkey and dog show.\\nThat sulky boy was here again this afternoon. I was almost scared, he wouldn't go away.\\n\", \"I just gotta see a guy.\\nDon't go to the bar, Ray. I know him, that ain't a good idea.\\nSee a guy.\\nWhere... where you going?\\nYou can stay at my place, I'll drop you there.\\n...Drive me to a motel?\\n\", \"-- You split the team, man. And what was that crap with the standpipe? You'd think you and a hose were never introduced before.\\n-- C'mon, Stephen.\\nY'know, you got an awful short memory for direct orders. I told you to stay beside me.\\n\", \"Just an expression man, don't mean nothin'.\\nI can read a police file, shithead, and quit calling me Jack.\\nWhat gang you talkin' about, Jack?\\nLuther was part of the gang?\\nDon't worry, I got a move for ya. An awesome move. A guy named Luther. Ganz'll be paying him a visit.  We go to him right away.\\nOkay, let's get down to it.  I did my part and got you out.  So now you tell me where we're goin'?\\n\", \"What?!\\nActually,no. I was wondering why Cliff likes to wear another man's underpants.\\nHe's a former boyfriend. We lived together for about six months.  And yes, I'll admit it.  I've still kind of got a thing for him.  That's what you wanted to know, isn't it?\\nWhy, thank you!  Very nice to have met you, Cliff!  May I ask you a question?\\n\", \"Check it out.\\nIt's a damn convention.\\nWhat the hell's going on out there?\\n\", \"I guess we won't.\\nWell, I guess we won't be going to church today.\\n\", \"I know.\\nYou're in over your head.\\n\", \"Quick!  Run a trace on the culvert leading off the auto-shop maintenance pit.\\nComin' up the Central Reservoir.\\nWhere is she?\\n\", \"What is it, I can't --\\nI was scanning the horizon to see what I could pick up.  Look there, on that screen.\\n\", \"Just fine.\\nHow's your sister Pearl?\\nO, she's all right.\\n\", \"Bullshit...\\nThe pigs are gonna start flooding us with dope. Huey wants us to stop them.\\nWhat are you saying?\\nYou better just kill me Tyrone. And when Huey gets out, when Oakland's just wall to wall junkies, you tell him you blew away the only chance we all got. I'm sure he'll be real happy about that.\\n\", \"Don't you think it's about time you shut yours?  Who are you to tell a man like Frank D'Angelo to shut up?!\\nFrank, you don't listen!  J.J. just told you to shut your mouth!\\n\", \"How'd you and this detective come to trace her to Fairvale?\\nShe left Phoenix a week ago yesterday. And no trace until...\\nNow. Your sister is missing how long?\\nWill you help us? I think something's wrong out there!\\n\", \"I did steal a squirt of perfume. What do you think? It's Clinique Happy.\\nI don't get a klepto vibe from you. Evil genius? Maybe. Arsonist? Wouldn't rule it out.\\nWell, you don't just invite a random pregnant teenager into your house and leave her unsupervised. I could be a total klepto, for all you know.\\nWhat? No! Do we come off like paranoid yuppies or something?\\nDid your wife send you up here to spy on me?\\nSorry. I was just getting something.\\nWhoops! Yikes, I didn't expect to see you up here.\\n\", \"Applejack's stuck in the elevator?\\nYou're kidding.\\n\", \"Why should I? I know you're not a cop, so what is it tonight? Another two-fifty to watch you sleep?\\nDon't run away.\\n\", \"Mr. Sebastian is a host who wants to be appreciated.  We'll appreciate him and he'll cooperate.\\nIf he won't cooperate?\\nHe knows what he's doing.\\n\", \"Man take it easy!  You're sweating like crazy!\\nI should have known something like this was going to happen!\\n\", \"You watch.  One day, I'll hit the jackpot, get the big house, car, clothes.  I'll have more money than God.\\nYou know what they say.  You have the same chance of winning whether you play or not.\\nSometimes I can just feel the numbers.\\nLemme guess.  The lottery.\\nI plan to.  You want a tip?\\n\", \"I wish Uncle Ethan was here. Don't you, ma?\\nWhat, Ben?\\nIt's all right, ma...I been watchin'... Only I wish...\\n\", \"This is Jason Voorhees. They'll be looking for an excuse to send him to the chair.  If we're going to argue insanity, we're going to need something a little more concrete.\\nGreat. Battling head doctors. We'll confuse the jury enough to create a reasonable doubt. Hung jury.\\nThis is bad, Joe. The Prosecution is already putting together a team of psychiatric specialists. They'll argue exactly the opposite. They'll say he's just pretending to be catatonic and he's completely sane.\\n\", 'Maybe. Maybe not.\\nI hear the humming. Like electricity. High voltage maybe.\\nYou hear that?\\n', \"All them people shoutin' your name like they were doing tonight! Shit! That arm you got'll get'chu on a box of cereal...\\nI'm a what?\\nHoly shit, Ronnie!  You're a fuckin' rock star.\\nWhat?\\nHoly shit!\\nNo. I just got a messed up stomach.\\nAre you wasted?\\nWell. That's my vomit. I came in here to get sick. I thought I'd make the toilet but... anyway, I got sick.\\n\", \"Incredible.\\nI guess I didn't look. You know I don't pay attention to those things...\\nBut it can't be! We can't be out of gas! I filled it myself yesterday!  Wasn't it full when you drove to Brewster this morning?\\n\", \"You sure it belongs to his wife?\\nHe has his wife's jewelry hidden in among his clothes over there.\\nJewelry?\\n\", \"I'll got your money to you. No sweat.\\nFuck you.  You got no right for this kind of play.\\n\", \"What's the difference?\\nNo. No you said HER BODY's been missing.\\nI said she's been missing for-\\nWhat did you just say?\\n\", \"See ya 'round Faz.\\nNectar of the bowling gods, Faz.\\nThe Faz sure took care a her.  Breeze makes a nasal GRUNT as he tries to suppress a snigger. Sid - trying to keep a straight face - elbows him. Sid and Breeze approach the coffin. Breeze pulls an orange whip out of a paper bag and carefully places it on the coffin lid.\\n\", \"Longer?  It's getting shorter.\\nIt's not getting any longer.\\n\", \"Ninotchka, you know I am your friend, you can trust me.... Did you bring back anything else?\\nThank you, Anna. I'll dry it up here when I wash it next. I should hate to see our country endangered by my underwear.\\nYou know how it is today... all you have to do is wear a pair of silk stockings and they suspect you of counter-revolution.\\nI see.\\nWhen I passed through the laundry yard today I saw all the women huddled around this so I brought it up here. Things like this create a bad feeling. First they didn't know whose it was. Then they saw the Paris label and did it start a commotion! Some said it's what we all ought to wear and others said it's like hanging foreign ideas on our clothesline. It undermines our whole cause.\\n\", \"No. I won't be long.\\nTo see your father?\\n\", \"I don't fly in anything that doesn't show movies.\\nYou ever fly an F-14?\\nI'm still second best.\\nWell no...inverse order.\\nIn that order?\\nFood...and you...my F-14!\\n\", '-- This is Mr. Laszlo.\\nWell then, perhaps you also ---\\n', \"He wasn't superhuman Dewey. He wasn't superhuman at all.\\nWatch out Sid. Randy said the killer's always superhuman.\\n\", \"You are in no position to give orders, Dr. Jones.\\nThat's far enough!\\n\", \"I'm fine--but thank you.\\nBetty's making some pineapple kabobs ...\\nOh, no thanks ...\\nWant some bridge mix?\\n\", \"Oh... yeah.\\nNo. I guess you're here for the backpack.\\n\", 'Look Jack Im late.  Id love to help you out some more but I gotta go handle my business...  Happy trails.\\nBut what do I do?\\n', \"Looks like we're in a dead heat after one hole. This is turning into quite a rivalry.\\nAn eight.\\nWhat'd you get?\\nI'll give you an eight.\\nI don't know. Eight or Nine.\\nHow many strokes?\\n\", \"Walk? Naturally I can walk.\\nWell, if you're awright, why you hanging on the bannister. Can you walk or not?\\n\", \"Such a shame. You ought to take better care of yourself.\\nNothing's wrong with my skin.\\n\", \"She was too young for him.\\nA man who can't control his woman is funny.\\n\", \"No, no, not exactly. It isn't a farm in the sense that it's a farm. Not at all. It's a... dairy establishment. You'll 1ike it! I'm sure you'll like it because it's so... peaceful!\\nYou mean a farm?\\nAhh-hh, you'll like it. It's a fine, outdoor-type job.\\nWell. What kinda job is it?\\nYes, Tennessee, a lovely state!\\nTennessee?\\nWell, I have got news! I called long distance and spoke to Cousin Hop and you'll be glad to know, Rose, I have found you a job!\\n\", \"Quit it!\\t\\t\\t\\t\\t\\t\\t * \\t\\t\\t\\t\\t   * You're not a killer, Lenny. That's why\\t\\t\\t * you're so good at it.\\t\\t\\t\\t\\t\\t   *\\nI should kill you.\\t\\t\\t\\t\\t\\t\\t *\\n\", \"Prepare for a change in lifestyle.\\nI worked for him... still do.  Said he'd make sure I was left alone if I helped put Wade away.  He kept his promise for twenty years.\\n\", \"Don't know. Really. I grew up with him.  I've missed him too, but now, every time I see him, I get confused. Hardly a day goes by he's not in my mind. Even now, I feel he's here --\\nWhat about you?\\nI know.\\nHe -- loves you.\\n\", \"The payoff.\\nThen you tell me, what just happened?\\nEverything happens for a reason.  That's what my dad said.\\nWhat the fuck?\\n\", \"Mr. Chairman, the witness is being non-responsive...\\nWell, I know that I lost my job because of one meeting I went to when I was a kid in college.  I know that I've been branded a communist, which I'm not, but even if I was, it shouldn't matter, or what do we have a Bill Of Rights for?\\nAll right, Mr. Appleton.  That was what you knew then.  What do you know now?\\n\", \"Bet I do.\\nBet you say that to all the girls, Perfect Tommy.\\nThat's why I wear a fifty dollar hat.  Was a two hundred dollar hat, I hadda kill you.\\n\", \"Please.\\nDon't say it.  Orange juice?\\nFord Fairlane, I'm Colleen Sutton and I need your help.  I have a problem and it pertains to the music industry.  What is it they call you?  Mr. Rock and...\\n\", \"Do you want to deliver this baby?\\nI've got to stop and help those people.\\nPlease.  It's not going to wait.\\n\", \"I'll do my best.\\nI can't tell you how happy we are that Cecile is going to be attending Oakwood with you this fall. You've always been an inspiration to Beau and I on raising her. We just hope she can rise to the high standards which you've set for her.\\n\", 'Oh, no... you.\\nPardon me... Miss... ?\\n', \"You just can't, okay?  Trust me on this.\\nWhy?\\nWhattaya mean, why?  'Cause you can't!\\nWhy?\\nListen to me, very carefully, okay?  You're not a terminator any more.  Alright?  You got that? You can't just go around killing people!\\n\", \"And you'll go on saving me? Again and again?\\nI'm responsible for you now, you know.  The Chinese say that once you have saved someone's life, you are responsible for it forever. And so I'm committed. And I have to know.\\n\", 'Hunting -- we had to hunt him --\\nDo what?\\nHe -- He made me do it --\\n', \"No. I've never seen a mummy look like this. He's, he's still...\\nIs he supposed to look like that?\\n\", \"I know. That's why I'm asking.\\nThey're awfully heavy.\\nHow 'bout over there?  No wait. Do me a favor. Bring 'em to the back room.\\n\", 'Caesar, come here.  Sit.  We talk now.  You too, Johnnie.\\nPop?\\nNo, Johnnie.  No goddamned phones. Not now.\\n', \"I don't want to see her hurt.\\nI care for her.\\n\", \"Well, I don't want to put you out.\\nStranger my foot, it was my grandmother who helped you into this world.\\n\", \"She's lying, Norman.  Just like she lied about fixing your suit.\\nBut why would she -- ?\\nI was just in the cafeteria, there's plenty of food in there.  Take a look for yourself.\\nYou didn't say that about the food?\\nShe's cracking, Norman.\\n\", \"I don't care what you did for me. I don't think I want to know you anymore -- all you do is make me feel badly about myself.  You have my number.\\nI'll take you... why not?\\nI'll take a bus.\\nHere are the keys to my apartment. I'm going to park you in my place while I take Carol home.\\n\", \"I don't see how they could be in on it. They're the best. They're hand- picked.\\nNo! You can't! They're in on it.\\nAll right, let's just...let's get security in on this.\\n\", \"In an hour they're gone!\\nThey're rounding up a posse.  Figure an hour?\\nWhen are they coming?\\nThey're coming.\\nWell?\\n\", \"I thought so!\\nYes! That's it!\\n\", \"Okay.  I'm sorry I said your dream was stupid.\\nNo, let's go to work.  Okay?\\nI will.  I'm trying.  Meanwhile I got some crack left, you wanna get high?\\nI thought you were giving up that drug shit.\\nDon't be mad at me.  I'll pay you back.  I promise.\\nI just saw somebody pulled out of a dumpster.  I wonder how much she made tonight?\\n\", \"I'm a quick learner.\\nYou gotta excuse him. Yesterday he didn't know Pinot Noir from film noir.\\n\", 'Hello, Mrs. Brigman.\\nHello, Brigman.\\n', \"No no, you don't understand -- this is no ordinary oil -- this is miraculous... this oil was brought from heaven by a white dove to crown King Clovis in this... very... cathedral...\\nThirty years ago?  I'm not surprised it's gone...\\nWell... at the coronation of King Charles VI...\\nAnd when was that?\\nI don't understand... the holy oil of Clovis... it was quite full the last time I saw it...\\n\", \"Yeah.\\nWe're alright. Your Ma's sleeping. You want me to get her?\\nHow you and Ma doing? Haven't seen you in town for a while.\\nFrom Wickham's. Been a while. Like some coffee?\\nYou remember Margie Fogg?\\nSleeping.\\nJesus, Pop, how can you stand the cold, dressed like that? Where's Ma?\\n\", \"Bye.\\nGreat, bye.\\nOh, see?  This is great.  Ten o'clock.\\nIt's my favorite place --\\nYeah, You know it? You know Billingsley's?\\nBillingsley's?\\nNo, no, just casual maybe, maybe I thought -- there's a spot I like to go, it's real nice that overlooks a golf course and the course is lit up at night --\\nOh sure yes, that's fine, late dinners are good.  Should I get dressed up or -- ?\\nWhat about ten o'clock, is that too late?  I don't get off and then --\\nEight o'clock?\\nNo, I'm off tonight.  I would lov-like, to go tonight, I can pick you up, I can pick you up here at about what time?  What time?\\nDo you wanna go tonight?  I mean, are you working?\\n\", \"Reed, we're running out of time.\\nCome on, Ben, come on...\\n\", \"It's the truth.\\nI don't believe you, Margaret. Frank wasn't like that.  I'm the villain in the family, remember?\\n\", \"I, uh...\\nI guess you won't be coming home tonight.\\n\", \"Holy cow -- India? How do you know we're in --\\nIndia...\\n\", \"His blood's always up.\\nYou should stay away from a man when his blood is up.\\n\", \"This is different.  This is better. Think of when you're working out, Oz. You need a partner, someone to spot you.  Someone to keep you motivated.\\nDude, it's not like I haven't been trying to get laid.\\n\", 'None at all.\\nAny particular reason?\\nThat way.\\nWhich direction?\\n', \"In a week... we will never have had this conversation.\\nAll right.  I'll come for dinner. And in the meantime... you'll think about what we discussed?\\n\", 'It could have been worse, John.  It could have been a lot worse.\\nSo how much for our first tour.  Two no-shows and one sick triceratops.\\nVisitor vehicles are on their way back to the garage.\\n', \"Tell me!\\nI don't know.\\nNobody?!  What's her name?\\nNobody.\\nWho is she?\\n\", \"Well -- she's dead.\\nIt was Jane McKenna who sent you the dumper.\\nAbout the dumper, didn't he tell you that?\\nIt didn't go anywhere.  But that's not why --\\nDidn't he tell you what you wanted?\\nNo.\\nDid you like my friend Frankie?\\n\", 'Nothing in particular, just putting it away for a rainy day.\\nWhat are you saving for?\\nWell, I guess you found my secret hiding place.\\n', \"He's the dead guy.\\nI don't understand.\\n\", \"-- why not? C'mon, take me out of here, somewhere I don't have to lie to anybody and I'll fuck your brains out, you little fiend, I adore you --\\n-- we can't now.\\n-- Just get me out of here, baby. I can't stand it. It's killing me.\\n\", \"Let's see if I can bring it back to life.\\nDon't, Lee--that's dead.\\n\", \"They've already been identi- fied.  There's no doubt.\\nLieutenant, are you sure it's them?  Maybe I should see the ...bodies.\\nHere, drink some of this...\\n\", \"Hey -- you can't do that.\\nI'm going out there.\\n\", \"Maybe I'll drop in and see your Mommy.\\nI live right here.\\nNo, darling, I hadn't intended to.\\nAre you coming to see us. Miss Callahan?\\nHello, Amy.\\n\", 'Getting paid for doing shit.\\nWhat?\\nMust be nice.\\n', \"No -- comin' right at us! Slow ahead, he'll hit us head on --  Slower! Throttle back ---\\nComing right to us!\\n\", 'Hey.\\nHey.\\n', 'But different.\\nWeird.\\n', 'Shut up.\\nBe a Mohican.\\n', 'Wanker more like, what does he know about writing.\\nSon of the American ambassador and a banker - good enough?\\nWhat is it with that Josh guy? Who does he think he is shoving that Reflections rag down our throats?\\n', \"It's bad news when you can't even bribe kids.\\nShe's tired.\\n\", 'I...\\nSo you killed him?\\n', \"Bomb must have gone off inside the ship. Nothing we can do about it now. Hey, it looks like... the skipper. He made it. Commander Powell made it!\\nWhat happened, Doolittle?\\nHere I am. I think I'm spinning... We're both falling, Talby, in opposite directions, away from each other. My -- my jetpack's gone.\\nDoolittle, Doolittle, where are you?\\n\", 'After the cold war ...\\nLondon. The World Council of Ministers meets soon on global defence. If you can control the weather, you control the world.\\n', 'Zombies set to knock out local air defense four hundred miles from primary.\\nCheck.\\n', \"Advancement, of course.  Go to Split City. See Miss Mofet, an old patient of mine. M-O-F-E-T... Now go. Go.  I don't think Miggs could manage again so soon, even if he is crazy - do you?\\nWhat's that, Dr. Lecter?\\nNo. But I will make you happy... I'll give you a chance for what you love most, Clarice Starling.\\nThen please - do this test for me.\\nI would not have had that happen to you. Discourtesy is - unspeakably ugly to me.\\n\", \"We're too late. Take a look.\\nWhere is he?\\nWho're these guys?\\n\", \"No, baby... I'm glad you're coming tonight.\\nNo, that's okay, I'll do It now.\\nComb my hair out later, honey.\\n\", \"Uhh ... Wong, Henry Wong.  He was in on the same job.\\nWait a minute, wait a minute... who's this?\\nThey all pulled a bunch of jobs with Ganz about four years ago.\\nWho are all these?\\n\", 'Sluggish. Like a wet sponge.\\nSluggish. Like a wet sponge.\\n', \"Best not to ask.  Shouldn't even talk about it.\\nWho does?\\nHe gets one day of confession each year.  Today is the day.\\n\", 'More bullshit!  More bullshit!  What kind of odds are they giving me?  There must be some kind of office pool.  One month?  A couple of days?\\nI do care about you!\\nI trusted you!  I thought you cared about me?!\\nStop being nuts!\\nLet go!\\nStop, will you?!\\n', \"Well, let's help them with that. Take the legs.\\nThey don't even realize they should be frightened.\\nI could get used to this, looking down on people.\\n\", 'Hurry -- hurry --\\nCome on -- come on --\\n', \"Now, if you'll just point me in the right direction . . . This direction?  Good night, Mrs. Bailey.\\nOh, George!\\nWell, here's your hat, what's your hurry? All right, Mother, old Building and Loan pal, I think I'll go out and find a girl and do a little passionate necking.\\n\", \"Didn't think you'd use that fibrillator on Norris if you were one of them.\\nThank you.\\nI guess you're okay.\\n\", 'Sixty-three.\\nWhat year?\\nNash.\\nWhat kind?\\nDo you carry door handles?\\nParts.\\n', \"Did it ever occur to you guys that Joanna Kramer's not the easiest person in the world to live with?!  Did it?!  For one thing she's always thirty minutes late.  You can set your watch by it--\\nWhat d'you know about how Joanna felt?  You went off to an office every morning and you'd come dragging home at seven or eight every night and as long as dinner was on the table you thought everything was swell.\\nListen, Joanna Kramer's got a goddamn good life.  She's got a husband that loves her.  She's got a terrific kid.  She's got a wonderful home--\\nI mean...things.  Ted, Joanna's very unhappy and--\\nThings? What kind of things?\\nTed, Joanna and I used to talk a lot and...well, she told me a lot of...ah, things about the two of you.\\nHostile?  Me?  Thelma, I'm not hostile.  I am anything but hostile.  But if you want to know what I am.  I'll tell you what I am. What I am is, I am hurt.  I am very hurt.  And I just want to know one thing, okay?  Just one thing ...Why?  That's all I want to know...Why?\\n\", \"You're welcome.\\nThank you.\\n\", \"It's not too far away now.\\nWhat's so exciting?\\n\", \"One would assume so.\\nUntil you're on the net where the corporation can't touch you, you can't open the system.  They'll eject it if you do.\\n\", \"I'll miss you, Mike...\\nYou could do better.\\nHe's very caring, in his way.  You haven't seen him at his best.\\nYou asked.\\nTell it like it is.\\nWhat's to like?\\nYou don't like him, do you?\\nWhat about Neil?\\n\", 'No, no, whoa!\\nWhoa!\\n', \"What's in it for me?\\nI need a favor.\\n\", \"It's always been a source of deep regret, but the mountains surrounding us have made reception almost impossible.\\nNot even a radio?\\n\", \"So what do you got, a fuckin' Hyundai engine under there? Can I make it back to my house?\\nHad one, now I'm fucked again.\\nSo, you finally got a job Morgan?\\n\", 'Ok.  So, lemme just ask you a couple questions to start --\\n-- go, go, go.  I\\'m givin\\' pearls here. And I\\'II tell you samethin\\' else: I\\'m not succeding in the bush because I\\'m Frank TJ Mackey.  If anything, there are women out there that want to <u>destroy</u> me -- it makes it twice as hard for me, I run into some little muffin, knows who I am, knows my schemes and plans -- shit, she\\'s gonna wanna fuck around, prove to her friends, say, \"Yaddda-yadda-yadda, I saw that guy, he wasn\\'t anything, didn\\'t get me.\"  So me? I\\'m runnin\\' on full throttle the whole fuckin\\' time.  Dodging bullets left and right from terrorist blonde beauties. But I\\'II tell you this: The battle of the bush is being fought and won by Team Mackey.  Can I have a cigarette?\\nI\\'m gonna start rolling --\\nJust one look, one hesitation, one subtle gesture for me to know -- And Bing-Bam-Boom I\\'m away on a tangent -- I get so fuckin\\' amped at these seminars and lemme tell you why: Because I Am What I Believe. I am what I teach, I do as I say, I live by these rules as religiously as I preach them: And you wanna know what? I\\'m gettin\\' pussy left, right, up, down, center and sideways.\\nAll it takes is one second?\\n', \"No.\\nI'll ask you once, Nick -- for the record did you kill him?\\n\", 'MMmmm.\\nAll right if I turn out the lamp, sweetheart?\\n', \"Some other time.\\nDon't you want to see the rest of the exhibit?\\nCome on, Charlie. Let's go find another and better suspect.\\n\", 'Skipping stones.  Look how flat that water is, you can get ten skips on a good one.\\nWhat are you doing?\\n', \"You've been working out.\\nThis is wrong in so many ways.\\n\", \"Really? Who thinks that, sir?\\nSome people think you have an attitude problem, Beckett.\\nPerhaps... you're right. I've certainly been busy. With the Kronos complaint, a preliminary injunction hearing and the Saunders trial all falling at the same time...\\n\", 'Well, it happens.\\nI was so confused, so conflicted, so... unstable.\\nOh, yeah.\\nThank God you were here.\\n', \"Even better. Wayne call your station, tell 'em we're going live a little early today. Make it happen!\\nIt's video.\\nDo you have a back up.\\nNot good.\\nHow is it?\\n\", \"That's it, Adam, pretend it's one of those arcade things, the tighter you squeeze, the more of a man you are...Ooh, that's it.\\nYou think you're so...but you're just...\\n\", \"It's me.  Put the gun down.  Maya...\\nOh, Peter.\\n\", 'Aversion?  No... \"Hatred\"... \"Loathing\"...\\nYou still have an aversion to the water?\\nAnd by sea!\\nI\\'m embarrassed to be so indecisive... after so long apart and after you\\'ve traveled so far...\\n', 'Have mercy on this child ... let this child live ...\\nMother of God ...have mercy ...\\n', \"We're not moving fast enough for him.\\nWhat is it?\\n\", \"I would like to go now, please.\\nIf he had no idea, why did the Jews need saving? This is the question, Emmi, to all Germans: Why did the Jews need saving in this country? Why, if people had no idea?\\nBut he had no idea, a lot of people had no idea. I only realised what was really going on when I got arrested.\\nEmmi, stop! I want to show you something. Let me show you something and then if you want to leave, you can leave, please please. His friends, they did this. And he gave them birthday concerts.\\nI have been questioned by the Gestapo just like that. Just like you questioned him.\\nWhat's not right?\\nI can't do this. It's not right.\\nWhat is this, Emmi?\\nI'm sorry but I have to leave. I'll find other work. You'll have to get someone else, that's all.\\n\", 'So you became the hero.  And I became the schmuck.\\nShe was beaten, and bloodied, and it was going to go on, uni...\\n', \"Wear a coat of domestic mink.  For the love of God, though, Penelope, don't lightheartedly advertise that the last of the jaguars died for you.\\nWhat?\\nOh no!  Wear a coat of cotton--wear a coat of wool.\\n\", \"Point out all the lies and fucked-up thinking.\\nThat doesn't mean I don't like hearing it.\\nI thought you hated Torah.\\nTo hear them read Torah.\\nWhy did you come tonight? To see me?\\n\", 'Butt-ugly owl.\\nWe should be so lucky.\\n', \"That's the way of it, Fettes.  You bring the lassie to me.\\nI suppose one must pass through this purgatory to the heaven of being a good doctor.\\n\", \"Not to a nurse.  Sleep is a cure.\\nShe hated sleep.  She used to say it was a thief -- stealing away her life, an hour at a time...\\nI don't know.  I once asked Dr. Maxwell the same question.  He said he thought she was like a sleepwalker who would never waken.\\nDoes she suffer?  Does she know what she is?\\n\", \"Good luck, kid.\\nThank you.\\nHer name is Sandra Templeton. She's going to Auburn. The semester's almost over, so you better hurry.\\n\", 'Stuckey!  He wants an \"appointment\" with me after you leave.  You my pimp now or did he think that up on his own?\\nWould you please calm down.  Tell me what happened.\\nI\\'ve been with stinking old men who\\'ve made me want to puke but I\\'ve never had anyone make me feel as dirty as you did tonight.\\nI don\\'t know what you\\'re talking about.\\nClean the slut up, take her out, huh?!  What are you trying to prove!?  I\\'m not a piece of meat for you to offer to your friends!\\n', \"Yeah, I'm gonna worry about you, too.\\nSure.  And you're gonna take the vows next Tuesday, right.\\nHey...no more good times like before.  Until you come back.\\nPanther!  Same thing...\\nPanther...\\n\", \"He causes problems. He was here earlier.\\nUh-huh.\\nHe's drunk.\\nCan you tell me what happened?\\n\", \"One moment, and they're at the <u>Seven-</u> <u>Eleven</u>.  They botched the fallback plan, they...\\nStarling...?\\nI don't mind being the token <u>woman</u>, what I'm suggesting, send me out there with a token <u>man</u>... who are these Warriors, our cobbled together Strike Force?  I'm in the room with a fugitive felon...\\n\", \"Looks real to me.\\nIs it real?\\nHe's not there.  But look what I found in his room\\n\", \"I don't have a VCR at my house.\\nWhy don't you beat off at your house?\\nWhat's up guys?\\n\", \"Women in London must have learned to not breathe.\\nI'm told that dress is the very latest fashion in London.\\nDifficult ... to say.\\n\", 'Thank you.\\nNice place.\\n', 'Catherine, you are a blanket of gloom. Wherever you are, the rain follows. Someday, you gonna smile, and we gonna declare a holiday.\\nYou will see. A coupla months, you gonna be an old lady, sleeping onna couch in her daughter-in-law\\'s house.\\nI don\\'t sell this house, I tell you that. This is my husband\\'s house. I had six children in this house.\\nWell, that\\'s all. You will see. Today, tomorrow, inna week, he\\'s gonna say to you, \"Hey, Ma, it\\'s no good being a single man. I\\'m tired-a running around.\" Then he\\'s gonna say, \"Hey, Ma, wadda we need this old house? Why don\\'t we sell this old house, move into a nicer parta town? A nice little apartment?\"\\n', \"Right, mom. And I'm still a virgin!\\nYou haven't been doing anything stupid, right, Dade?  Right, Dade?!\\nWell, yeah, I just haven't found one as charming as you yet.\\nCan I cut the electricity to his room so he'll sleep normal hours? He's been playing with his computer all night for a solid week.  Well yes, he could be playing with himself. Mmm hmm. Yes I'll ask. Dade, you like girls, don't you?\\n\", \"Don't be like that. Think of me as the Moses of dirty windshields leading you through the desert of dead bugs.\\nNo, I know I don't want it cleaned. Get out of here.\\nYou just think you don't want your windshield cleaned.\\nI don't want my windshield cleaned.\\nJust giving you the gift of a clean windshield. Only cost you a dollar.\\n\", \"It's my house.\\nYou're naked.\\n\", 'I didn\\'t make you. You thought it best.  But, look, I take full responsibility.\\nYou made me have you erased! I loved you. I love you! How could you --\\nIt was a mutual decision.\\nYou made me have an abortion.\\nI have a family, Mary.\\nDo you love me? Did you love me? Something. I listened to my tape. I can\\'t believe I\\'ve been sitting right in front of it for a year. It\\'s like listening to someone else\\'s story. I mean, I hear myself talking about having sex with you and I can\\'t even imagine you naked. I can\\'t even say \"naked\" to you!\\nI don\\'t know what I\\'m supposed say, Mary. I want to do the right thing here.\\nThanks.  So... do we talk about this... or what?\\n', 'I think you want to go to the west wing.  Through there.\\nYes?\\n', \"That's it! The Missionaria Protectiva has been here planting protective legends against a day of Bene Gesserit need. And that day has come. I must play out this sham.  I know the Dark things and the way of the Great Mother. Miseces prejin.\\nAs the legend says.\\nI know the Bhotani Jib and Chakobsa, all the hunting languages.\\nYou know the ancient tongues?\\n\", \"The matrix formed in a day. The life forms grew later -- at a wildly accelerated rate. Can I cook or can't I?\\nYou did this -- in a day?!\\n\", 'Your infrared camera?\\n184  CONTINUED:\\n', \"[We don't know! We never see his face! We have to wait in the other room. He was screaming to the girl that...]\\n[Who's the boss?]\\n[The boss did or his brother.]\\n[Don't lie to me.]\\n[No!]\\n[Who killed her? You?]\\n[Four-seven-four-seven.]\\n[The number you use at the bank machine.]\\n[The what?]\\n[What's the PIN number?]\\n\", \"Well, whatever reasons Mrs. Christian has for engaging the services of a private investigator, I should certainly be a party to. But, since she feels differently, I can only go on the record as having expressed my adamant disapproval.\\nThat's right.\\nYou are a private investigator?\\nOf what sort?\\nAs Mr. Christian's attorney and one of the executors of his estate, it concerns me that a meeting of this sort should take place without my being asked to attend.\\nI'm listening.\\nYes, I do have something to say.  I insisted on being here as soon as I heard Mrs. Christian contacted you.\\n\", \"You're a menace! A walking pestilence. I do know who you are, Taylor. As I know that others of your kind must live in the Forbidden Zone.  You have just six hours to make a full confession. After that I'll employ surgery to obtain one.  Guards!\\nI'm not protecting anybody! That hearing was a farce. What have I done?\\nIf you are protecting others of your kind, it will cost you your identity.\\nI don't know.\\nYou may well call it upside down, since you occupy its lowest level. And deservedly.  The eastern desert has never been explored -- because we've always assumed that no life can exist there.  Save yourself, Taylor. Tell me -- is there another jungle beyond the Forbidden Zone?\\nThank you for calling me Taylor.  Dr. Zaius, I know who I am. Who are you? How did this upside down civilization ever get started?\\nI admit that where there's one mutant there's probably another. And another. A nest of them. Where's your nest, Taylor? Where are your women?\\nThen you admit --\\nBecause you're not unique. There was the one you call Landon --\\nAll right, suppose I am a mutant? Why does the appearance of one mutant send you into a panic?\\nOf course.\\nThat's exactly what Zira and Cornelius claim. You're talking heresy, doctor.\\nCertainly not.\\tYou're a mutant.\\nI take it you don't believe the prosecutor's charge -- that I'm a monster created by Dr. Zira.\\nA fort! Unconsciously, you chose a name that was belligerent.  Where were you nurtured?\\nWhat about it?\\nThen how is it we speak the same language?  Even in your lies, some truth slips through! That mythical community you're supposed to come from -- Fort Wayne'?\\nMy tribe, as you call it, lives on another planet in a distant solar system.\\nYou lied. Where is your tribe?\\nI told the truth at that 'hearing'of yours.\\nHowever, it's within my power to grant You a reprieve.  That is why I summoned you here tonight.  Tell me who and what you really are and where you come from, and no veterinary will touch you.\\n\", \"So be it.\\nYes, Dudley.  I am.\\nYou'll reap the benefits, but are you truly prepared to be despised within the department?\\n\", \"Help me!\\nPush 'em back.\\nThat's some gash. His guts keep spilling out.\\nHe'll never make it.\\nTotal delirium.\\nHe's burning up.\\n\", \"Please. Please don't forgive me. I've always hated you for that.\\nI forgive you.\\nYou can't forgive me. After what I've done.  I've fucked up bigtime. I've been bad. Real bad.\\nI forgive you.\\nMe?\\nI forgive you.\\n\", 'Yes, sir.\\nOh, make that uniform blue.\\n', \"That's a comfort...\\nThey'll respect him now...\\n\", \"Because we're the biggest things to hit this town since Godzilla.\\nWhy's that?\\nYou know, Nick, we can't lose.\\n\", \"Great, great. I hope you make it.\\nNothing, I reconciled with Ron, you know, the guy I divorced last summer.\\nHey, baby, what's happening?\\nHi, George.\\n\", \"No.\\nWe didn't just have sex in the bathroom?\\nI'm serious.\\n\", 'Catch him, you fool!\\nRun, Toto, run!\\n', \"We've got to keep her away from that bastard!\\nWait... *wait*! Are you sure?\\n\", \"Of course it does.  Obviously in denial.  Maybe we can help.\\nWhat? It's nothing, the cast comes off in six weeks.\\n\", 'Come on, Mr Lombard. You tailed your man all the way here from Europe. You told me you knew where to find him, remember? Or didnt I hear you right?\\nHuh... I have no... ... hard evidence...\\n', \"There's absolutely no reason to think this is going to have any impact on you. I'm embarrassed to have --\\nWhoa, whoa, what are you doing? I want to know what's in here.\\n\", 'No problem, I enjoy talking in my sleep.\\nCan I come in?\\n', \"Why? Why not? Where are you goin'?\\nNot tonight. Just give me something to take home to hold me over till tomorrow.\\nLet's ditch this place and party.\\nSure, but I can't get it till after school tomorrow.\\nYeah, and I'm gonna need that ten thousand dollars back.\\nI'm not... Bobby I'm gonna need some more stuff. I mean it. I'm out.\\nDon't get funny with me again.\\nWhat is the world coming to when you kill a guy for baby laxative?\\nNo shit...  We killed a guy for baby laxative.\\nBaby laxative? We can't snort baby laxative.\\nThe stuff we got last night.\\nWhat was?\\nBad news, kid, it was baby laxative.\\n\", 'To our family.\\nTo our family.\\n', \"You know what I'm doing with my part of the gold?\\nBut that's the great thing about Gold.  A little goes a long way.\\n\", 'I want you to forget the company you work for for thirty seconds, and tell me if you really think that Sammy is faking his condition.\\nMrs. Jankis, what do you want from me?\\n', \"Money from home, see, there's your strength, you put things in earthy terms any man can understand, son. I warn you I'm gonna use you, I'm gonna run you ragged!\\nGonna be like money from home.\\nI got a hunch, Joe Buck, it's gonna be easier for you than most.\\nReady for anything.\\nReady for hard work, son?\\nUh, yessir.\\nYes, I believe you are. Cowboy, huh?\\nWell, uh, I'm raring to go.\\nLonesomeness is something you take. You bear? Dammit, you take it and go about your business, that's all.\\nYessir, I can see that.\\nI'm lonesome. I'm lonesome so I'm a drunk. I'm lonesome so I'm a dope fiend. I'm lonesome so I'm a thief, a fornicator, a whore-monger. Poop, I say, poop! I've heard it all and I'm sick of it, sick to death.\\n\", 'I\\'ll remember you said that.\\nI\\'d ignore her.  People can Change, Doc.\\nIgnore her?\\nI don\\'t know.  Probably ignore her.\\nYou know damn well who I mean. That dusky-hued lady Satan.\\nI see.  And what would you do if \"she\" walked in her right now? WYATT \"She\"?\\nWell yeah.  Pretty much.  I mean I Was no angel when we met but People change Doc.  I mean sooner Or later you gotta grow up.\\nSo tell me, Wyatt.  I\\'m curious. Do you actually consider youself A married man?  Forsaking all Others?\\n', \"Is she as pretty as you?\\nShe's from Connecticut. She belongs to his stupid country club.\\nSo, what's this Sarah got that you don't? Three tits?\\nThat's terrible!\\nWhat could I do?  He's a man who followed his pecker to greener pastures. I'm a middle- aged high-school dropout with stretch marks and a fat ass. Happens every day. At least to women like me.\\nNo!\\nI didn't even get to go to his birthday party.\\n\", \"No, Deak, not a great story.  We backed this guy, he's our boy!  We gave him a vote of confidence, we gave him a million dollars.\\nHelluva story!\\nListen, Deak, what if Bubber <u>has</u> got something to hide?  What if he's the wrong guy, not really the hero...?\\nGale shoulda aired that bit first, she's the one who found this clown LaPlante!  She let Channel Eight get a beat on us.\\n\", \"Oh yeah. There's a lot of stuff. Currants and strawberries ... Here. I'll show you.\\nThey just grow like that?\\nI picked them myself. They grow wild up here.  Mmm. So sweet.\\n\", 'A passenger...\\nYou tell it straight or I pull the trigger. Who are you?\\n', \"'Bout two hours left.\\nBut there's still daylight left.\\n\", \"I don't mean that. I worry when you try to ignore it.\\nIt's just a nightmare.\\nI worry when you get like this.\\nI'm trying, Marie, Okay?\\n\", \"C'mon, it's true -- But that don't bother me -- I just wanna prove somethin' -- I ain't no bum... It don't matter if I lose... Don't matter if he opens my head... The only thing I wanna do is go the distance -- That's all. Nobody's ever gone fifteen rounds with Creed. If I go them fifteen rounds, an' that bell rings an' I'm still standin', I'm gonna know then I weren't just another bum from the neighborhood...\\nDon't say that.\\nIt ain't so bad, 'cause I was a nothin' before --\\nOh, Rocky -- you worked so hard.\\n...I dunno.\\nWhat're we going to do?\\nI been watchin' the movies -- studyin' -- He ain't weak nowhere.\\n\", \"No!\\nWhat's your fuckin' worry? If it's not your time...? I could get nailed runnin' this red light and you all wouldn't get shit! Only me, right?\\n\", \"I'm one of the proofers.  I slipped the article in.\\nYou what?!  How did you do that?\\n\", \"It takes about twenty to twenty-two hours for the current to reach here from Lisca Bianca.\\nIt's already two hours... What are we going to do?\\n\", \"Well try dammit!\\nI can't.\\nI'm gonna lower you to the chimney, okay Jamie?\\n\", \"How long was I gone?\\nWhat day?\\nPeter, what are you talking about? What malfunction?  What day is this?\\nIt's all right, the important thing is you're safe --\\nWait -- hold on a minute --\\nThank God.  When we lost contact, I thought -- we thought... but you're okay.  We're still trying to determine the nature of the malfunction.  Did you notice anything at all that --\\nI'm -- I'm fine.\\nEllie -- are you okay?\\n\", \"Naw, them dudes is assholes.  Especially that dog - Cheeco.  Watch this little ass, he's sneaky.  Plus, I got something better than a Cadillac.\\nThey ever let you hit the switches on that Cadillac?\\nJoker, he just got out of the pen.  Li'l Joker, he just got out of Youth Authority.  And Baby Joker, he just got out of Juvenile Hall.\\nWho is that?\\n\", \"A break for what?\\nTo conceal a small break in the surface of the sphere.\\nWhat?\\nNo.  They don't represent a message. They aren't decorative at all.  They have another purpose entirely.\\n\", \"The girl's got rhythm.\\nUm-hmmm.\\n\", \"I'll risk it.\\nIt's your call, but you're gonna be sorry when you're in one of those everyday situations that call for a battery-operated vagina and you don't have one.\\nWell, it's tempting, but no thanks.\\nMy boss tells me I have to do more suggestive selling.\\nPardon me?\\nCan I interest you in a battery operated-vagina?\\nYeah... guess so.\\nBig date tonight?\\n\", \"You get time off to sleep.\\nIt's not your time. I get time off.\\nAnd on my time.\\n\", \"Ah, Dick, let's talk about it in the car. We can't be late.\\nThey didn't even want a callback. They just hired me like that. Me and Peter Breck are the two heavies. We start shooting Monday. My call is for seven o'clock in the morning.\\n\", 'We had a run-in, he and I. He knew about us, he knew about my shoulder, he knew exactly where to hurt me...\\n...what?\\nOr did you steal it from your old friend Renard?\\nWhat are you talking about?\\nA little. Does it matter? After all, whats the point of living if you cant feel alive?  Isnt that right, Elektra? Isnt that your motto?\\nWhats wrong with you?  Are you crazy?\\n', 'Shut up!\\nHanover, listen...\\n', \"I hate to do this, but I've got to have time to dig up some help. I think I know where I can get some real cash. Snap into it, Sampson. We will lick this thing yet.\\nYes, sir.\\nGet all the big bills in the place. Take them out and get them changed. Get nothing but ones and fives. Distribute them among the tellers. Tell them to take their time. Stall as much as possible. Count and recount the money.\\nMr. Dickson! Mr. Dickson!\\n\", \"It's your job to know.  If something goes wrong up there, the other eighteen people aboard can't be wondering if he's gonna do his job or not.\\nI didn't know sir.\\nWhy wasn't I made aware of this Bill?\\n\", 'Norm?\\n...Hello?\\n', \"Let's go upstairs, okay?\\nWhat is it?\\n\", 'Oh! That\\'s wonderful, Harold. Go - and love some more.\\nNever! Never! I\\'ll never forget you. I wanted to marry you. Don\\'t you understand! I love you. I love you!\\n\"And this too shall pass away.\"\\nIt\\'s true. I can\\'t live without you.\\nOh, Harold, don\\'t upset yourself so.\\nBut Maude, you don\\'t understand. I love you. Do you hear me? I\\'ve never said that to anyone in my life before. You\\'re the first. Maude. Please don\\'t leave me.\\nI feel giddy.\\n', 'Ok.  Wait here.\\nYeah.\\n', \"Certainly, glad to be of help.\\nFine. Thank you, Mr. Franklin.\\nOf course, Doctor, I understand.\\nI see.  Mr. Franklin, I must ask you and Miss Hayes to keep this incident with Mr. Gardiner to yourselves. There's no telling what he was involved in, and the matter may be extremely confidential. So please, not a word.\\nWell, he said it was his, he walked us through it.\\nHmmm. You say he showed you his garden?\\nAnother thing that baffles me, Doctor - what was his connection with the deceased? Major financial dealings, obviously - but our firm has no record of any such transactions.\\nI have no idea...\\n... And he told us that he had been living there since he was a child, working as a gardener. He showed us a room in the garage, where he said he stayed, and I... Well, I didn't really believe him, of course - but why the act?\\n\", '--It is the old wound, that has been opened. I have always known it would be the gateway to my death, for it has never healed. Let my heart do its job, my King, and pump me empty...\\nYou are that and much more. You are its greatest knight, you are what is best in men. Now we will be together--\\nMy salvation is to die a Knight of the Round Table.\\n', 'Shut up.\\nYou messed with the scene.\\n', \"No --\\nI don't like your tone.\\nYou're no good for this business. It's just a joke to you...\\nPoor girl -- chloroform would give her a rotten headache... I know -- I had it in the war. Besides, she's very pretty -- not young but --\\nWhy?\\nNo -- no -- no -- no...\\nChloroform on a handkerchief from behind -- while you...\\nHow?\\nI could take care of her.\\nThe floor clerk is out there in the corridor -- she sees everything ---\\nWhy?\\nNo --\\n\", 'Just a minute.\\nI thought you were getting me a drink?\\n', \"Me neither.\\nWe'll find out soon enough.\\nEvan, you're hysterical. You study for this?\\n\", 'Anything for a fellow pilgrim. Sometimes we need help getting where we want to be.  Reverend Jackson Pete Sayer of Dumon County, please to make your aquanauts.\\nThank you.\\n', \"I got lost.  Oh here ... look at this!\\nWhat were you doing in a library?\\nI looked in the library. They got covers with nothing inside them.\\nWhat?\\nIt's too weird David. This place is giving me the creeps. Did you know all the books are blank?\\nOne date, Jen--that's all I'm asking. If you don't go out with this guy we could throw their whole universe out of whack.\\nNo way.\\n\", \"Dark and large.  With vines -- no, not vines.  Not alive.\\nWhen you're here, in the city, where do you live?\\n\", \"Don't give me that bullshit.\\nNothing wrong.\\nSo what's wrong?\\n\", \"It was obvious from the way he first looked at me.\\nIt's a violent response to something we haven't figured out.  Don't let the cowardly demeanor fool you: He is ruthless.  Unblinking in his prejudice.\\nYou feel sorry for Solaris, or for me?\\nHe does have a point.  That's just not the way I'd like to see it proven.\\nHe has a point.\\nWell.  He doesn't think it's God, but for different reasons than me. He's thinking: If I can figure out how to make it stop, than I am smarter than it is, and therefore it cannot be God.\\nSartorius wants to destroy it.\\n\", 'Can I tell Diane that Peggy Sue got drunk or is that a deep family secret?  Well?\\nMom, sit down for a minute. This is so nice, all of us being together again like this.\\n', \"Lower me down.\\nExcept he's lost, and I'm not.\\nYou don't disappoint, Doctor Jones. You're a great deal like your father.\\nBingo.\\n\", \"First the supplier couldn't find the invoice. Then the order came up short. Then I missed the four o'clock plane from Cleveland and had to rent a car, and then I got a darn flat tire on the highway! Can you beat it? Boy, your boss must've been furious when I didn't show up by the end of the day today, huh?\\nWhat?\\n\", \"Like you couldn't believe.\\nYour line of work.  You must meet a lot of men play fast and loose with the truth.\\n\", 'I want you to come.\\nYeah?\\n', \"Oh, good. For a minute I thought you'd lost your mind.\\nWhat do you think?  Maybe I could reach the ledge without falling. No, forget it.\\n\", \"What, what, what, what do they got that can pass for the Old Mill...\\n...why am I here...?\\nI'm bleeding, Bill, I'm <u>bleeding</u>...\\nYeah, but they never made a movie here.\\nYou told me that about the <u>last</u> town.\\nA jacket for five dollars...I can buy this town for fifty bucks.\\n\", \"Maybe you don't know what it's like where Mrs. Crawford comes from -- but I do.  I came from a neighborhood just like hers.  This is a whole other world for her.  She's a poor working woman who has been thrust into a room full of highly educated and mostly unsympathetic people.  So, she puts on her best dress, fixes her hair and tries to present herself as intelligently as possible.  Being poor and having pride is not a crime, Mr. Dulaney -- and before you attempt to impeach another witness' testimony in my courtroom -- your foundations better be based on something other than semantics.\\nNo, Your Honor.\\nMr. Dulaney, before you cast aspersions on the District Attorney's Office by suggesting they've coaxed this witness to say things that aren't true -- you better have more than a hunch.  Do you?\\n\", \"Ronny, please...\\nNo. I'm gonna wait.\\nYou've gotta get outta here.\\n\", \"If I get antsy I'll kill a few small animals.\\nGo to your reunion, Martin. See those people and discover what they mean to you. Try not to kill anybody for a few days, see how you feel.\\n\", \"It wasn't difficult. Only an apostate or a lunatic would flee to the Forbidden Zone.  I see you brought along the female of your species.  I didn't realize a man could be monogamous.\\nHow did you know we'd come here?\\n\", 'I suppose I can, sir; are you planning to take a swim?\\nThat should be enough. Can you enclose it to hold water?\\nAbout 60 feet, Admiral.\\nScotty, how long is this bay?\\n', \"Give 'im a fryin' pan.\\nI ain't got nothin' in *my* han'.\\nJust in case. Sit up back an' if anybody tries to climb up--let 'im have it.\\n\", 'To fame.. salud..\\nDrink!\\n', \"No! I swear I'm not!\\nMaybe she's telling the truth. Maybe she's not part of it.\\n\", 'I\\'m waiting for your offer, Clarice. Enchant me.\\nA moth... How did you predict that?\\nWas it a butterfly?\\nShe had an insect deliberately inserted in her throat. That hasn\\'t been made public yet. We don\\'t know what is means.\\nMmm. And what else...?\\nThey all were.\\nBig through the hips. Roomy.\\nYes.\\nLife\\'s too slippery for books, Clarice. Typhoid and swans came from the same God.  Tell me, Miss West Virginia - was she a large girl?\\nBy the book, he\\'s a sadist.\\nThat is both impudent and untrue... Tell me, how did you feel when you viewed our Billy\\'s latest effort?  Or should I say, his \"next-to-latest\"?\\nI was your choice, Dr. Lecter. You chose to speak to me. Would you prefer someone else now? Or perhaps you don\\'t think you can help us.\\n', \"You what?\\nI know what you're going to say but don't say it.  These drinks are all on the tab.  I'm gonna see you this Tuesday payday, I promise.  I give you my word.\\n\", '\"No,\" what?\\nNo.\\n', 'Thank you, Mrs. Charles.\\nYou give such charming parties, Mr. Charles.\\n', 'Low on gas.  Got to refill.\\nWhat are you doing.\\n', \"Well, thank you. Eh, officer, you might turn off the radio. Saves the battery.\\nOh yes. That's fine.\\nTell me --  -- is that car parked all right?\\nEh, yes, ma'm.\\nWell, it's a tricky turn.\\nYes, ma'am. Somebody had some trouble parking.\\nGood afternoon, Officer. Bit of trouble here?\\n\", \"No. Someone else. A girl.  I'm not sure she was... real.\\nNot our mystery guest again.\\nWhen you found me yesterday, at the pool. I'd seen... something.  Someone.\\n\", 'I left them here. I was doing some work here.\\nGov, want to leave me that one. How come he got to play with this one, anyway.\\n', 'Monkey, this is too exciting! I can\\'t believe our dream is coming true.\\nNo. No. I heard from Superintendent ZIMMER this morning and evidently he\\'s so impressed with our Special Needs class, he\\'s bringing Richard MOFFIT himself to the Thanksgiving Day parade- check in hand.\\nYou got the Extender?\\nWell I\\'ve got a little surprise for you.\\nIt\\'s a surprise.\\nWhat?\\nGuess what\"s under these coconuts.\\nWho\\'s there?\\nKnock knock.\\n', 'Dr. Argon, I demand an explanation.\\nWheelchair accessible.\\n', 'Oh, dont be that way.\\nId be happy to. Youre welcome.\\n', 'Uh, yes...I mean...whenever necessary. You know.\\nSo...you hang out here a lot?\\nDon.\\nDetective Kendall...uh Campbell? KIMBALL Kimball.  Call me Don.\\n', 'Goodbye.\\nGoodbye, Charlotte.\\n', \"I think you had better leave\\nI'd watch her mate - she has thing for men - they disappear near her.\\n\", \"...Maybe a picture of me in the liner notes...\\nI wanna live with a musician.  She'd write songs at home, ask me what she thought of them, maybe even include one of our private jokes in the liner notes.\\nI wanna date a musician...\\n\", 'How could it be? I hate Africa.\\nPleasant journey?\\n', 'It\\'s an awful thing, let me tell you. My Aunt used to say,  \"divorce is the sister-in-law of death\".\\nMy parents were divorced.\\n', 'I thought myself your friend, Mary. Just good-bye isn\\'t enough for a friend.\\nDon\\'t make me tell you, Jason.\\nIt isn\\'t that -- you said, \"have to go.\" What could compel you --\\n', 'Oh, well, then. It probably wont work out.\\nIts complicated, Dad.\\nSo whyd you let him go?\\n', 'I see the truth of it. -- REVEREND MOTHER  Could he be the one?... Maybe... but will he be ours to control?  You know when people speak the truth?\\nPain by nerve induction... A human can resist any pain. Our test is crisis and observation.\\n', \"Make love to me.\\nIt's OK.  It's OK.\\n\", \"Then try talking to my lawyer. Good evening, gentlemen.\\nNot good enough.\\nNo.\\nAnything else you want to add before I talk to her?\\nWhy do men and women usually see each other?\\nWhy's she seeing Bud White?\\nLynn Bracken.\\nWhat's her name?\\nA vulgar term, but yes.\\nIs the Veronica Lake look-alike one of your whores?\\nFor the last time.  I may suborn women into illicit activities, but they're handsomely compensated, I treat them well and make sure the men they deal with show them every due respect.\\nBud White's been here?\\nI'll tell you what I told Officer White when he asked me about Susan's death.\\n\", \"I'm not asking.\\n<u>This is an operations desk</u>.\\nI want a second opinion.\\nWe've been down here for two weeks banging our heads against the wall. We've been <u>sleeping</u> down here.  We just got our first lead fourteen hours ago, and now? -- now that we <u>finally</u> have something to work with -- you want to bring planning personnel down here?  I'd rethink that.\\n\", 'Very slowly.\\nAre you saying somebody threw these things in with our guy, and they slowly ate him alive?\\n', 'Like a leech.\\nIt would explain the suction- like appendages.\\n', 'Any of us?\\nNine.\\nHow many others killed?\\nDead.\\nThe rest of the secret service?\\n', \"You're the boss.  Let's head out to the tarmac.  Matheson, have you been totally briefed?\\nAppreciate it.\\nWe've got a full crew, but we can squeeze one more, right.\\nMatheson has been transferred from the Denver office to Frisco. As a professional courtesy between offices, I was asked if he could hitch a ride.\\nRichard Travers.\\nI'd like to have a word with you. This is Agent Matheson, FBI.\\nGood morning, Walt.\\nRich...\\n\", \"Yes.  Unlike our syndicates, your criminals don't understand the words 'honor' and 'duty'... We can't afford not to deal with them.\\nThe families who control the casinos?\\nOur associates in New York were close to closing a deal with us.\\n\", \"I got a lot of free time, you know what I mean?\\nI knew I could count on you.\\nI don't, but I'm pretty sure. It's a long story but I cloned a cellular linkup with a binary code scrambler and sent it through the phone network mainframe.  If they start a trace it will be to a Pic'n'Save on Pico.\\nNo thanks.  How do you know they can't catch you on this thing?\\nDonut?\\n\", \"Hold the record. Alone.\\nCome on, what can you do with six billion you can't do with four?\\n\", \"Thanks, Doc. Maybe I'll come back with a date. Or an elephant.\\nTell the nurse when you've got a few free days. She'll make all the arrangements.\\nI hope they still fit. Do I get to keep the glove?\\nYeah, we check you into Mt. Hebron for a few days, run lots of tests, charge a bundle. You can pull your pants up now.\\nI'm sure it's not for a lack of looking. Maybe I should get a real complete physical. You give Alan an annual, don't you?\\nI can't discuss another patient. You know that.  Well, I can't find anything wrong with you.\\nGee, Alan's been looking kind of sick lately. Is he all right?\\nJust relax.\\nWhoa, look out there. You really need the whole fist?\\nThere is?\\nAnyway, I'm surprised Alan got the policy so easily. I know there's a history of cancer in the family.\\nBreathe easy...\\nDid I say 'kidneys'? I meant my ear. Maybe I should see an ear dahhh --  Ever serve time?\\n\", \"Grady... oh.\\nIt's Grady, Mr. Torrance.  Delbert Grady.\\n\", \"Doesn't threaten me, honey. I'm happy.\\nCarl makes me happy and that threatens this family, doesn't it?\\n\", \"I could.\\nAnd you're the guy who's going to open it.\\n\", 'I don\\'t want excuses, I want that weird-looking stuff called \"cash.\"\\nI had one <u>puff</u> on a <u>pipe</u>.\\nI need some security. I don\\'t trust you anymore.\\nWhy?\\nThen give me that Zippo.\\nI haven\\'t got it.\\nPay me, and I shut up.\\nDon\\'t start again. If you win a bet, you can\\'t keep winning it ..\\nSo am I. But where\\'s my money?\\nMaybe not? I\\'m feeling lucky ..\\n', \"Couldn't go on? You'll give the performance of your life.\\nI won't play tonight.  I couldn't. Not possibly. I couldn't go on...\\n\", 'Yeah. I just saw him outside. Maybe the robins are here.\\nLook Jeffrey.\\n', \"Is there a fire, then?\\nWell, you got me here so do your worst but I'll take one of you with me.  Oh, I know your game, get me in the tiled room and out come the rubber hoses but I'll defy you still.\\n\", 'He\\'s worse than an animal. Jail\\'s too good for scum like that.\\nWhat makes you so high and mighty? Did you ever look at your own eyeballs in a mirror. You don\\'t get eyes like that from...\\nHe shoots dope too.\\nI think Cancer\\'s make the best lovers. My whole family are air signs.\\nHe looks like a killer.\\nI\\'m a Libra too. That\\'s why we get along so well.\\nHuh?\\nHe never killed nobody. He\\'s a Libra.\\nThat fella \"Sport\" looks like a killer to me.\\nWho\\'s a killer?\\n', \"Just cause I don't kiss her booty like you...\\nLike you don't give her a hard time.\\nAlways on my ass, anyway.\\nMom's going to hate it.\\nNo.\\nThat too tight?\\n\", \"Jabez -- for the good of your soul ... please come with us.\\nShe'll do nothing of the kind! She's going to church with me, right away!\\n\", \"I am not Shinzon. I am his Viceroy. We are sending transport coordinates.\\nPraetor Shinzon, I'm pleased to...\\nEnterprise. We are the Reman Warbird Scimitar.\\n\", 'Let it go.\\nAll that I once loved lies in a shallow grave. By my hand.\\n', \"He's still just sitting there. God, this is totally unbearable!\\nWhat's he doing now?\\n\", \"I am for you.\\nGood king of cat's, nothing but one of your nine lives.\\nWhat wouldst thou have with me?\\nCalm, Dishonorable, Vile Submission! Thou art my souls hate! Tybalt! You ratcatcher, will you walk?\\n\", \"The Yater -- the clear one with the thin stringer.\\nWhich one's the Colonel's?\\nNo -- no, Captain.\\n\", 'What?  Oh yeah.  Coming.\\nCinn?\\n', 'Partially, yes.\\nIs that why you want to finish the pipeline?\\nGod no. All those horrible loved ones and relatives. I dont want to talk to those people. I just want to...talk to my father.  I loved him.  Im not sure he knew that.\\nFunerals arent exactly memorable.\\nI havent been able to recall a single moment of that day...until now.\\nYes.\\nI met you at my fathers funeral.\\n', \"Ok. Let's see what happens.\\nYeah, I'll go.\\n\", \"That's what I'm trying to tell you! ...\\nWait.  Wait.  The chains.  Where did the chains come from?\\nYeah!  That's what I'm saying!  And it was lying next to him.  And he pointed at it before he passed out and ...\\nWait a minute.  He was already ... wounded ... when you found him?\\nThe thing!  He was lying there in the street, moaning.  But he pointed at it ... 22\\nTaken what?\\nNo.  I'd seen him in there a few times before.  He was just a punk.  I'd never like danced with him or anything. Anyway, he was a thief.  He must've taken it from the statue.\\nDid you know him?\\nBut I don't know anything!  Really.  I just came out of the club and the kid was already in the street.  He ...\\nYes I do.  Terri, something awful happened to that boy.  I have to find out what it was.\\nOh.  You wanna talk about that stuff.\\n\", \"And something else, too, Miss Saunders-- the spirit of it--the idea--the--\\nNow, let's get down to particulars. How big is this thing? Where is it to be? How many boys will it take care of? If they're going to buy it-- how do they make their contributions? Your Bill has to have all that in it--\\nI did--and--and I got right up.\\nTry sitting down.\\nUh--have you?  Did you ever have so much to say about something--you couldn't say it?\\nYes, Senator--*twice*.\\n--that's the main idea, Miss Saunders. The United States Government isn't going to buy or build this camp-- just lend us the money. You've made a note of that, huh?\\n\", \"Then you're going to kill me.\\nI'm gonna get that fucking bandleader, Colonel. No deal. No fucking deal.\\n\", 'Right here.\\nYo, where\\'s the \"D\"?\\n', \"My guests were shredded. It's your sick little scene now, Steven: enjoy. I'm going to go run scalding water on the places you just touched me, and then I'm calling a cab.\\nNow, there's the simple country gal I married. Let's go back down and greet your guests -- show them the real you: corny as Kansas on the Fourth of July.\\n\", \"If you don't even know what you want to do...\\nI'm not sure, but...\\nHelp how? What do you want to do?\\nThen help me, Sebastian. You're an important man in this town. If you'll help, the rest will.\\nI like Bodega Bay as well as any man. If I thought...\\nIt's happening. Isn't that a good enough reason?\\nNo. I don't, Mitch. Because I can't see any reason for it.\\nDo you believe it's true, Sebastian?\\nLook, Mitch, even if this is true, even if all the birds...\\n\", \"I'm thinking about going to bed.\\nYou're thinking of things best not thought of, Louis.\\nJud, I buried my son today and I'm very tired. I wonder if we could just--\\n\", \"It's more a place where souls are found, Mr. Riddick.\\nThink a soul could get lost there? With all those pilgrim-types?\\nNew Mecca....\\n\", \"Bruce Wayne?  Date?  He called you up and asked you for a date?  Shit.  HEY, MIRANDA!  C'MERE!  Now pay close attention to this.  Miranda -- tell my friend here what you told me about Bruce Wayne.\\nGuess who's got a date with Bruce Wayne?\\n\", 'Solis said to keep you clear of this.\\nAnything on Korda so far?\\n', \"No.\\nLook, would you argue with your doctor?\\nI don't know, I think...\\nIt's great.  You wanna speak to Mary? Hold on...\\nDon't you think there's too much gold?\\n\", \"Well, I had my fun, I've drunk my fill and I tickled some good-lookin' fillies I'm on borried time.\\nSo help me God!\\nDown to the wire?\\nAnd I still say you stand up and I'll stand up with you.\\nBaseball bats that's just for openers. They'll put the muscle on you, turned-around collar or no turned-around collar.\\nI'll go down the line, Kayo, believe me.\\nIf I stick my neck out, and they chopped it off, would that be the end of it? Or are you ready to go all the way?\\nWhat do you think?\\nAre you on the level, Father?\\nYou still call it ratting?\\n\", 'Now if you don\\'t want to be the --  -- fifth person ever to die in meta-shock from a planar rift, I suggest you get down behind that desk and don\\'t move until we give you the signal \"Stabilize -- All Clear.\"\\nYou got a flux and a half.\\n', \"It's our money.  He had no right to take it.\\nThirty-thousand, that's all.  We would have been free and clear.  You didn't have to kill him.\\nHe was going to take the money.\\nYou killed him!\\nI shot him.\\n\", \"Okay.  So, tell me about yourself.  Are you married?\\nLet's change the subject.\\nDo you?\\nOf course.\\nWhat about you?  Aren't there times when a young child is telling you a story so sad you just want to cry?\\nThat's true -- but the emotion is still there.  They just learn to control it.\\nTake your average cop.  They deal with death everyday.  If they let emotions get in the way it would cloud their judgement.\\nHow can you look at it so clinically?\\nOh, but it is.  He's killing a person everyday and challenging us to catch him.  That's a game.  It has rules and objectives.\\nThis isn't a game.\\nIn a way.\\nAnother kind of game?\\nI think it's interesting.\\n\", 'Did you remove anything in the cellar!\\nWhat...amulet?\\n', \"You're wrong man! Any way you cut it Rhah, Barnes is a murderer.\\nYou guys trying to cure the headache by cutting off the head. 'Lias didn't ask you to fight his battles and if there's a Heaven - and god, I hope so - I know he's sitting up there drunk as a fuckin' monkey and smokin' shit cause HIS PAINS HE DONE LEFT DOWN HERE. Baaaaaaaaa!\\nFuck this shit!\\n\", 'Mordechai!\\nEsther!\\n', 'Maybe I could take a look at it now.\\nWell, can I make an appointment?\\nSure.\\nOh, well, do you fix refrigerators?\\n', 'Nya-nya-nya!\\nThis is hot!\\n', 'We better, Congress invented him.\\nAre you going to keep him, sir?\\n', \"All right, but if you change your mind, let me know.  I gotta go get Ma in the morning anyway.\\nThanks, but I've got plans.\\nYou want to come out to the house tomorrow? The way the bookings been piling up, Donna's decided to really lay it on.  Turkey, stuffing, the whole bit.  Kitchen's so full of food you can hardly move.  We could use another appetite.\\n\", \"We sell 'em to you for three bucks a piece!\\nA beauty, isn't it?  We bought three of them for the rec room.\\n\", 'Try it.\\nOh yeah? How is it?\\nA Fiddlehead Sauvignon Blanc.\\nWhat are you drinking?\\n', 'Hello second mate Barnes. I was instructed to come to the bridge.\\nWhere the hell did you come from?\\n', 'Mexico.\\nWhere is it?\\nGood.\\nSeveral.\\nDo you have a passport?\\n', \"I do.\\nDon't say that. That's like... telling a guy before you have sex you'd better be good. You don't do that.\\nI bet my career on you. You'd better be good.\\nNo, uh...\\nNow look. Don't freeze up on me. I picked you because you had kind of a relaxed, go-with-the-flow quality. You're not going to lose that, are you?\\n\", \"You're damn right that's what it is.\\nIs that what it is?\\nWhat are you all, blind?  It's a shark.  Look -- teeth, jaw, gills.\\n\", \"This is special, darlin'.  Please?\\nSpeak up.  Clear it with the Captain if you need a file.\\n\", \"A dirtbag behind the counter holding a sawed-off. A Berretta nine millimeter in his belt. A female hostage, red dress, on the floor in front of the cereal display. Male hostage, jeans and blue checked shirt, three feet to her right. Another male hostage, white pants, green shirt, Nikes, laying in front of the magazine rack. A female dirtbag with a gun under her shirt, sitting against the beer cooler, trying to pass herself off as a hostage, and there's a special on toilet-paper, four for a buck twenty-nine.\\nWhat did you see?\\n\", 'Alert seventy-fifth rangers.  Code Blue. Tell them Elvis has left the building.\\nLooks like it.  Yes, sir.\\nIs it--?\\nColonel Vitelli.  We got a busted-in cold vault inside.\\n', \"As happens so frequently here on Romulus, a new government came to power. They decided to abandon the plan -- frankly, I think they were afraid I'd be discovered and it would lead to war. They weren't ready for that.\\nWhat happened?\\nAnd when I was ready they were going to replace you with me, an exact biological duplicate. Put a Romulan agent at the heart of Starfleet to.influence your command structure. It was a bold plan.\\n\", \"You know Dr. Mason's son, Jeffrey Mason, don't you, James?  You met him in the County Hospital six years ago.\\nAll I see are dead people.  Everywhere. What's three more?\\n\", 'The man who bought them last week was American. I did not see him but I heard. I knew you would come.\\nYes.\\nI was expecting you. You are American too, of course.\\nMonsieur Flix -- ?\\n', \"You know he was an only child, like you.  I know how bad you feel about what happened--sailing into that storm.  But I don't blame you, Truman.  I never have.\\nI'm telling you, if it wasn't him, it was his twin.  Did Dad have a brother?\\n--Darling--\\nThey never found Dad's body--maybe somehow--\\nAbout time they started cleaning up the trash Downtown.  We don't want to end up like the rest of the country.\\nIt was Dad, I swear, dressed like a homeless man.  And you know what else was really strange?  A businessman and a woman with a little dog appeared from nowhere and forced him onto a bus.\\nIt doesn't sound insane, Truman.  I swear I see him ten times a week--in a hundred faces.  I almost hugged a perfect stranger in the salon last Thursday.\\n\", \"Seeing how my stock's doing...\\nWhat are you doing?\\n\", \"I happen to know so.\\nI think so.\\nThat would be a monumental waste of time, wouldn't it, Will?\\n\", 'My mother used to tell us of the young man who decided to ride to the next village and how she was afraid that -- not even mentioning accidents -- the span of a normal happy life might fall far short of the time needed for such a trip.\\nBut your goal is so hard to reach. Do you think the official network would surrender to one man?  We would never think of attempting anything remotely as difficult.\\n', \"Five seconds.\\nI don't know.  How long before it kills us?\\n\", \"He's just finding another reason for bumpin' us off. Don't ya see. He needs us ta die. He needs our bodies.\\nLet me go! I'm the one he wants. This is all happening because of me. If I turn myself in...\\n\", 'Good point.\\nWho are you apologizing to?\\nSorry, sorry.\\n', \"Four. But no one this important.\\nWho'd you kill?\\n\", \"Nine, ten. And you're awake! Open your eyes, dammit!\\nEvan wake up, oh please wake up!\\n\", \"I'd like to make another tape.\\nOkay.\\nLook, I'm just going to come right out and tell you why I'm here, okay?\\n\", 'That wants to be a Yes.\\nThat would be a No.\\nYou and Wichita go to school together, right?  Have you two ever...\\nAre we allowed to start hating \"Wendy\" yet...\"Gee Wichita, I guess mosquitoes have always liked me.\"\\n', \"Okay sir, but Mert and Aloysius'll have to scratch Xes - only four of us can write.\\nBoys, that was some mighty fine pickin' and singin'. You just sign these papers and I'll give you ten dollars apiece.\\nHot damn, boy, I almost believe you did sell your soul to the devil!\\n\", \"Yes, that's right.  I'm being terribly rude.  Bob?  Oh, Bob. This is Bob Tuckett.  Bon was at Oxford too.\\nThank you.  We met last week, at the Wajda film.  You won't remember. We didn't actually speak.\\nWas that your chapter, then?  It was very good.\\n\", \"You seen my snake-skin shoes?\\nFolks can dance when they want. Didn't buy that mirror ball for nothin'.\\nWhy you stop havin' dancin' on Saturday? Used to have bands... all kind's live shit. Like a wake up in here, now.\\nHe black alright, he just ain't blue.\\n\", 'Who?\\nLieutenant Anus has discovered the cold-blooded killer behind everything.\\nYou okay?\\n', \"Millionaires, man.\\nIt'd make their heads explode.\\n\", \"Elly...\\nShe's dead.  She's gone.  And now you're just gonna go away and never come back, too.  I hate this place; it isn't fair.\\nYou brought flowers.  As long as you don't forget her, Elly, she lives.\\n\", 'It will take some time for the forms to clear before you go to Gettysburg.\\nThe estate stuff is pretty straight forward.  Just lots of forms and an appearance at the county seat.\\n', \"Of course not.  If Mars had an atmosphere, he's lose control.\\nCohaagen knows it makes air.  But the bastard won't turn it on.\\n\", 'No one. Maybe I dreamed it all up.\\nAnyone there?\\n', \"That's very nice of you. I don't get many dinner invitations on the job. It would be a welcome change. Thanks.\\nWould you like to stay for dinner?  There aren't many choices in town and ... anyway, you'd have to eat alone. So would I.\\n\", \"So you see, I'm not going anywhere.  Unless it's upstairs with you ...\\n... That's great.\\nI'm replacing Lazarro. Nice, safe desk job -- just like you wanted.\\n-- what?\\n\", 'Oh?\\nAs a matter of fact, I thought I might give it to you.\\n', \"How much? How much do you wanna bet?\\nI'll betcha! That's bullshit and I'll betcha! You're fulla shit!\\nYou wanna bet?\\nBullshit! That's bullshit!\\n\", \"Such as?\\nTed, all my life I'd either been somebody's daughter or somebody's wife, or somebody else's mother. Then all of a sudden, I was a thirty-two-year-old, highly neurotic woman who had just walked out on her husband and child.  I went to California because that was about as far away as I could get.  Only... I guess it wasn't far enough. So I started going to a shrink.  Ted, I've had time to think. I've been through some changes. I've learned a lot about myself.\\nJoanna, I don't give a--\\nTed, listen to me...You and I, we had a really crappy marriage--  Look, don't get so defensive, okay?  It was probably as much my fault as it was yours... Anyway when I left I was really screwed up--\\nAre you out of your mind?! You're the one that walked out on him, remember?\\nI want my son.  I'm through sitting in coffee shops looking at him from across the street. I want my son.\\nYou want what?!\\nTed...The reason I wanted to see you...I want Billy back.\\nYou've been living here, in the city?\\nWatching my son...Ted, I've been living in New York for the past two months.\\nHe is...  You sat in that coffee shop across from school--\\nHe looks like a terrific kid.\\n\", \"I mean, it's jammed.  If we went... um...\\nWell, sometimes, some, uh...\\n\", 'Hey, man. I took the Kings to the Cup.\\nYou should play another team. The Kings are bitches in this game.\\n', 'You can see the outline of a butt.\\nHow can you tell?\\nSomeone jumped on your car with their butt...\\n', \"Always the negative...\\nExcuse me.  I got a bullet in my leg.\\nStop limpin' around like that.\\n\\nHey-Hey-Hey-Hey-Hey.  We are heroes, my man.  It's time to start actin' like it.\\nThey aren't exactly gonna publicize this,  Arlo.\\nWe're the shit.  Bigtime.\\n\\nYou know, we saved half a million people from a full-body peel.\\n\\nWell.\\n\", \"General Korrd's military strategies were required learning when I was a cadet at the Academy. When they put me out to pasture, I hope I fare better than Korrd.\\nThe same. He's apparently fallen out of favor with the Klingon High Command.\\nNot General Korrd.\\n\", \"You're in a beehive, pal, didn't you know? We're all busy little bees, full of stings, making honey day and night-  - aren't we, honey?\\nOutside of a beehive, Margo, your behavior would hardly be considered either queenly or motherly!\\n\", 'If I were sitting where I normally sit, I would say \"Calls for speculation.\"\\nDidn\\'t this investigation, with its attendant publicity, catapult you into the office you now hold?\\nYes, Mr. Dowd.\\n', 'The Bismark in Chicago. You familiar?\\nYou own a hotel, sir?\\n', \"Why not?\\nAlright. Let's talk business.\\n\", 'Which is neutral, no doubt.\\nNot the usual Swiss procedure, Mr. Bond, but you understand, a man in my position..\\n', \"Paranoia's only reality on a finer scale.\\nYou're too goddamned paranoid.\\nHe's no concern of mine, as long as you don't talk to him.  Don't talk to anybody.  You understand?  Not with everything that's going on right now.\\nLeave him alone, Tran.\\nLenny the loser.  Panhandler of stolen dreams.\\n\", 'With butter, if they got any....\\nGreat!  Find one for me.\\n', \"I stand corrected.  Wyatt.  You're An oak.\\nSatisfied?\\n\", \"Thought you'd feel that way, Garry. You were the only one who could have gotten to that blood plasma...  ... we'll do you last...\\nPure nonsense.  This won't prove a damn thing.\\n\", 'What?\\nHey, I can get you three hundred cash for two hours.\\n', \"Haven't they already?\\nYou shouldn't be -- you were nearly just killed, sweetheart.  And now that our birthday girl is finally here, let the games begin!\\nI'm not laughing, Steven.\\nI'm impressed: I don't think Evelyn's ever said those words to anything with genitalia.\\nDon't touch me!\\n\", \"No, you can't.\\nI cannot get rid of you.\\n\", 'Sure.\\nI know.  Salt and pepper?\\nYeah.  You know, people are always like, \"What\\'re you gonna major in?\" And I don\\'t know.  And they\\'re like, \"You\\'ll figure it out.\"  Yeah?  When?\\nWell, you\\'re not.  Oil and vinegar?\\nOh thank God, I thought I was the only one.\\nWell, I mean, business is okay, and lacrosse is awesome, but what am I gonna be, a pro lacrosse player?  I really have no idea.\\nYeah.  So wow, you\\'ve got it figured out.\\nWell, State\\'s got a good business school.  And I can probably walk onto the lacrosse team.  Green peppers?\\nOh, yeah.  So what\\'re you gonna major in?\\nYou want onions?\\nWhat?\\nOnions?\\nYeah, well my parents wanted me to go to Northwestern.  I didn\\'t want to write all those extra essays they make you do -- I mean, how am I supposed to know what my \"most emotionally significant moment\" was?  So when my U of M acceptance came in December, I said the hell with it.\\nSo you\\'re going to Michigan?\\nThat\\'s nice.\\nMy dad\\'s always here running the store, busy and stuff...and I fill in once a week so he can get a night off.\\n', 'I never mad it.  The Greyhound bus I was on splattered some chick all over the road and we had to stop.\\nYeah, so what happened?\\nI got that beat.  So like, last May, I was supposed to stay at this cheesy bed and breakfast in Pennsylvania. There was a major gas leak no one knew about and all the guests suffocated during the night.\\n', \"Then I'm your man.  Come on, Toots.\\nYes.\\nNo kidding?\\nAs a matter of fact, I'm very good at double solitaire.\\nThinking? What about some double solitaire?\\nI'm thinking.\\nHow about you Lovey? Come on. Let's you and I play a game of honeymoon bridge.\\n\", \"You can't expect us to stay here --\\nMom, I want to go to the fair ...\\n\", \"I hear you're a good thief.\\nYeah.\\nYou're Anthony?\\nNice to meet you, Applejack.\\n\", \"What's stopping you?\\nYou know, sometimes I wish I was one of those girls they're letting in the flight program these days. God, I'd love to fly.\\n\", 'Yes.  Dropping off.\\nAre you asleep?\\n', \"It's out by the pumping station, shot full of holes.\\nWhy?\\nReally.  I thought maybe that was his snowmobile outside.  By the way -- your truck also 'on the fritz?'\\nSam?  We haven't seen him.\\nWhat about that hotheaded marshal, Sam Wilder?  I heard he was in the middle of this mess.\\nSomewhere along the pipeline.\\nWhere'd you say Eric is?\\nThe radio's on the fritz.\\nWhat happened here?\\n\", 'Thank you.\\nYes.\\nCan you see the lesions on your chest in this mirror?\\n', \"A Mark Cross overnight case, anyway. Compact, but ample enough.\\nThat's a suitcase?\\nYou said I'd have to live out of one suitcase  I'll bet yours isn't this small?\\n\", \"Major Burns will be out of your tent in twenty-four hours.  Tell them Captain Pierce and Captain Forrest are on their way.\\nYou work those kind of hours, you got to have rest. Which you can't get with somebody jabbering away on a direct line to heaven.\\n\", \"I hope it means something, otherwise...\\nBut the white dot in the middle; that's got to mean something.\\nSomething like that.\\nX marks the spot.\\n\", \"Go!\\nI can't!\\n\", \"No.\\nMr. Sim I want you to return to Dr. Bright's.  I believe she is hiding something of ours there.\\n\", \"A few bucks.\\nTake your shot, kid -- You got money for trainin' expenses?\\nI feel bad about walkin'.\\nHey -- if a good man can make a better life, let him make it.\\nY'know I won't be able to work for ya no more.\\n\", \"Yeah. Oh, God bless!\\nFifteen years, yeah.\\nFifteen years, huh?\\n'cause, uh, you never know what's gonna happen.\\nYeah. Oh, yeah. Yeah, I see. I guess-\\nYou know, I don't like to show my body to a man of my gender-\\nOh, I see, I see.\\n'Cause I don't like to get naked in front of another man, you know-it's, uh...\\nWhy not?\\nMe? No, no, no, 'cause I never shower in a public place.\\nWell, didn't you take, uh... uh, a shower at the club?\\nNo, I know, but... but, you know, I'm all perspired and everything.\\nWell, I mean, you don't have to, you know.\\n\", \"Y'know it is, a bit.\\nLiberating isn't it?\\n\", \"I looked everywhere. I even went to his house. It's locked up.\\nYou can't?\\nI can't find him.\\n\", \"Oh, well, I don't have them yet, but\\nScooby and a few other students of different socioeconomic backgrounds.\\nAnd you want Scooby to be the focus of all this?\\n\", \"Yes, I know. Did something unexpected crop up?\\nI just picked up some things for the night at the general store. You see, I hadn't planned on spending much time here.\\nIt's utilitarian, I'll say that for it.\\n\", 'Oh, there must be one in Paris... They have everything in Paris.\\nHow do you know there is a Grand Hotel?\\n', \"You like the way I cook...you think I make out a great invoice.  PHIL You and Nick and...and this...this garage are my whole life...I love you. Sure you don't have the greatest taste in music...but there's not another woman who could look so sexy in that smock. What I'm try... what I'm trying to sa...\\nYou know how I feel about you.\\nI think this place is just fine.\\n\", \"Couldn't be possible.  Must have weighted a ton and a half...\\nThis storm do that?\\n\", \"Nothing.\\nNothing came in for me yet?  No calls?\\nDoesn't look like it.\\nAw, bullshit, you heard wrong.\\nLong night, too, from what I heard ... Word's going around that in addition to losing Ganz for the second time, and in addition to Haden busting you back to Patrolman, some jig beat the crap out of you.\\nSo do you...been a long day.\\nYou look awful.\\n\", \"Hey buddy!\\n... but I'm starting to think I was... I was going to...\\nHey buddy!\\n\", \"Anna's the witch.\\nRemember....what Mary Brown said-- she could see the witch's hands on my face, her mouth sucking on mine.\\n\", 'Up the trail.\\nWhere is he?\\nClay Phillips.\\n', \"Oh.\\nYearse!\\nYour hearse?\\nBut this is my car.\\nWell, it's a new experience for me.  Good on curves. Shall I take you home, Harold?\\nYes.\\nOh, so do I. They're such fun, aren't they? It's all change. All revolving. Burials and births. The end to the beginning and the beginning to the end -  - the great circle of life. My, this old thing handles well. Ever drive a hearse, Harold?\\nYes.\\n\", 'Come in, boy -- come in.\\nExcuse me, Dr. MacFarlane --\\n', \"Spock?\\nI'm getting a voice message... wait ... short range band. They say their Chambers coil is shorting their COMM system.\\n\", \"Yeah, you don't want to tire him out, Doctor.\\nI hope you're pretty nearly through with me, Doctor, I'm getting a little fatigued.\\n\", \"From over the mountain --\\nBelle Dee. I'm from over the mountain.\\n\", \"Okay!\\nOh yeah!  Put it in your mouth!\\nDon't you want me to?\\nAre you gonna do what I think you're gonna do?\\n\", \"Oh shit.\\nThen don't do it.\\nDon't be mean, this is hard for me too.\\nWhat is that?  What are you doing with your hands?  Talk to me, you're talking like that girl Sheila.\\nLloyd, I love you, okay?\\nWhat, I'm sorry I said that.  Forget I said it, it's what I thought I meant, but forget it.\\nYou have Corey and DC.  I have my dad.\\nYou told your dad?\\nJust my dad.\\nDid you tell anybody?\\nNo, that's fine.  She'll tell everybody, but that's fine.\\nShe figured it out.  I'm sorry if that upsets you.\\nWhy, did you tell Corey what happened?\\nDid you talk to Corey?\\nNo.\\nIs this because of your dad?\\nNo I didn't.\\nYou shared it with a dick.\\nLloyd, we shared the most intimate thing two people can share.\\nYeah you do.\\nNo, I don't, I don't.\\nI feel like a dick.  You must think I'm a dick.\\nI think that we should stop going out on dates.\\nWell, if we're friends, why can't we see each other?\\nNo.  We decided that we're friends.  I mean, I know it's a terrible word...\\nIt sounded like you did.\\nNo, no.\\n'Cause I'm worried, did you just break up with me?\\nWe decided...\\nOkay, what did we just decide?\\n\", \"Brad. Please don't tell Mom and Dad...\\nSince when do you shop at the Flea Market anyway?\\n\", \"I never told you my name.\\nYou said it a million times!\\nBut I never said Rudy.\\nYou were screaming you weren't Nick! And we just didn't fucking believe you!\\nNo --\\nI... you told me your name was Rudy. You told me a million times, back in the truck, telling me you weren't Nick --\\nYou said Merry Christmas, Rudy.\\n\", \"Who is Gino?\\nIt started way before I was around. I think basically it's because he thinks Johnnie is a complete idiot. But Johnnie runs Chicago because Gino is his father.\\nWhy?\\nLike each other?  They hate each other.\\nIt sounded like he and Caesar don't like each other.\\nJohnnie?\\nAll right, now, tell me about Johnnie.\\n\", \"Oh.  Three hundred coming right up.\\nHey, they blew me.\\nYou gave it to the girls for one.\\nThree hundred.\\nHow much?\\nThanks.  You interested.  It's festival seating, so...\\nScalping to a funeral, you're a pretty sleazy guy.\\nGot those Vomit invites here...\\n\", 'Paul, do everything you can to buy time. I will call you back.\\nThe French. They supply the Rwandan army.\\n', 'Oh, this is too much. I\\'m gonna have to play these numbers. Remind me to pick up a Lotto ticket.\\nThat\\'s it.\\nIn room \"302\" at ten o\\'clock?\\nMe, too. 101?\\n', 'Want to try it again?\\nThank you.\\nHey, this works pretty good.\\n', \"Well, Joe said it, and he's right.\\nI'm so comfortable with that, Walt, I can't <u>tell</u> you...\\n\", 'No way to know.  Without more tests, experiments.\\nReed.  How close are we to a cure?\\n', 'IF-GOD-WILLS-IT!\\nAsia can be found to the west -- and I will prove it.\\n', \"I took pity on the poor woman. This house is too big for one person to live alone in...\\nWell it's nice that you were there for her.\\n\", \"God!  How old are you?  There ain't no bat.\\nThere was no blood, man.  My brother says... all the bad things you done... they come back and haunt you...\\n\", \"He wasn't his father. I was kinda hoping he'd want the job, though.\\nWas his father stationed here?\\nHe's my angel.\\nYour son.\\n\", \"That's what I came to see you about.\\nWould you be willing to meet with the group of scientists I am calling together?. Perhaps you could explain your mission to them, and they in turn could present it to their various peoples.\\n\", 'Warm water, no doubt.\\nScotch and plain water, I think.\\n', \"What about everything? How did this happen?\\nI can tell you anything else.\\nI've told you a lot of private stuff.\\n\", 'Yes, Boss.\\nYou cut that up fer lunch, Luke.\\n', 'And make it fast.  My horse is getting tired.\\nCopy that.\\n', \"Yeah.  He still tries popping up all over the place.  But he can't join the party unless you call on him.  Get the Deetzes out by yourselves! I gotta go.\\nOur cemetery?\\nHe's a freelance bio-exorcist. Claims to get rid of the living. But he's a troublemaker.  He's pushy.  He's been sleazing around that cemetery for 500 years.\\nWhat do you mean?  What's he do?\\nNo, you don't!  He does not work well with others.\\n\", 'Impossible.\\nNot a note.\\n', \"But the rest is me. I'll dress like I want.\\nIt is my business. It's my name.\\nMind your own business.\\n\", \"I mean, I'm standing right outside the LADIES ROOM door at the PLAZA THEATER. Your girlfriend Christine is in there.  She's all alone. Everyone's in the auditorium waiting for the film to start.  I've got my knife.\\nWhat do you mean?\\n\", \"And here I've made you all hot and sweaty.\\nNot until my car's fixed.  I don't know how long that's going to take.\\nYou're lucky you didn't break down in the desert.  Day like today, you'd be dead in no time. When you leaving?\\nDidn't have a choice.  My car overheated up the road.\\nDrove into Superior?  What for?\\nI just drove in this morning.\\nYou don't have that dead look in your eyes like the only thing you live for is to get through the day.\\nWhy you say that?  Just because I help a lady with her package?\\nYou're not from around here, are you?\\nYou're welcome, Grace.\\nThank you, Bobby.\\n\", \"Oh, don't worry about that ... Tell me ... doesn't Camelot own that stretch of farmland up by the mountains?\\nIt's just that when I'm in this genre, I tend to get over-excited and start to leap around and wave my sword about ... and ...\\n\", 'Yes - I have had it for years.\\nThis must be one of the old keys -\\n', \"We have to!\\nWe can't leave him!\\nGo!\\n\", \"-- But don't you remember?  I'm already dead.\\nYou're in there.  Everybody dies, you go with them --\\nTrue.\\nSystem's running on auxiliary.  Only seven hours before it crashes.\\n\", \"Thanks.\\nGood.\\nI've got a 24 hour a day reminder of Roger, for the rest of my life. I have had three lovers in four years, all boring, all achingly self-sufficient all friends of yours I might add, and all of them running a distant second to a warm bath.  Look at me, Laurel, look at me.  I'm the oldest 26 year old in the world!  How do I look?\\n\", \"Impossible to calculate.\\nThat's stupid. What are the odds of you being seen with me?\\nIt's against the rules. Dealers are forbidden to talk to punters.\\nYou know what? I'd like to buy you a drink.\\nOh hello.\\n\", \"I'm unemployed.\\nAnd what do you do, sir?\\nPapers.  Just papers.  You know, my papers.  Business papers.\\nIn the briefcase?\\nHuh?  Oh.  Yeah.  Tape deck.  Couple of Creedence tapes.  And there was a, uh. . . my briefcase.\\nAnd was there anything of value in the car?\\nGreen.  Some brown, or, uh, rust, coloration.\\nColor?\\n1972 Pontiac LeBaron.\\n\", \"No, I think something's up. The Sky Marshall's here.\\nWe gotta choke on the mud and the blood and Fleet can't be bothered to spit.\\n...I'm sorry.\\n\", 'I\\'ll bet. Jesus  \"The grand productive days.\" What a goddamn phony.\\nAnd my input was mostly... EDITORIAL, really, when he\\'d been drinking-\\nHah!\\nBarton, honestly, only the last couple-\\nI want to know!\\nBarton!\\nI want to know how many of Bill\\'s books you wrote!\\nBarton, I think we should concentrate on OUR little project-\\nWhat do you mean so to speak?! Audrey, how long have you been his... secretary?\\nWell, Bill was ALWAYS the author, so to speak-\\n', 'Of course.\\nPeople remember me from the news.  Can you drive me back now?\\n', \"How long do you have to chase a dream before you realize it's not gonna happen?\\nWe take the licks and he gets the chicks.\\nOh, who gives a damn who he is? I can't take this anymore. Night after night we're on the streets, busting our humps -- and for what?\\n\", \"I told you.  I knew it was there. Didn't I tell you?\\nThere's easily a hundred-thousand in there.  More than that.\\n\", \"Yes, well, everybody in Casablanca has problems. Yours may work out. You'll excuse me.\\nOh, but if you knew what it means to us to leave Europe, to get to America! Oh, but if Jan should find out! He is such a boy. In many ways I am so much older than he is.\\nGo back to Bulgaria.\\nOh, yes, please.\\nYou want my advice?\\nAnd he never knew, and the girl kept this bad thing locked in her heart? That would be all right, wouldn't it?\\nNobody ever loved me that much.\\n\", \"Oh that's just fine. Where were you when Ma and Daddy needed you? It's too late, Mal. Now you finally show up and all you can think of is to get yourself killed.\\nI'm not sure, but I got an idea. And when I am sure, they're going to pay.\\nWho did it?\\nHe was murdered.\\n\", \"Remember slamming the cab door in my face and..  you know, it came very dangerously close to emasculating my nose in a...\\nYeah, it was.\\nIt was a terrible evening.\\nYeah?\\nYeah, really.  You do.  You do.\\nOh, no.\\nYou look wonderful.\\nI'm fine.\\nGood.  How are you?\\nHow are you?\\nOh, that's putting it mildly.  We did everything but exchange gunshots.\\nWe didn't hit it off.\\n...and I thought I'd come in and...and we could replay, uh, the whole, uh...\\nYeah.\\nI was walking past and I saw you in here...\\nI recall you.\\nYes, you do recall, right?\\nI remember you.\\nMmm, I don't know if you remember me, but we had the worst night of my life together.\\n\", \"You're hanging by a very thin thread, dude.  And I dig that about you.\\nI'm out here for you!  You don't know what it's like to be me out here for you. It is an up-at-dawn pride-swallowing seige that I will never fully tell you about! Okay?! Help me help you help me help you.\\n\", \"When I told her I was going to bring her father, she kind of melted. It's the first time Felicity's talked nice to me since she left home. She's dying to meet you, Auggie.\\nCut it out. Just cut it out, okay? It's starting to get on my nerves.\\nShe looks just like you.\\nAnd what's that supposed to mean?\\nWait till you see her, Auggie.\\nLook, I'm not saying you don't have a daughter. It's just that she's not my daughter.\\nThat I wasn't bullshitting you, sweetheart. At least you'll know I've been telling the truth.\\nYeah, like what?\\nRelax, okay? You don't have to do anything. Just go in there and pretend. It won't kill you to do a little favor like that. Besides, you might even learn something.\\nI think you'd better stop the car and let me out.\\nIt was the only way, Auggie. Otherwise, she wasn't going to let me see her.\\nYou what?\\nI told her she was going to meet her father.\\n\", 'It is a gravestone in the cemetery of ______ in my native Florence.\\n<u>Do</u> you.  What does that mean?\\nI know it well.\\n...this one is my favorite.  It has not title.  They should call it \"fetch,\" whaddaya think...?\\n', \"Hey... he didn't look like no big time criminal to me.\\nYou saw that?\\nHey, it's none of my business why you're chasing a retard... You want my opinion, it's the girl. She had to hold the guy's hand all the way to the car like he was a kid.\\nYeah, thanks. We appreciate that.\\nBecause I'm the one who called the police, you know.\\nHuh?... Eh... no, there isn't\\nIs there a reward in this?\\n\", \"No, no, I just wanna get Norm some night crawlers.\\nYou don't think he's mixed up in -\\nHeck, yah. Ya think, is Dave open yet?\\nYah, woulda been cold out here.\\nSomebody shut his lights. I guess the little guy sat in there, waitin' for his buddy t'come back.\\n\", \"I told you...\\nNow, where's Mulligan? Where's Vivo?\\n\", \"Then her faith will have to be bigger.\\nBut... her army's so small now...\\nDon't worry, Charles.  If God is still with her, she will be victorious.  We're not her judges... we're just spectators.  Let her go to Compiegne, as you let her go to Orleans, and let God decide her fate.\\n\", \"Well, I guess I got it wrong, then. Seems to me I heard about a guy just your age that got hit so hard in the guts it almost killed him.\\nI don't see why not.\\nIf you stayed a crook, do you think you'd live to be my ripe age?\\nI don't?\\nThat's good. You don't really belong on this side of the fence, you know.\\nThat's right.\\nYou're getting off the grift?\\n\", \"I won't.\\nDon't wake up Vada.\\nI want juice!\\nThere's water in your bathroom.\\nI...ahhh...umm...I'm thirsty.\\nNicholas! What are you doing up?\\n\", \"I'm gone.\\nYou know what he mean, dude.\\nWhat you mean talk to her?\\n\", 'Need a ride?\\nNo.\\nShe had on high heels?\\nNo.\\nHave you seen a woman about so high?\\n', \"And it was the last. Hasn't written a letter since. Not even a birthday card.\\nWe all had to write a last letter home.\\n\", \"Paxcow says it's almost too late! We have to go back! Paxcow says it's almost too late!\\nEllie...Ellie...what...\\nPaxcow says it's almost too late!\\n\", 'Who is in authority here?\\nI can help you...\\n', \"I should be a swallow! Right now I would be sitting in front of the Caf de Paris picking up flakes of French pastry that would melt in my bill.\\nIf you asked him why he left France I bet he couldn't name one good reason.\\nIt is, Ninotchka! It is! He must have been in Paris! You can see it in his whole attitude! He just picked up a crumb of our black bread, shook his head, and dropped it.\\n\", '-- power too -- promise me that --\\n-- yes --\\n', \"Well considering what a colossal douche bag you are, David, maybe I'd do best to simply kick your ass all over the capitol.\\nYes.\\nReally?\\nConsidering the enormous exposure to which you've subjected this firm, I'd think you'd do best to simply answer my questions.\\nTold by whom?\\nI'm told you had an affair with Rachel Banks four years ago.\\nI don't know who he is.\\nWhy don't you just call Brill directly.\\n\", 'Nothing.\\nWhat are you doing?\\n', \"Johnny's C.D.?\\nYour timing swallows the massive one. Grendel just tried to kill us, he's about to frame and kiss Don, and we can't do shit.  Don't even ask about those discs. Goddamn that Art Mooney with a star by his name!  It's tied to Johnny's C.D., I know.\\n\", \"It's Igor at the Wax Museum! You'll find your judge embalmed in wax! He's a statue of Voltaire, with all the other corpses! The whole place is a morgue -- do you hear? -- a morgue!\\nYou lie!\\nNo -- the only thing I did for him was to keep track of the man named Worth that runs the place where you arrested me tonight.\\nBut you were in on it -- you worked for him.\\nNo -- I didn't. It was Igor at the waxworks.\\nYou killed him!  Come on with the rest of it. You killed him!\\n\", \"That's right.  So all we have to do is watch a bunch of movies and learn from their mistakes.\\nAnd they get caught!\\nC'mon, Luce, people do it in the movies all the time.\\n\", \"My house?\\nYes.  Well, I'm coming over to your house to get some things.\\nAre you coming home?\\nI thought I could give you a lift back.\\nHi.\\nHi.\\nShit!\\n\", \"Chet! From downstairs!\\n...Who?\\nChet!\\n...Who is this?\\nHow d'ya like your room!\\n\", 'Shut your mouth.\\nEarly, just think...\\n', 'Can you stop that slaughter?!  Can you free these men?\\nWhat are you fighting for in here? The good of Rome?  I can end this madness now!  Take the job for the sake of the Gods, live!\\n', \"We ain't goin' down the river.\\nYou got a chance in that boat -- by morning you could be five miles down the river.\\n\", \"I said I was sorry...\\nTerrific.  I now have a hundred dollar dry cleaning bill.\\nIt was an accident.\\nDon't help me, just get more napkins. And soda water.\\n\", \"Right ...\\nDouble-meaning intended, right?\\nOkay, then. I'll catch you later, Randall ...\\n\", \"...that.\\nYes...\\nCause, cause, it's...it's the <u>simple</u> things, that...\\nMmm.\\n...such a pleasant sound.\\n\", 'Mercutio\\nAnd the title of this piece?\\nWe are in desperate want of a Mercutio, Ned, a young nobleman of Verona\\n', 'Listen to this, I love this part --\\nPlus he fell in love with a temp.\\nAnd then you left him? He lost weight there and you left him?\\nYes.\\nNo.\\nCan too.\\nThat\\'s impossible.  A guy can\\'t --\\nHe lost all the weight... there.\\nYou never told me you left your husband because he went on a diet.\\nYou want to hear about destiny? If my husband hadn\\'t gone on a diet, which caused me to leave him, I would never have been on that flight to Miami, and met Rick, and ended up having sex in the bathroom of a 727 with that nob you slide that says \"vacant-occupied, vacant- occupied, vacant-occupied\" --\\n', \"I know.\\nIt wouldn't work.\\n\", \"I can do Heather's handwriting as well as my own.\\nAdolescence is a period of life fraught with anxiety and confusion.\\nLike a suicide thing?\\nWe did a murder. In Ohio, that's a crime. But if this was like a suicide thing.....\\n\", 'These are the latest word in android replicant technology.  Lethal, efficient, brutal. And no man can resist their charms. Send in the soldiers!\\nBreathtaking, Frau. These automated strumpets are the perfect bait for the degenerate Powers.\\n', \"Thanks, Mary. You can bring her in.\\nHoward, your one o'clock.\\n\", \"I'm an ape, mom. I'm an ape. And apes don't drop lines.\\nI'm in the book, if you ever want to drop me a line or something.\\nYes.\\nYes, I suppose so. I suppose I knew that was going to be what you would say. It's good to see you again though.\\nIt's a pleasure to meet you, mother. But I'm an ape like dad was...  And I have to go back into the woods now... forever.\\n\", 'Time for my coach to turn back into a pumpkin.\\nJack, must you go?\\n', \"I'm telling you this because... we've all lost our children, Mr. Ansel.\\nWhy are you telling me this?\\nThat's my daughter.  Or it may be the police to tell me that they've found her dead.  She's a drug addict.\\n\", \"That you are a slave, Neo.  Like everyone else, you were born into bondage, kept inside a prison that you cannot smell, taste, or touch. A prison for your mind.\\nWhat truth?\\nThe Matrix is everywhere, it's all around us, here even in this room. You can see it out your window or on your television.  You feel it when you go to work, or go to church or pay your taxes.  It is the world that has been pulled over your eyes to blind you from the truth.\\n\", 'Hello\\nHi, Patrick. I thought that was you.\\n', \"Or worse, we'll have a hostage situation on our hands.\\nAlright, we know who we're looking for.  Play it cool until the time is right to make a move.  Let's not pull any gangbusters shit or we'll lose him again.\\n\", \"Just be patient with me, Shu Lien.\\nI wish there were something more I could do to help you.\\nI thought by giving away the sword, I could escape the Giang Hu world. But the cycle of bloodshed continues.\\nIt's not our affair.  Even if Wudan accepts her, her husband might object.\\nFor her, they might make an exception.  If not, I'm afraid she'll become a poisoned dragon.\\nBut Wudan does not accept women.\\nThat's not for her.  She should come to Wudan and become a disciple.\\nShe's an aristocrat's daughter. She's not one of us.  In any case, it will all be over soon.  You'll kill Fox, and she'll marry.\\nShe needs direction... and training.\\nI knew she would intrigue you.\\nYou did your job well.  But, this girl... I saw her last night.\\nMy job was to get the sword back, without embarassing anyone.  I wasn't about to ruin her life, or her father's.\\nTrue.  But I must borrow it for one last mission.  Jade Fox must die at its edge.  Did you know what you were hiding when you covered for that girl?\\nBut it's not your sword anymore. You gave it to Sir Te.\\nI admit, getting it back makes me realize how much I'd missed it.\\n\", \"I'm wasting time.\\nPROBABLY going.\\nYou can get back on the telephone and tell them exactly what walked out of here last night. And tell them where he's going.\\nThere's nothing else I can do.\\nI notified everybody! Nobody listened.\\nHe was your patient, Doctor. If the precautions weren't sufficient, you should have notified...\\n\", \"Yeah.  I've noticed.\\nLook, you started this.  Now show me everything.  I can handle myself.\\nWell, that's the grenade launcher ...you probably don't want to mess with that.\\nWhat's this?\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_vecs_val = []\n",
        "\n",
        "for text in texts_val:\n",
        "  output = nlp_features(text,truncation=True,max_length=514)\n",
        "  output = np.array(output)\n",
        "  vec = output[0]\n",
        "  bert_vecs_val.append(vec[0]) # store the vecs\n"
      ],
      "metadata": {
        "id": "R0jt2NAq3BnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_vecs_train = []\n",
        "\n",
        "for text in texts_train:\n",
        "  output = nlp_features(text,truncation=True,max_length=514)\n",
        "  output = np.array(output)\n",
        "  vec = output[0]\n",
        "  bert_vecs_train.append(vec[0]) # store the vecs"
      ],
      "metadata": {
        "id": "xCkM4Da2m-ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqmVXEns99r3",
        "outputId": "721e92f3-82d0-47fb-83ae-efe88af847c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bert_vecs_train, labels_train)\n",
        "labels_predicted = clf.predict(bert_vecs_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CMH8GWonXzP",
        "outputId": "bc5b2c9b-ed0c-494a-feed-a93b95509f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(labels_val, labels_predicted)\n",
        "print(f\"{accuracy=:.3f}\")\n",
        "\n",
        "precision = precision_score(labels_val, labels_predicted, average=\"macro\")\n",
        "print(f\"{precision=:.3f}\")\n",
        "\n",
        "recall = recall_score(labels_val, labels_predicted, average=\"macro\")\n",
        "print(f\"{recall=:.3f}\")\n",
        "\n",
        "f1 = f1_score(labels_val, labels_predicted, average=\"macro\")\n",
        "print(f\"{f1=:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT02NikvnqHN",
        "outputId": "336f638b-1844-4141-d33d-848fcfbddc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy=0.389\n",
            "precision=0.242\n",
            "recall=0.164\n",
            "f1=0.148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traniner of HuggingFace\n"
      ],
      "metadata": {
        "id": "12TsKMaupclC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  ['drama', 'thriller', 'comedy', 'action', 'romance', 'crime', 'adventure', 'sci-fi', 'mystery', 'horror']\n",
        "def switch_label_to_int(label):\n",
        "  if label == 'drama':\n",
        "    return 0\n",
        "  elif label == 'thriller':\n",
        "    return 1\n",
        "  elif label == 'comedy':\n",
        "    return 2\n",
        "  elif label == 'action':\n",
        "    return 3\n",
        "  elif label == 'romance':\n",
        "    return 4\n",
        "  elif label == 'crime':\n",
        "    return 5\n",
        "  elif label == 'adventure':\n",
        "    return 6\n",
        "  elif label == 'sci-fi':\n",
        "    return 7\n",
        "  elif label == 'mystery':\n",
        "    return 8\n",
        "  elif label == 'horror':\n",
        "    return 9\n",
        "labels_train = [switch_label_to_int(label) for label in labels_train]\n",
        "labels_val = [switch_label_to_int(label) for label in labels_val]\n",
        "labels_test = [switch_label_to_int(label) for label in labels_test]"
      ],
      "metadata": {
        "id": "Od-xW3V9axMe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_datadic = {}\n",
        "conv_datadic['train'] = []\n",
        "conv_datadic['val'] = []\n",
        "conv_datadic['test'] = []\n",
        "for i in range(len(texts_train)):\n",
        "  dic = {}\n",
        "  dic['text'] = texts_train[i]\n",
        "  dic['label'] = labels_train[i]\n",
        "  conv_datadic['train'].append(dic)\n",
        "for i in range(len(texts_val)):\n",
        "  dic = {}\n",
        "  dic['text'] = texts_val[i]\n",
        "  dic['label'] = labels_val[i]\n",
        "  conv_datadic['val'].append(dic)\n",
        "for i in range(len(texts_test)):\n",
        "  dic = {}\n",
        "  dic['text'] = texts_test[i]\n",
        "  dic['label'] = labels_test[i]\n",
        "  conv_datadic['test'].append(dic)"
      ],
      "metadata": {
        "id": "mh1IpotcCOl3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "a7RRE101M26K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9397ee17-669a-45c0-bfb0-66494bdbf83b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, multiprocess, multidict, frozenlist, charset-normalizer, async-timeout, yarl, responses, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.10.1 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "eAndQS6-My1i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_dataset_train = Dataset.from_list(conv_datadic['train'])\n",
        "conv_dataset_val = Dataset.from_list(conv_datadic['val'])\n",
        "conv_dataset_test = Dataset.from_list(conv_datadic['test'])\n",
        "conv_dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQhjadtVNB6W",
        "outputId": "fa00e2dd-4593-4f3a-86b7-7bb22deef250"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 3269\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_dataset_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYAQFt-XZJp6",
        "outputId": "8dc99ee4-6d72-466c-e594-8a6ca2302277"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \"Then you will please wait outside.\\nI'm the stenographer.\\nWhat...! --\\n\",\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\",truncation=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "kwIiVsh_9Tyk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "dab922c1433444bbb1db24cabfee6561",
            "c72c1916877c4de9be6b96e32bf02f78",
            "ab70895939d347f2bbc8ce1fa6cc11a8",
            "af969efac688418891f44487490cd379",
            "0c722d3787a04c8faf1f76ff53131a2f",
            "c1b2d95f7cb94b219f5a8f47db90bc5f",
            "7743271ba96b48cabcb61d2065e78019",
            "7d4aa8465c6c4e2b809a2e6be3d7149a",
            "d4fa21df733341c5b85066fc8acddfd4",
            "18ca497df892419aa267a0aa0dc9ad09",
            "b45976781c1844a3b7da5d650915bc9f",
            "816e9cc14c4c4d2ab95349bc67956309",
            "5ea7034a0acb44cfa56cda6c4abe6281",
            "d5eefa29fb6f478082924fbfff4fc7ee",
            "0ab28aa8d0414176b863cfea6176815a",
            "df5d0fa744c04e8b81302892b6454ce7",
            "68775d9fe0494365af076e27b285c373",
            "3d8ec3d5e345470b870ea7be8489587f",
            "00cca07f965448bab8ffda1b3bfe7218",
            "a0855413cad5473ab0372533889a7761",
            "db35985902d84ce6b4fc5abe0456f586",
            "16642f70133949d7825a3d2d2a0edd20",
            "4647f9ebda384145a2150a020c78a370",
            "c632e2e6b13e4d6db83134405c51758c",
            "9b0ba2af3bbc4903a262d44cbb9989da",
            "1c3db980c2864a7198552867f54a56d4",
            "f9b28e2655704a5c8d3b5d3521c65a69",
            "fe61201b4e864dba8a62e30ce5310e75",
            "b0eab9b43df04bc7a3fdbd68d359ead9",
            "116e8b8a82614f80b487b6b35e39c305",
            "682ec492d2184f018d32f07740829a22",
            "6b4b767b76d44e25a7b38426d3815732",
            "4726d8e32a7f4268b52aaf4d16a9c805",
            "b950dc46496f4196806b79d5b6bddd7b",
            "ff106d7b68834411abbec79f8c9266fc",
            "4601ba2fe3d14f0fadc87bb3328e19bf",
            "d30727fa72d54d209045786a46d93cdb",
            "fb01b202fb654dbba0d6513c88f87db0",
            "6098816412734a93a3ece9881947d039",
            "6425bf64d0204bcc8feeea8bc3900191",
            "a3362ae854514b0d9d9dda47a0a3a88d",
            "32c6e3d9c1b544579f6b7935c64284fe",
            "a73ac7c1fb2d4a0db4e07774116f2738",
            "3251c351e2cd4c27b1a3733d5db1bfda"
          ]
        },
        "outputId": "10b141d1-d770-4392-b169-09c6f7c22f7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dab922c1433444bbb1db24cabfee6561"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "816e9cc14c4c4d2ab95349bc67956309"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4647f9ebda384145a2150a020c78a370"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b950dc46496f4196806b79d5b6bddd7b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(one_conv):\n",
        "    return tokenizer(one_conv[\"text\"], truncation=True)\n",
        "tokenized_conv_train = conv_dataset_train.map(preprocess_function, batched=True)\n",
        "tokenized_conv_val = conv_dataset_val.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "02cba1b8bf3844d185c921a5b5a743b4",
            "e25ad3927f9f46e78e4b1d6c5b3dd1de",
            "75e2269ccb3b4481aa8fb3cf682b1b9d",
            "30a7afc503a2403888b821e02c1046da",
            "b26773f9f19d4772a1a3b1d9aeadb547",
            "9d07ab7f97f14d538bc158d7e0919029",
            "b5bdcb547a5843c48776e37cc1888a45",
            "2d57269f8fdd485abbc48bce0d40f974",
            "721f29669b0c46eb81309dcc2e149df5",
            "76ed3d9f082d4027b083bdb757b46bdd",
            "c536a7eb455e45f09a1756edc433d77d",
            "287cbec667cf4117a74e4f586444cdf9",
            "9df1da4369574bba955934a7c3db96fd",
            "152a807b479b409993333ec8b827a639",
            "d0be6df4cdf24ae88e08b6382aaa7c05",
            "392a0ca7827b487f9f8632280ac7ad7b",
            "4383ac11880a453ca7c6eb518cdf5e64",
            "930afc566aee49e899ad34910ab30a1c",
            "4173058baa3d46cfbc88c93880c62b3f",
            "62ea97ef97e8473bb3fab08a3ff71d6e",
            "d63e3d468207432a950c2480af0121d0",
            "e504fcccd43748d690d819c1a42be08a"
          ]
        },
        "id": "4-37LN21IEcx",
        "outputId": "f6764ea6-bc7b-4123-c907-298046968592"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3269 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02cba1b8bf3844d185c921a5b5a743b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1090 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "287cbec667cf4117a74e4f586444cdf9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-eUX-axrLBu",
        "outputId": "b3ed59b9-8dc7-4ad8-ed76-332c4328ef4b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.10.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.25.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "1673c04c36424306990cbfe314cef6fd",
            "cfc767c429ef4051bc5e2e07230a70d8",
            "1e17619e56cf499a8aded3278e8e0d00",
            "b18adcf0219d4057a26a6872fe01274d",
            "52aa8c23ba2a482182de9c7ceec32cff",
            "167637d86c194bf79e34602086a652b3",
            "2f180ed5381c4f058bfc4aae6f3a50e1",
            "fc85b1e2ad2b45998404d786879eef5d",
            "7fde3e4d8890410aa536671ff97e88f8",
            "f8d33b1e29e24c97a2334005a6b4d050",
            "ee9a390dcd794b05b945aa93ae6cca30",
            "87cbcad6e0054d3d80eb66d703ae0932",
            "1c4cde89e2c8403eb511ef072064161c",
            "c570fec0498e44779ec191064ba9a24a",
            "5a9f453746e745c6bf8ca21e123398dd",
            "cf0093f3d75f400887d6a716028d5eb3",
            "746b362005d84d8bb8291bfe9661007c",
            "e5253d4fe05e493c8e830f5fb40e8fbb",
            "2ae3c219862c4e3f9ce6486ce31c0cc2",
            "aa93f0e64459459db0186a1313168c20",
            "38de4bfb562841f19c11b316bb8b1b82",
            "5074cbd83d4f42d29e62882b709c5cee",
            "f79c081b5e36492baac02dca77ee2a2c",
            "8bee7caa254f474faf7fdaf3b4c138ee",
            "58ebb54134a14f2ba2cd8dbd7b9230b3",
            "a26837a455d5481c9243edf1ea2005d5",
            "f9a74c2c7c79436f83a0cb14a1c0e9f9",
            "c9eb01bca13a4d5b971e4a42076055b1",
            "43c97024a2444166b961fe9af6e008cc",
            "0ad4d1f5802c4dbe845487ccb2074661",
            "73f0321586fe482ca62425453224e42d",
            "1cad13c645144a8fbebd0834ebc45ed7",
            "578c2dfa6fb64211b7a41afb9ffa5007",
            "47a3bf68bf884a48a26fb1a7a81d7cbe",
            "05fc4b2c6d224e79b2f4df85d1c8f6a1",
            "4c74487646004bbc8f9b73bff94f0ff8",
            "79e3f77edbfb4fc8ab7e663683d1f280",
            "22a19ca5411f440c94555143979cddc2",
            "6049bad1aadd4105b94c20d3b1c151ad",
            "30ed8d5d9aa04a6498699442c275b84a",
            "9ad0eeadaf5d44f9a4545457b0905959",
            "fbd3c67cbc4b4d1094d314f6850aa659",
            "b645315f6b8f4620a3d4f800d169838c",
            "466facddd1024e3e858ca4dee49d6abc"
          ]
        },
        "id": "C4oMZqIHrGI9",
        "outputId": "faa95c17-64c2-49bc-a5bd-33b73d42985d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1673c04c36424306990cbfe314cef6fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87cbcad6e0054d3d80eb66d703ae0932"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f79c081b5e36492baac02dca77ee2a2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47a3bf68bf884a48a26fb1a7a81d7cbe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
        "    precision_score = precision.compute(predictions=predictions, references=labels,average=\"macro\")\n",
        "    recall_score = recall.compute(predictions=predictions, references=labels,average=\"macro\")\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels,average=\"macro\")\n",
        "    result_metrix = {\"accuracy_score\": accuracy_score['accuracy'], \n",
        "              \"precision_score\":precision_score['precision'], \n",
        "              \"recall_score\":recall_score['recall'], \n",
        "              \"f1_score\":f1_score['f1']}\n",
        "    return result_metrix"
      ],
      "metadata": {
        "id": "-IxMqQnXrPSz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"drama\", 1: \"thriller\", 2: \"comedy\", 3: \"action\", 4: \"romance\", 5: \"crime\", 6: \"adventure\", 7: \"sci-fi\", 8: \"mystery\", 9: \"horror\"}\n",
        "label2id = {\"drama\": 0, \"thriller\": 1, \"comedy\": 2, \"action\": 3, \"romance\": 4, \"crime\": 5, \"adventure\": 6, \"sci-fi\": 7, \"mystery\": 8, \"horror\": 9}"
      ],
      "metadata": {
        "id": "3tuIsA8wJFxr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\" ,num_labels=10, id2label=id2label, label2id=label2id)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\" ,num_labels=10)\n",
        "# model = TFRobertaModel.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "7cbc611d806e4763a8335146cfbc3805",
            "22989ce0dbe940e6b13a212564c920f4",
            "832c8dadd997417bbb86b231952b60d9",
            "6e97df345b4a4b8e86b97f1bcdee00a0",
            "4eb7ba2929a44ae898def92b9d5a6eb1",
            "702089decb914dc4bed2b45845db619a",
            "a07b2eb1899b4855adb7d36fb6f319e9",
            "62098086b3674ee6b98896373eb69556",
            "0a10497d3c1b4663b0fc10ea5618c7e2",
            "5fe72e202b9a40b99e16130b27655364",
            "1e9f0eaecfb24a50bb9d976653f79cc0",
            "71938fb6dfee4702abede73d4825b8f5",
            "f55d949fafd64d2eb265b16faf4eee81",
            "cc8f4b462e4a43d8bea2cc123d641f90",
            "5e96ec9c03bd4f6a991aa4991704a44c",
            "ec6cbf78d8c046f7a739fa70fc1f5ee7",
            "f5c489de8fe2443b9522e2c3548aec67",
            "69eecc3232d243718fefa08ae64c05ea",
            "d582c3f6753342ba9653a4ffc4262aec",
            "38e50013e31a4ac1be9fafa479d2276c",
            "f1a466a27bb04d4f94af63c7fbe1ec47",
            "86277c909a1341128b9e03d9903d35b8"
          ]
        },
        "id": "uJq68S2eKAzz",
        "outputId": "73495f74-ce17-47ca-dcf7-c8706561186b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cbc611d806e4763a8335146cfbc3805"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71938fb6dfee4702abede73d4825b8f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "batch_size = 16\n",
        "epochs = 1 # This is set low so that the training can happen quickly for this lab.\n",
        "# weight_decay = 0"
      ],
      "metadata": {
        "id": "aUsGPLjWqAyv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed, TrainingArguments, Trainer\n",
        "set_seed(42)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"first_try_bert_model\", # HuggingFace wants a name for your model\n",
        "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
        "    learning_rate=learning_rate, # Hyperparameter\n",
        "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
        "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
        "    num_train_epochs=epochs, # Hyperparameter\n",
        "    # weight_decay=weight_decay, # Hyperparameter\n",
        ")"
      ],
      "metadata": {
        "id": "rznPwYZD44Us"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # The model you want to train\n",
        "    args=training_args, # The various training arguments set up above\n",
        "    train_dataset=tokenized_conv_train, # The data to use to update the weights\n",
        "    eval_dataset=tokenized_conv_val, # The data to use \n",
        "    tokenizer=tokenizer, # The tokenizer used on the data\n",
        "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "sHG0skm5qA4n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "7qy6n07uWW2n",
        "outputId": "5444ce8e-af1f-4005-ebb8-e30c1525d002"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [205/205 02:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy Score</th>\n",
              "      <th>Precision Score</th>\n",
              "      <th>Recall Score</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.784714</td>\n",
              "      <td>0.239450</td>\n",
              "      <td>0.023945</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.038638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=205, training_loss=1.7950278677591462, metrics={'train_runtime': 132.5227, 'train_samples_per_second': 24.667, 'train_steps_per_second': 1.547, 'total_flos': 345002428445136.0, 'train_loss': 1.7950278677591462, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning"
      ],
      "metadata": {
        "id": "-iPKkBED6uia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In this part, the tuning result were lost. I bought a colab plus in another google account and shared the notebook because the origin account have already run out of the computing units. The problem is that without privacy to save changes and accidently close of colab, all code-running-result are lost."
      ],
      "metadata": {
        "id": "4ggnFfAj4f9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "batch_size = 8\n",
        "epochs = 10 # This is set low so that the training can happen quickly for this lab."
      ],
      "metadata": {
        "id": "yC1R9Mvb5q_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "batch_size = 16\n",
        "epochs = 30 # This is set low so that the training can happen quickly for this lab."
      ],
      "metadata": {
        "id": "J_W30noB5uhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-7\n",
        "batch_size = 8\n",
        "epochs = 20 # This is set low so that the training can happen quickly for this lab."
      ],
      "metadata": {
        "id": "kBqDN0Zh5u3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed, TrainingArguments, Trainer\n",
        "set_seed(42)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"first_try_bert_model\", # HuggingFace wants a name for your model\n",
        "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
        "    learning_rate=learning_rate, # Hyperparameter\n",
        "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
        "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
        "    num_train_epochs=epochs, # Hyperparameter\n",
        "    # weight_decay=weight_decay, # Hyperparameter\n",
        ")"
      ],
      "metadata": {
        "id": "7E_76z8k6T8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # The model you want to train\n",
        "    args=training_args, # The various training arguments set up above\n",
        "    train_dataset=tokenized_conv_train, # The data to use to update the weights\n",
        "    eval_dataset=tokenized_conv_val, # The data to use \n",
        "    tokenizer=tokenizer, # The tokenizer used on the data\n",
        "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "ja2H96V86WLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "6cDvyWBg6MA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(tokenized_conv_val)\n",
        "metrics"
      ],
      "metadata": {
        "id": "jH1Gc1kW6ZYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "HSlDmy7A6ZgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(tokenized_conv_val)\n",
        "metrics"
      ],
      "metadata": {
        "id": "ViLGr0zC4dYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(tokenized_conv_val)\n",
        "metrics"
      ],
      "metadata": {
        "id": "LmuvjPmN6i3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "D0_gtkHs6eT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3JtKeds5ZWb"
      },
      "source": [
        "## **Part 6 Conclusion and Future Work**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7089b2-21dd-4ad8-c7b2-08675815fcba",
        "id": "YdMYdgNJGsw1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf_train_best.shape=(3291, 10369)\n",
            "tfidf_test_best.shape=(1098, 10369)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=False, max_features=None, max_df=0.7)\n",
        "tfidf_train_best = vectorizer.fit_transform(texts_train)\n",
        "tfidf_test_best = vectorizer.transform(texts_test)\n",
        "\n",
        "print(f\"{tfidf_train_best.shape=}\")\n",
        "print(f\"{tfidf_test_best.shape=}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_clf_tfidf_best = LogisticRegression(C=10, class_weight='balanced')\n",
        "lr_clf_tfidf_best.fit(tfidf_train_best, labels_train)\n",
        "labels_predicted_best_test = lr_clf_tfidf_best.predict(tfidf_test_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd7b5eb-ee7c-49de-f401-ac51f3899cb1",
        "id": "qI47Lsv9-Vtn"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy_test = accuracy_score(labels_test, labels_predicted_best_test)\n",
        "print(f\"{accuracy_test=:10.3}\")\n",
        "\n",
        "precision_test = precision_score(labels_test, labels_predicted_best_test, average=\"macro\")\n",
        "print(f\"{precision_test=:10.3}\")\n",
        "\n",
        "recall_test = recall_score(labels_test, labels_predicted_best_test, average=\"macro\")\n",
        "print(f\"{recall_test=:10.3}\")\n",
        "\n",
        "f1_test = f1_score(labels_test, labels_predicted_best_test, average=\"macro\")\n",
        "print(f\"{f1_test=:10.3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a89e4f-72dc-4269-f5ac-f4425933e7de",
        "id": "9nujRudP8CZB"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_test=     0.343\n",
            "precision_test=     0.283\n",
            "recall_test=     0.206\n",
            "f1_test=     0.214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "\n",
        "all_labels = ['drama', 'thriller', 'comedy', 'action', 'crime', 'romance', 'adventure', 'sci-fi', 'mystery', 'horror']\n",
        "\n",
        "def plotConfusionMatrix(labels_test, labels_predicted_best_test):\n",
        "  x = confusion_matrix(labels_test, labels_predicted_best_test, labels=all_labels)\n",
        "  plot = sn.heatmap(np.array(x), \n",
        "              annot=True, \n",
        "              annot_kws={\"size\": 12}, \n",
        "              fmt='g', \n",
        "              cbar=False,\n",
        "              xticklabels=all_labels, \n",
        "              yticklabels=all_labels\n",
        "            )\n",
        "  plot.set(xlabel='Predicted', ylabel='Actual')\n",
        "  return plot\n",
        "\n",
        "plotConfusionMatrix(labels_test, labels_predicted_best_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "-29mZLlP_ZQO",
        "outputId": "63a65c5c-903b-4e07-ae2f-811c49801b22"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Predicted', ylabel='Actual'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter_test = Counter(labels_test)\n",
        "conter_predict = Counter(labels_predicted_best_test)\n",
        "counter_test,conter_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z2IedWc_YcI",
        "outputId": "237aa368-06cd-47df-e10f-f582be2113f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Counter({'comedy': 273,\n",
              "          'drama': 282,\n",
              "          'crime': 118,\n",
              "          'action': 278,\n",
              "          'horror': 60,\n",
              "          'adventure': 56,\n",
              "          'sci-fi': 8,\n",
              "          'romance': 6,\n",
              "          'thriller': 10,\n",
              "          'mystery': 7}),\n",
              " Counter({'action': 270,\n",
              "          'drama': 318,\n",
              "          'comedy': 259,\n",
              "          'adventure': 47,\n",
              "          'crime': 130,\n",
              "          'horror': 55,\n",
              "          'thriller': 15,\n",
              "          'romance': 1,\n",
              "          'mystery': 3}))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70b0513a376f412ca5f4553b09e1ee43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fcfb90046284121b6e3159176651e3c",
              "IPY_MODEL_eb0f8786efc9481199e204014e335c7b",
              "IPY_MODEL_7eab3fa83cae47dcaf08bbe2cc9082cd"
            ],
            "layout": "IPY_MODEL_cc4c73c6c1f04134a0f92c2c7d721c89"
          }
        },
        "4fcfb90046284121b6e3159176651e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b45520dfb5f24d0abb6ecc0b3d25a2cd",
            "placeholder": "​",
            "style": "IPY_MODEL_f6f64a46d9524dc39b8280b3b66c2075",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "eb0f8786efc9481199e204014e335c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4486f5192669409f8acadd997a769328",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84bb2ad8785a40919edf94bccc72cd39",
            "value": 481
          }
        },
        "7eab3fa83cae47dcaf08bbe2cc9082cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe82996bd9d84288a9c15a234480a628",
            "placeholder": "​",
            "style": "IPY_MODEL_0299f19ef3414c5b9eda6d107995b048",
            "value": " 481/481 [00:00&lt;00:00, 21.7kB/s]"
          }
        },
        "cc4c73c6c1f04134a0f92c2c7d721c89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b45520dfb5f24d0abb6ecc0b3d25a2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6f64a46d9524dc39b8280b3b66c2075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4486f5192669409f8acadd997a769328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84bb2ad8785a40919edf94bccc72cd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe82996bd9d84288a9c15a234480a628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0299f19ef3414c5b9eda6d107995b048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03aeefeb18614c61a80f6d4c15075490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2da34a3298f54e96b9ff27ee9c2d813d",
              "IPY_MODEL_d49f046b79524561ba089e668cb0e07f",
              "IPY_MODEL_dce995808c674d62bdee514deee10572"
            ],
            "layout": "IPY_MODEL_7e989da963ab48a1812c4b4c5d643a74"
          }
        },
        "2da34a3298f54e96b9ff27ee9c2d813d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_684c1761ab604349b3647df639082987",
            "placeholder": "​",
            "style": "IPY_MODEL_fd3fc3c33eea486dbba47ac3cc13be4f",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "d49f046b79524561ba089e668cb0e07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b968ef8f1c458f865b75e2aa300a17",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17688e0369c348a9bb639a302c1623e1",
            "value": 501200538
          }
        },
        "dce995808c674d62bdee514deee10572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cd5b852d6494bd183d30202ab0a318a",
            "placeholder": "​",
            "style": "IPY_MODEL_cf34b219123e4691827b403a217ef892",
            "value": " 501M/501M [00:02&lt;00:00, 186MB/s]"
          }
        },
        "7e989da963ab48a1812c4b4c5d643a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684c1761ab604349b3647df639082987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3fc3c33eea486dbba47ac3cc13be4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4b968ef8f1c458f865b75e2aa300a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17688e0369c348a9bb639a302c1623e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cd5b852d6494bd183d30202ab0a318a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf34b219123e4691827b403a217ef892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed63770749a949b3b4123783fbe1e5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fe2fe4174c34154967f49b2c7a2397e",
              "IPY_MODEL_31e2ab94fceb4099aa092454884068e9",
              "IPY_MODEL_c60fd96d5f864300be9f0a16b8bb29a8"
            ],
            "layout": "IPY_MODEL_67ef8a12ff12465db77c4ff4e6e1ef30"
          }
        },
        "9fe2fe4174c34154967f49b2c7a2397e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d026a591d1a0433cab335aad8dd90fa0",
            "placeholder": "​",
            "style": "IPY_MODEL_971c8f7f758c405e8e7eeef504faf3f3",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "31e2ab94fceb4099aa092454884068e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c360234c174cca9e245b73c0ff07f1",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_315555863cfe4db4aff639399aff731e",
            "value": 898823
          }
        },
        "c60fd96d5f864300be9f0a16b8bb29a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b43662ab5b049abaa823ff038dc7fbe",
            "placeholder": "​",
            "style": "IPY_MODEL_39002aa039884397a3e00bc31e2a1afe",
            "value": " 899k/899k [00:00&lt;00:00, 2.10MB/s]"
          }
        },
        "67ef8a12ff12465db77c4ff4e6e1ef30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d026a591d1a0433cab335aad8dd90fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971c8f7f758c405e8e7eeef504faf3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1c360234c174cca9e245b73c0ff07f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315555863cfe4db4aff639399aff731e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b43662ab5b049abaa823ff038dc7fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39002aa039884397a3e00bc31e2a1afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4842d1ab7524311829d667fd2cf6044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa19af44f90c47b2b1f0fe54983d4eb9",
              "IPY_MODEL_e9375c3467324168805aa2640363119e",
              "IPY_MODEL_94fcbe56075f4004a970e141364b5054"
            ],
            "layout": "IPY_MODEL_61cb4777eed148f8b55f3fc7372ae527"
          }
        },
        "fa19af44f90c47b2b1f0fe54983d4eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32cd7cede3a547d6914eba30e0e01286",
            "placeholder": "​",
            "style": "IPY_MODEL_a91c7bbfb9dd48d09c361dd747f0906e",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "e9375c3467324168805aa2640363119e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_846036bbbf694bb1a0aa37bc844f45e4",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_887f862a9b0c4c56ba739f1b120ed1ef",
            "value": 456318
          }
        },
        "94fcbe56075f4004a970e141364b5054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0271a3ace24313800be28e235e4450",
            "placeholder": "​",
            "style": "IPY_MODEL_0fedb96a8bdc4822aa247090b6a71e00",
            "value": " 456k/456k [00:00&lt;00:00, 1.29MB/s]"
          }
        },
        "61cb4777eed148f8b55f3fc7372ae527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cd7cede3a547d6914eba30e0e01286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a91c7bbfb9dd48d09c361dd747f0906e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "846036bbbf694bb1a0aa37bc844f45e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887f862a9b0c4c56ba739f1b120ed1ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb0271a3ace24313800be28e235e4450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fedb96a8bdc4822aa247090b6a71e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a792a857208a443db18a98362ce7cd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75b22fcdda06470fafc1f588c2ff289c",
              "IPY_MODEL_7ebc5d18840a4adf817e1863f2985c5d",
              "IPY_MODEL_6f68c304c9f44310b0392c4c39a3266e"
            ],
            "layout": "IPY_MODEL_00a4ea835e4e459584b830c798c655a6"
          }
        },
        "75b22fcdda06470fafc1f588c2ff289c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210fb92f4f8e4043868ef067630a5b54",
            "placeholder": "​",
            "style": "IPY_MODEL_fb0852227002409ba7203a61cb620b3c",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "7ebc5d18840a4adf817e1863f2985c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d28bcbfc3664d98aa7479543c17e843",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ef64c51d16542528269426f66d22de6",
            "value": 1355863
          }
        },
        "6f68c304c9f44310b0392c4c39a3266e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9997d321bc6c4c188a951d6382297063",
            "placeholder": "​",
            "style": "IPY_MODEL_c082edceb1dd4cedb1f67c214c6cc173",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 2.62MB/s]"
          }
        },
        "00a4ea835e4e459584b830c798c655a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210fb92f4f8e4043868ef067630a5b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb0852227002409ba7203a61cb620b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d28bcbfc3664d98aa7479543c17e843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef64c51d16542528269426f66d22de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9997d321bc6c4c188a951d6382297063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c082edceb1dd4cedb1f67c214c6cc173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dab922c1433444bbb1db24cabfee6561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c72c1916877c4de9be6b96e32bf02f78",
              "IPY_MODEL_ab70895939d347f2bbc8ce1fa6cc11a8",
              "IPY_MODEL_af969efac688418891f44487490cd379"
            ],
            "layout": "IPY_MODEL_0c722d3787a04c8faf1f76ff53131a2f"
          }
        },
        "c72c1916877c4de9be6b96e32bf02f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1b2d95f7cb94b219f5a8f47db90bc5f",
            "placeholder": "​",
            "style": "IPY_MODEL_7743271ba96b48cabcb61d2065e78019",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "ab70895939d347f2bbc8ce1fa6cc11a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d4aa8465c6c4e2b809a2e6be3d7149a",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4fa21df733341c5b85066fc8acddfd4",
            "value": 28
          }
        },
        "af969efac688418891f44487490cd379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18ca497df892419aa267a0aa0dc9ad09",
            "placeholder": "​",
            "style": "IPY_MODEL_b45976781c1844a3b7da5d650915bc9f",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.27kB/s]"
          }
        },
        "0c722d3787a04c8faf1f76ff53131a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1b2d95f7cb94b219f5a8f47db90bc5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7743271ba96b48cabcb61d2065e78019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d4aa8465c6c4e2b809a2e6be3d7149a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4fa21df733341c5b85066fc8acddfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18ca497df892419aa267a0aa0dc9ad09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b45976781c1844a3b7da5d650915bc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816e9cc14c4c4d2ab95349bc67956309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ea7034a0acb44cfa56cda6c4abe6281",
              "IPY_MODEL_d5eefa29fb6f478082924fbfff4fc7ee",
              "IPY_MODEL_0ab28aa8d0414176b863cfea6176815a"
            ],
            "layout": "IPY_MODEL_df5d0fa744c04e8b81302892b6454ce7"
          }
        },
        "5ea7034a0acb44cfa56cda6c4abe6281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68775d9fe0494365af076e27b285c373",
            "placeholder": "​",
            "style": "IPY_MODEL_3d8ec3d5e345470b870ea7be8489587f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "d5eefa29fb6f478082924fbfff4fc7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00cca07f965448bab8ffda1b3bfe7218",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0855413cad5473ab0372533889a7761",
            "value": 483
          }
        },
        "0ab28aa8d0414176b863cfea6176815a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db35985902d84ce6b4fc5abe0456f586",
            "placeholder": "​",
            "style": "IPY_MODEL_16642f70133949d7825a3d2d2a0edd20",
            "value": " 483/483 [00:00&lt;00:00, 32.0kB/s]"
          }
        },
        "df5d0fa744c04e8b81302892b6454ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68775d9fe0494365af076e27b285c373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8ec3d5e345470b870ea7be8489587f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00cca07f965448bab8ffda1b3bfe7218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0855413cad5473ab0372533889a7761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db35985902d84ce6b4fc5abe0456f586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16642f70133949d7825a3d2d2a0edd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4647f9ebda384145a2150a020c78a370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c632e2e6b13e4d6db83134405c51758c",
              "IPY_MODEL_9b0ba2af3bbc4903a262d44cbb9989da",
              "IPY_MODEL_1c3db980c2864a7198552867f54a56d4"
            ],
            "layout": "IPY_MODEL_f9b28e2655704a5c8d3b5d3521c65a69"
          }
        },
        "c632e2e6b13e4d6db83134405c51758c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe61201b4e864dba8a62e30ce5310e75",
            "placeholder": "​",
            "style": "IPY_MODEL_b0eab9b43df04bc7a3fdbd68d359ead9",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "9b0ba2af3bbc4903a262d44cbb9989da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116e8b8a82614f80b487b6b35e39c305",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_682ec492d2184f018d32f07740829a22",
            "value": 231508
          }
        },
        "1c3db980c2864a7198552867f54a56d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b4b767b76d44e25a7b38426d3815732",
            "placeholder": "​",
            "style": "IPY_MODEL_4726d8e32a7f4268b52aaf4d16a9c805",
            "value": " 232k/232k [00:00&lt;00:00, 409kB/s]"
          }
        },
        "f9b28e2655704a5c8d3b5d3521c65a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe61201b4e864dba8a62e30ce5310e75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0eab9b43df04bc7a3fdbd68d359ead9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116e8b8a82614f80b487b6b35e39c305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682ec492d2184f018d32f07740829a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b4b767b76d44e25a7b38426d3815732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4726d8e32a7f4268b52aaf4d16a9c805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b950dc46496f4196806b79d5b6bddd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff106d7b68834411abbec79f8c9266fc",
              "IPY_MODEL_4601ba2fe3d14f0fadc87bb3328e19bf",
              "IPY_MODEL_d30727fa72d54d209045786a46d93cdb"
            ],
            "layout": "IPY_MODEL_fb01b202fb654dbba0d6513c88f87db0"
          }
        },
        "ff106d7b68834411abbec79f8c9266fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6098816412734a93a3ece9881947d039",
            "placeholder": "​",
            "style": "IPY_MODEL_6425bf64d0204bcc8feeea8bc3900191",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "4601ba2fe3d14f0fadc87bb3328e19bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3362ae854514b0d9d9dda47a0a3a88d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32c6e3d9c1b544579f6b7935c64284fe",
            "value": 466062
          }
        },
        "d30727fa72d54d209045786a46d93cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a73ac7c1fb2d4a0db4e07774116f2738",
            "placeholder": "​",
            "style": "IPY_MODEL_3251c351e2cd4c27b1a3733d5db1bfda",
            "value": " 466k/466k [00:00&lt;00:00, 633kB/s]"
          }
        },
        "fb01b202fb654dbba0d6513c88f87db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6098816412734a93a3ece9881947d039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6425bf64d0204bcc8feeea8bc3900191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3362ae854514b0d9d9dda47a0a3a88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c6e3d9c1b544579f6b7935c64284fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a73ac7c1fb2d4a0db4e07774116f2738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3251c351e2cd4c27b1a3733d5db1bfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02cba1b8bf3844d185c921a5b5a743b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e25ad3927f9f46e78e4b1d6c5b3dd1de",
              "IPY_MODEL_75e2269ccb3b4481aa8fb3cf682b1b9d",
              "IPY_MODEL_30a7afc503a2403888b821e02c1046da"
            ],
            "layout": "IPY_MODEL_b26773f9f19d4772a1a3b1d9aeadb547"
          }
        },
        "e25ad3927f9f46e78e4b1d6c5b3dd1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d07ab7f97f14d538bc158d7e0919029",
            "placeholder": "​",
            "style": "IPY_MODEL_b5bdcb547a5843c48776e37cc1888a45",
            "value": "Map:  92%"
          }
        },
        "75e2269ccb3b4481aa8fb3cf682b1b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d57269f8fdd485abbc48bce0d40f974",
            "max": 3269,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_721f29669b0c46eb81309dcc2e149df5",
            "value": 3269
          }
        },
        "30a7afc503a2403888b821e02c1046da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ed3d9f082d4027b083bdb757b46bdd",
            "placeholder": "​",
            "style": "IPY_MODEL_c536a7eb455e45f09a1756edc433d77d",
            "value": " 3000/3269 [00:00&lt;00:00, 7225.89 examples/s]"
          }
        },
        "b26773f9f19d4772a1a3b1d9aeadb547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "9d07ab7f97f14d538bc158d7e0919029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5bdcb547a5843c48776e37cc1888a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d57269f8fdd485abbc48bce0d40f974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721f29669b0c46eb81309dcc2e149df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76ed3d9f082d4027b083bdb757b46bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c536a7eb455e45f09a1756edc433d77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "287cbec667cf4117a74e4f586444cdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9df1da4369574bba955934a7c3db96fd",
              "IPY_MODEL_152a807b479b409993333ec8b827a639",
              "IPY_MODEL_d0be6df4cdf24ae88e08b6382aaa7c05"
            ],
            "layout": "IPY_MODEL_392a0ca7827b487f9f8632280ac7ad7b"
          }
        },
        "9df1da4369574bba955934a7c3db96fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4383ac11880a453ca7c6eb518cdf5e64",
            "placeholder": "​",
            "style": "IPY_MODEL_930afc566aee49e899ad34910ab30a1c",
            "value": "Map:  92%"
          }
        },
        "152a807b479b409993333ec8b827a639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4173058baa3d46cfbc88c93880c62b3f",
            "max": 1090,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ea97ef97e8473bb3fab08a3ff71d6e",
            "value": 1090
          }
        },
        "d0be6df4cdf24ae88e08b6382aaa7c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63e3d468207432a950c2480af0121d0",
            "placeholder": "​",
            "style": "IPY_MODEL_e504fcccd43748d690d819c1a42be08a",
            "value": " 1000/1090 [00:00&lt;00:00, 8006.31 examples/s]"
          }
        },
        "392a0ca7827b487f9f8632280ac7ad7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4383ac11880a453ca7c6eb518cdf5e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930afc566aee49e899ad34910ab30a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4173058baa3d46cfbc88c93880c62b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ea97ef97e8473bb3fab08a3ff71d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d63e3d468207432a950c2480af0121d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e504fcccd43748d690d819c1a42be08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1673c04c36424306990cbfe314cef6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfc767c429ef4051bc5e2e07230a70d8",
              "IPY_MODEL_1e17619e56cf499a8aded3278e8e0d00",
              "IPY_MODEL_b18adcf0219d4057a26a6872fe01274d"
            ],
            "layout": "IPY_MODEL_52aa8c23ba2a482182de9c7ceec32cff"
          }
        },
        "cfc767c429ef4051bc5e2e07230a70d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_167637d86c194bf79e34602086a652b3",
            "placeholder": "​",
            "style": "IPY_MODEL_2f180ed5381c4f058bfc4aae6f3a50e1",
            "value": "Downloading builder script: 100%"
          }
        },
        "1e17619e56cf499a8aded3278e8e0d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc85b1e2ad2b45998404d786879eef5d",
            "max": 4203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fde3e4d8890410aa536671ff97e88f8",
            "value": 4203
          }
        },
        "b18adcf0219d4057a26a6872fe01274d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8d33b1e29e24c97a2334005a6b4d050",
            "placeholder": "​",
            "style": "IPY_MODEL_ee9a390dcd794b05b945aa93ae6cca30",
            "value": " 4.20k/4.20k [00:00&lt;00:00, 287kB/s]"
          }
        },
        "52aa8c23ba2a482182de9c7ceec32cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "167637d86c194bf79e34602086a652b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f180ed5381c4f058bfc4aae6f3a50e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc85b1e2ad2b45998404d786879eef5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fde3e4d8890410aa536671ff97e88f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8d33b1e29e24c97a2334005a6b4d050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9a390dcd794b05b945aa93ae6cca30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87cbcad6e0054d3d80eb66d703ae0932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c4cde89e2c8403eb511ef072064161c",
              "IPY_MODEL_c570fec0498e44779ec191064ba9a24a",
              "IPY_MODEL_5a9f453746e745c6bf8ca21e123398dd"
            ],
            "layout": "IPY_MODEL_cf0093f3d75f400887d6a716028d5eb3"
          }
        },
        "1c4cde89e2c8403eb511ef072064161c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_746b362005d84d8bb8291bfe9661007c",
            "placeholder": "​",
            "style": "IPY_MODEL_e5253d4fe05e493c8e830f5fb40e8fbb",
            "value": "Downloading builder script: 100%"
          }
        },
        "c570fec0498e44779ec191064ba9a24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ae3c219862c4e3f9ce6486ce31c0cc2",
            "max": 6771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa93f0e64459459db0186a1313168c20",
            "value": 6771
          }
        },
        "5a9f453746e745c6bf8ca21e123398dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38de4bfb562841f19c11b316bb8b1b82",
            "placeholder": "​",
            "style": "IPY_MODEL_5074cbd83d4f42d29e62882b709c5cee",
            "value": " 6.77k/6.77k [00:00&lt;00:00, 466kB/s]"
          }
        },
        "cf0093f3d75f400887d6a716028d5eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746b362005d84d8bb8291bfe9661007c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5253d4fe05e493c8e830f5fb40e8fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ae3c219862c4e3f9ce6486ce31c0cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa93f0e64459459db0186a1313168c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38de4bfb562841f19c11b316bb8b1b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5074cbd83d4f42d29e62882b709c5cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f79c081b5e36492baac02dca77ee2a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bee7caa254f474faf7fdaf3b4c138ee",
              "IPY_MODEL_58ebb54134a14f2ba2cd8dbd7b9230b3",
              "IPY_MODEL_a26837a455d5481c9243edf1ea2005d5"
            ],
            "layout": "IPY_MODEL_f9a74c2c7c79436f83a0cb14a1c0e9f9"
          }
        },
        "8bee7caa254f474faf7fdaf3b4c138ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9eb01bca13a4d5b971e4a42076055b1",
            "placeholder": "​",
            "style": "IPY_MODEL_43c97024a2444166b961fe9af6e008cc",
            "value": "Downloading builder script: 100%"
          }
        },
        "58ebb54134a14f2ba2cd8dbd7b9230b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ad4d1f5802c4dbe845487ccb2074661",
            "max": 7546,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73f0321586fe482ca62425453224e42d",
            "value": 7546
          }
        },
        "a26837a455d5481c9243edf1ea2005d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cad13c645144a8fbebd0834ebc45ed7",
            "placeholder": "​",
            "style": "IPY_MODEL_578c2dfa6fb64211b7a41afb9ffa5007",
            "value": " 7.55k/7.55k [00:00&lt;00:00, 419kB/s]"
          }
        },
        "f9a74c2c7c79436f83a0cb14a1c0e9f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9eb01bca13a4d5b971e4a42076055b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c97024a2444166b961fe9af6e008cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ad4d1f5802c4dbe845487ccb2074661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f0321586fe482ca62425453224e42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cad13c645144a8fbebd0834ebc45ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578c2dfa6fb64211b7a41afb9ffa5007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47a3bf68bf884a48a26fb1a7a81d7cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05fc4b2c6d224e79b2f4df85d1c8f6a1",
              "IPY_MODEL_4c74487646004bbc8f9b73bff94f0ff8",
              "IPY_MODEL_79e3f77edbfb4fc8ab7e663683d1f280"
            ],
            "layout": "IPY_MODEL_22a19ca5411f440c94555143979cddc2"
          }
        },
        "05fc4b2c6d224e79b2f4df85d1c8f6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6049bad1aadd4105b94c20d3b1c151ad",
            "placeholder": "​",
            "style": "IPY_MODEL_30ed8d5d9aa04a6498699442c275b84a",
            "value": "Downloading builder script: 100%"
          }
        },
        "4c74487646004bbc8f9b73bff94f0ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ad0eeadaf5d44f9a4545457b0905959",
            "max": 7363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbd3c67cbc4b4d1094d314f6850aa659",
            "value": 7363
          }
        },
        "79e3f77edbfb4fc8ab7e663683d1f280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b645315f6b8f4620a3d4f800d169838c",
            "placeholder": "​",
            "style": "IPY_MODEL_466facddd1024e3e858ca4dee49d6abc",
            "value": " 7.36k/7.36k [00:00&lt;00:00, 386kB/s]"
          }
        },
        "22a19ca5411f440c94555143979cddc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6049bad1aadd4105b94c20d3b1c151ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ed8d5d9aa04a6498699442c275b84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ad0eeadaf5d44f9a4545457b0905959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd3c67cbc4b4d1094d314f6850aa659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b645315f6b8f4620a3d4f800d169838c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466facddd1024e3e858ca4dee49d6abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cbc611d806e4763a8335146cfbc3805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22989ce0dbe940e6b13a212564c920f4",
              "IPY_MODEL_832c8dadd997417bbb86b231952b60d9",
              "IPY_MODEL_6e97df345b4a4b8e86b97f1bcdee00a0"
            ],
            "layout": "IPY_MODEL_4eb7ba2929a44ae898def92b9d5a6eb1"
          }
        },
        "22989ce0dbe940e6b13a212564c920f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_702089decb914dc4bed2b45845db619a",
            "placeholder": "​",
            "style": "IPY_MODEL_a07b2eb1899b4855adb7d36fb6f319e9",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "832c8dadd997417bbb86b231952b60d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62098086b3674ee6b98896373eb69556",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a10497d3c1b4663b0fc10ea5618c7e2",
            "value": 481
          }
        },
        "6e97df345b4a4b8e86b97f1bcdee00a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe72e202b9a40b99e16130b27655364",
            "placeholder": "​",
            "style": "IPY_MODEL_1e9f0eaecfb24a50bb9d976653f79cc0",
            "value": " 481/481 [00:00&lt;00:00, 22.2kB/s]"
          }
        },
        "4eb7ba2929a44ae898def92b9d5a6eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702089decb914dc4bed2b45845db619a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07b2eb1899b4855adb7d36fb6f319e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62098086b3674ee6b98896373eb69556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a10497d3c1b4663b0fc10ea5618c7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fe72e202b9a40b99e16130b27655364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e9f0eaecfb24a50bb9d976653f79cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71938fb6dfee4702abede73d4825b8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f55d949fafd64d2eb265b16faf4eee81",
              "IPY_MODEL_cc8f4b462e4a43d8bea2cc123d641f90",
              "IPY_MODEL_5e96ec9c03bd4f6a991aa4991704a44c"
            ],
            "layout": "IPY_MODEL_ec6cbf78d8c046f7a739fa70fc1f5ee7"
          }
        },
        "f55d949fafd64d2eb265b16faf4eee81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5c489de8fe2443b9522e2c3548aec67",
            "placeholder": "​",
            "style": "IPY_MODEL_69eecc3232d243718fefa08ae64c05ea",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "cc8f4b462e4a43d8bea2cc123d641f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d582c3f6753342ba9653a4ffc4262aec",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38e50013e31a4ac1be9fafa479d2276c",
            "value": 501200538
          }
        },
        "5e96ec9c03bd4f6a991aa4991704a44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1a466a27bb04d4f94af63c7fbe1ec47",
            "placeholder": "​",
            "style": "IPY_MODEL_86277c909a1341128b9e03d9903d35b8",
            "value": " 501M/501M [00:23&lt;00:00, 21.4MB/s]"
          }
        },
        "ec6cbf78d8c046f7a739fa70fc1f5ee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c489de8fe2443b9522e2c3548aec67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69eecc3232d243718fefa08ae64c05ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d582c3f6753342ba9653a4ffc4262aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e50013e31a4ac1be9fafa479d2276c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1a466a27bb04d4f94af63c7fbe1ec47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86277c909a1341128b9e03d9903d35b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}